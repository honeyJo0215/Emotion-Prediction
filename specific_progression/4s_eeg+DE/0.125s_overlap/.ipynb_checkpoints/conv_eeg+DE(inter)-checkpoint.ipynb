{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a946caad-1f66-4e94-9418-55af09c2bd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 10:28:04.467964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 10:28:09.526500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-01-15 10:28:19.334257: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:28:20.301516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:28:20.305540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Sequential\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "\n",
    "# tf.config.optimizer.set_jit(False)\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #이것을 위로 두지 않으면 GPU 대신 CPU로 설정하는 게 동작하지 않음\n",
    "# TensorFlow 로그 레벨 설정\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "# GPU 설정\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Failed to set memory growth: {e}\")\n",
    "else:\n",
    "    print(\"No GPU devices found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce4c0a6-ddd7-4d41-a610-0483a94cc434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-Head Self Attention\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, **kwargs):\n",
    "        super(MultiHeadSelfAttention, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        weights = self.dropout(weights)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        print(f\"transpose 이후 attention의 shape: {attention.shape}\")\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf324a4-57f6-420e-bcb4-5a4027da023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feed Forward Network (FFN)\n",
    "class FeedForwardNetwork(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_units, **kwargs):\n",
    "        super(FeedForwardNetwork, self).__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(dense_units, activation=tf.nn.gelu)\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.dense2 = layers.Dense(embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(f\"ffn의 input의 모양:{inputs.shape}\")\n",
    "        x = self.dense1(inputs)\n",
    "        print(f\"dense1 layer이후:{inputs.shape}\")\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense2(x)\n",
    "        print(f\"dense2 layer이후:{inputs.shape}\")\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1993b621-c9ff-4ba6-a8e6-08408e203756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Temporal Encoding Layer\n",
    "class TemporalEncodingLayer(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_units, **kwargs):\n",
    "        super(TemporalEncodingLayer, self).__init__(**kwargs)\n",
    "        self.mhsa = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(embed_dim, ffn_units)\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        print(f\"transformer layer에 들어오는 input shape: {inputs.shape}\")\n",
    "        attn_output = self.mhsa(inputs)\n",
    "        print(f\"mhsa layer이후 shape: {attn_output.shape}\")\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection\n",
    "        print(f\"norm1 layer이후 shape: {out1.shape}\")\n",
    "        ffn_output = self.ffn(out1)\n",
    "        print(f\"ffn layer이후 shape: {ffn_output.shape}\")\n",
    "        out2 = self.layernorm2(out1 + ffn_output)   # Residual Connection\n",
    "        print(f\"norm2 layer이후 shape: {out2.shape}\")\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cb7325-7aa2-405e-a729-9629d2fc527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class ExpandLayer(layers.Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        super(ExpandLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.expand_dims(inputs, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb180a1c-dd6f-4ab4-86c0-7c1c48e5a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes, embed_dim, num_heads, ffn_units):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    print(f\"Input shape: {inputs.shape}\")\n",
    "\n",
    "    # Spatial-Spectral CNN blocks using Conv3D\n",
    "    x = ExpandLayer(axis=-1)(inputs)  # Add channel dimension #차원 확장\n",
    "    print(f\"After ExpandLayer: {x.shape}\")\n",
    "    x = layers.Conv3D(32, kernel_size=(3, 3, 1), activation=tf.nn.gelu, padding='same')(x) #3D 합성곱 연산\n",
    "    print(f\"After Conv3D(32): {x.shape}\")\n",
    "    x = layers.MaxPooling3D(pool_size=(1, 1, 2), padding='same')(x) #최대 풀링을 수행\n",
    "    print(f\"After MaxPooling3D(pool_size=(1, 1, 2)): {x.shape}\")\n",
    "    x = layers.Conv3D(64, kernel_size=(1, 1, 5), activation=tf.nn.gelu, padding='same')(x) #3D 합성곱 연산\n",
    "    print(f\"After Conv3D(64): {x.shape}\")\n",
    "    x = layers.MaxPooling3D(pool_size=(1, 1, 2), padding='same')(x)\n",
    "    print(f\"After MaxPooling3D(pool_size=(1, 1, 2)): {x.shape}\")\n",
    "    x = layers.Conv3D(128, kernel_size=(3, 3, 1), activation=tf.nn.gelu, padding='same')(x)\n",
    "    print(f\"After Conv3D(128): {x.shape}\")\n",
    "    x = layers.MaxPooling3D(pool_size=(1, 1, 2), padding='same')(x)\n",
    "    print(f\"After MaxPooling3D(pool_size=(1, 1, 2)): {x.shape}\")\n",
    "    x = layers.Dropout(0.5)(x)  #\n",
    "    print(f\"After Dropout(0.5): {x.shape}\")\n",
    "\n",
    "    # Flatten and prepare for transformer layers\n",
    "    x = layers.Flatten()(x) #다차원 텐서를 1차원 벡터로 변환(transformer에 입력하기 위해서)\n",
    "    print(f\"After Flatten: {x.shape}\")\n",
    "    x = layers.Dense(embed_dim, activation=tf.nn.gelu)(x) #1차원 벡터(평탄화된 벡터)를 embed_dim 차원의 벡터로 변환 (relu->gelu로 활성화함수 변경)\n",
    "    print(f\"After Dense(embed_dim): {x.shape}\")\n",
    "    \n",
    "    # Global Average Pooling instead of Flatten\n",
    "#    x = layers.GlobalAveragePooling3D()(x)  # Dimension reduction\n",
    "#    print(f\"After GlobalAveragePooling3D: {x.shape}\")  # e.g., (None, 128)\n",
    "    \n",
    "    # Dense layer with BatchNormalization and GELU for embedding\n",
    "#    x = layers.Dense(embed_dim, activation=None)(x)\n",
    "#    x = layers.BatchNormalization()(x)\n",
    "#    x = layers.Activation(tf.nn.gelu)(x)\n",
    "#    print(f\"After Dense(embed_dim) with BatchNormalization and GELU: {x.shape}\")\n",
    "\n",
    "    \n",
    "    # Emotion classification head before Transformer\n",
    "    emotion_logits = layers.Dense(num_classes, activation='softmax', name=\"emotion_logits\")(x)\n",
    "    print(f\"After Dense(num_classes): {emotion_logits.shape}\")\n",
    "\n",
    "    # Concatenate emotion predictions to transformer input\n",
    "    emotion_features = layers.Concatenate(axis=-1)([x, emotion_logits])\n",
    "    print(f\"After Concatenate: {emotion_features.shape}\")\n",
    "    emotion_features = ExpandLayer(axis=1)(emotion_features)  # Expand for temporal dimension\n",
    "    print(f\"After ExpandLayer for temporal dimension: {emotion_features.shape}\")\n",
    "\n",
    "    # Transformer layers\n",
    "    for i in range(3):  #2->3으로 변경\n",
    "        emotion_features = TemporalEncodingLayer(embed_dim + num_classes, num_heads, ffn_units)(emotion_features)\n",
    "        print(f\"After TemporalEncodingLayer {i + 1}: {emotion_features.shape}\")\n",
    "\n",
    "    # Classification head\n",
    "    x = layers.GlobalAveragePooling1D()(emotion_features)\n",
    "    print(f\"After GlobalAveragePooling1D: {x.shape}\")\n",
    "    x = layers.Dense(66, activation=tf.nn.gelu)(x)  #64->66으로 변경\n",
    "    print(f\"After Dense(66): {x.shape}\")\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    print(f\"After Dropout(0.5): {x.shape}\")\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1be1a7-cd42-4cd0-acab-4319d6a69112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "def load_data(data_dir):\n",
    "    X, y = [], []\n",
    "    unique_files = set()\n",
    "    print(f\"Loading data from: {data_dir}\")\n",
    "\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith(\"_FB.npy\"):\n",
    "            file_path = os.path.join(data_dir, file_name)\n",
    "            label = 1 if \"positive\" in file_name else 0\n",
    "\n",
    "            try:\n",
    "                data = np.load(file_path)\n",
    "                reshaped_data = np.transpose(data, (0, 2, 1)) if data.ndim == 3 else data\n",
    "                X.append(reshaped_data)\n",
    "                y.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "    if len(X) != len(y):\n",
    "        raise ValueError(f\"Data mismatch: X has {len(X)} samples, y has {len(y)} labels.\")\n",
    "\n",
    "    print(f\"Loaded {len(X)} samples. Labels: {np.bincount(y)}\")\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820dc93b-e295-4fd8-9f64-00cdfe421c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Training\n",
    "def train_and_evaluate_kfold(data_dir, model_save_path, k=5):\n",
    "    X, y = load_data(data_dir)\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "\n",
    "    # Class weights for imbalance handling\n",
    "    class_counts = np.bincount(y)\n",
    "    class_weights = {i: max(class_counts) / c for i, c in enumerate(class_counts)}\n",
    "    print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        print(f\"Starting Fold {fold + 1}/{k}...\")\n",
    "\n",
    "        X_fold_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_fold_train, y_val = y[train_idx], y[val_idx]\n",
    "        print(f\"Training samples: {len(y_fold_train)}, Validation samples: {len(y_val)}\")\n",
    "\n",
    "        # Initialize a fresh model for each fold\n",
    "        model = build_model(\n",
    "            input_shape=X_fold_train.shape[1:],\n",
    "            num_classes=2,\n",
    "            embed_dim=64,\n",
    "            num_heads= 2,   #2(o) 3(x) 6(o)\n",
    "            ffn_units=128\n",
    "        )\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                      loss=\"sparse_categorical_crossentropy\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        # Learning Rate Scheduler\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.0005 * (0.9 ** epoch))\n",
    "\n",
    "        # Early Stopping\n",
    "        #early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            epochs=100,\n",
    "            batch_size=16,\n",
    "            class_weight=class_weights,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[lr_scheduler], #, early_stopping],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # Evaluate the model\n",
    "        val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        fold_metrics.append({\"fold\": fold + 1, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
    "        print(f\"Fold {fold + 1}: Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the final model\n",
    "    model.save(model_save_path, save_format=\"h5\")\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "    # Print average metrics\n",
    "    avg_loss = np.mean([m[\"val_loss\"] for m in fold_metrics])\n",
    "    avg_accuracy = np.mean([m[\"val_accuracy\"] for m in fold_metrics])\n",
    "    print(f\"Average Validation Loss: {avg_loss:.4f}, Average Validation Accuracy: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384bd75e-dead-4475-a1ae-fae6fbfb57cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/bcml1/2025_EMOTION/DEAP_EEG/overlap4s_seg_conv_ch_BPF+DE/train_overlap0.125\n",
      "Loaded 81787 samples. Labels: [39882 41905]\n",
      "Class weights: {0: 1.0507246376811594, 1: 1.0}\n",
      "Starting Fold 1/5...\n",
      "Training samples: 65429, Validation samples: 16358\n",
      "Input shape: (None, 4, 8, 513)\n",
      "After ExpandLayer: (None, 4, 8, 513, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 10:30:55.533979: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:30:55.537906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:30:55.541624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:30:55.804692: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:30:55.807245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:30:55.809719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-01-15 10:30:55.812114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22360 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Conv3D(32): (None, 4, 8, 513, 32)\n",
      "After MaxPooling3D(pool_size=(1, 1, 2)): (None, 4, 8, 257, 32)\n",
      "After Conv3D(64): (None, 4, 8, 257, 64)\n",
      "After MaxPooling3D(pool_size=(1, 1, 2)): (None, 4, 8, 129, 64)\n",
      "After Conv3D(128): (None, 4, 8, 129, 128)\n",
      "After MaxPooling3D(pool_size=(1, 1, 2)): (None, 4, 8, 65, 128)\n",
      "After Dropout(0.5): (None, 4, 8, 65, 128)\n",
      "After Flatten: (None, 266240)\n",
      "After Dense(embed_dim): (None, 64)\n",
      "After Dense(num_classes): (None, 2)\n",
      "After Concatenate: (None, 66)\n",
      "After ExpandLayer for temporal dimension: (None, 1, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "After TemporalEncodingLayer 1: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "After TemporalEncodingLayer 2: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "After TemporalEncodingLayer 3: (None, None, 66)\n",
      "After GlobalAveragePooling1D: (None, 66)\n",
      "After Dense(66): (None, 66)\n",
      "After Dropout(0.5): (None, 66)\n",
      "Output shape: (None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 10:30:59.274275: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4296329856 exceeds 10% of free system memory.\n",
      "2025-01-15 10:31:02.110373: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4296329856 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1736937069.315019 2786397 service.cc:145] XLA service 0x7f0f9c0127b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1736937069.315068 2786397 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2025-01-15 10:31:09.500141: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-01-15 10:31:10.297298: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  19/4090\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.4633 - loss: 1.0350     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1736937083.911211 2786397 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer layer에 들어오는 input shape: (None, 1, 66)7m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5324 - loss: 0.7286\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13ms/step - accuracy: 0.5324 - loss: 0.7285 - val_accuracy: 0.6528 - val_loss: 0.6139 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.6905 - loss: 0.5797 - val_accuracy: 0.7947 - val_loss: 0.4213 - learning_rate: 4.5000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.8231 - loss: 0.3970 - val_accuracy: 0.8686 - val_loss: 0.2990 - learning_rate: 4.0500e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.9009 - loss: 0.2525 - val_accuracy: 0.8859 - val_loss: 0.2798 - learning_rate: 3.6450e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9315 - loss: 0.1884 - val_accuracy: 0.9103 - val_loss: 0.2279 - learning_rate: 3.2805e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9598 - loss: 0.1195 - val_accuracy: 0.9244 - val_loss: 0.2337 - learning_rate: 2.9525e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9745 - loss: 0.0819 - val_accuracy: 0.9107 - val_loss: 0.2722 - learning_rate: 2.6572e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9839 - loss: 0.0543 - val_accuracy: 0.9331 - val_loss: 0.2581 - learning_rate: 2.3915e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9886 - loss: 0.0400 - val_accuracy: 0.9353 - val_loss: 0.2114 - learning_rate: 2.1523e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.9902 - loss: 0.0343 - val_accuracy: 0.9402 - val_loss: 0.2158 - learning_rate: 1.9371e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9937 - loss: 0.0239 - val_accuracy: 0.9471 - val_loss: 0.2364 - learning_rate: 1.7434e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9944 - loss: 0.0201 - val_accuracy: 0.9483 - val_loss: 0.2321 - learning_rate: 1.5691e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9960 - loss: 0.0157 - val_accuracy: 0.9463 - val_loss: 0.2452 - learning_rate: 1.4121e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9967 - loss: 0.0130 - val_accuracy: 0.9468 - val_loss: 0.2478 - learning_rate: 1.2709e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.9969 - loss: 0.0132 - val_accuracy: 0.9473 - val_loss: 0.2353 - learning_rate: 1.1438e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.9975 - loss: 0.0085 - val_accuracy: 0.9507 - val_loss: 0.2591 - learning_rate: 1.0295e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9980 - loss: 0.0085 - val_accuracy: 0.9473 - val_loss: 0.3605 - learning_rate: 9.2651e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0082 - val_accuracy: 0.9505 - val_loss: 0.2744 - learning_rate: 8.3386e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9986 - loss: 0.0056 - val_accuracy: 0.9521 - val_loss: 0.2996 - learning_rate: 7.5047e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0045 - val_accuracy: 0.9554 - val_loss: 0.2997 - learning_rate: 6.7543e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0054 - val_accuracy: 0.9534 - val_loss: 0.2819 - learning_rate: 6.0788e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9991 - loss: 0.0034 - val_accuracy: 0.9488 - val_loss: 0.3826 - learning_rate: 5.4709e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 0.9540 - val_loss: 0.3350 - learning_rate: 4.9239e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9992 - loss: 0.0040 - val_accuracy: 0.9551 - val_loss: 0.3522 - learning_rate: 4.4315e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0028 - val_accuracy: 0.9563 - val_loss: 0.3318 - learning_rate: 3.9883e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9534 - val_loss: 0.3801 - learning_rate: 3.5895e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0024 - val_accuracy: 0.9568 - val_loss: 0.3206 - learning_rate: 3.2305e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0026 - val_accuracy: 0.9579 - val_loss: 0.3122 - learning_rate: 2.9075e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9586 - val_loss: 0.3651 - learning_rate: 2.6167e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0031 - val_accuracy: 0.9573 - val_loss: 0.3266 - learning_rate: 2.3551e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 0.9589 - val_loss: 0.3424 - learning_rate: 2.1196e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0016 - val_accuracy: 0.9596 - val_loss: 0.3609 - learning_rate: 1.9076e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9590 - val_loss: 0.3735 - learning_rate: 1.7168e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.1823e-04 - val_accuracy: 0.9575 - val_loss: 0.3885 - learning_rate: 1.5452e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0019 - val_accuracy: 0.9586 - val_loss: 0.3857 - learning_rate: 1.3906e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 3.6825e-04 - val_accuracy: 0.9594 - val_loss: 0.3868 - learning_rate: 1.2516e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.0319e-04 - val_accuracy: 0.9604 - val_loss: 0.4005 - learning_rate: 1.1264e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9590 - val_loss: 0.3986 - learning_rate: 1.0138e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 7.4801e-04 - val_accuracy: 0.9612 - val_loss: 0.3960 - learning_rate: 9.1240e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0010 - val_accuracy: 0.9604 - val_loss: 0.3907 - learning_rate: 8.2116e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 3.6801e-04 - val_accuracy: 0.9601 - val_loss: 0.4080 - learning_rate: 7.3904e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9603 - val_loss: 0.4014 - learning_rate: 6.6514e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9606 - val_loss: 0.3992 - learning_rate: 5.9863e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 9.2709e-05 - val_accuracy: 0.9604 - val_loss: 0.4181 - learning_rate: 5.3876e-06\n",
      "Epoch 45/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9608 - val_loss: 0.4185 - learning_rate: 4.8489e-06\n",
      "Epoch 46/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0010 - val_accuracy: 0.9601 - val_loss: 0.4249 - learning_rate: 4.3640e-06\n",
      "Epoch 47/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 2.7580e-04 - val_accuracy: 0.9603 - val_loss: 0.4293 - learning_rate: 3.9276e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0013 - val_accuracy: 0.9605 - val_loss: 0.4264 - learning_rate: 3.5348e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 7.1617e-04 - val_accuracy: 0.9611 - val_loss: 0.4226 - learning_rate: 3.1813e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9607 - val_loss: 0.4225 - learning_rate: 2.8632e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 7.9528e-04 - val_accuracy: 0.9611 - val_loss: 0.4218 - learning_rate: 2.5769e-06\n",
      "Epoch 52/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 8.4627e-04 - val_accuracy: 0.9611 - val_loss: 0.4212 - learning_rate: 2.3192e-06\n",
      "Epoch 53/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0017 - val_accuracy: 0.9611 - val_loss: 0.4216 - learning_rate: 2.0873e-06\n",
      "Epoch 54/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9611 - val_loss: 0.4220 - learning_rate: 1.8786e-06\n",
      "Epoch 55/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.7309e-04 - val_accuracy: 0.9609 - val_loss: 0.4207 - learning_rate: 1.6907e-06\n",
      "Epoch 56/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.4101e-04 - val_accuracy: 0.9611 - val_loss: 0.4215 - learning_rate: 1.5216e-06\n",
      "Epoch 57/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 3.6399e-04 - val_accuracy: 0.9605 - val_loss: 0.4215 - learning_rate: 1.3695e-06\n",
      "Epoch 58/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 2.0499e-04 - val_accuracy: 0.9605 - val_loss: 0.4236 - learning_rate: 1.2325e-06\n",
      "Epoch 59/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 2.5245e-04 - val_accuracy: 0.9606 - val_loss: 0.4229 - learning_rate: 1.1093e-06\n",
      "Epoch 60/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 3.7819e-04 - val_accuracy: 0.9611 - val_loss: 0.4237 - learning_rate: 9.9834e-07\n",
      "Epoch 61/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.9710e-04 - val_accuracy: 0.9612 - val_loss: 0.4241 - learning_rate: 8.9851e-07\n",
      "Epoch 62/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.4918e-04 - val_accuracy: 0.9611 - val_loss: 0.4243 - learning_rate: 8.0865e-07\n",
      "Epoch 63/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0014 - val_accuracy: 0.9612 - val_loss: 0.4225 - learning_rate: 7.2779e-07\n",
      "Epoch 64/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0014 - val_accuracy: 0.9610 - val_loss: 0.4218 - learning_rate: 6.5501e-07\n",
      "Epoch 65/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.2836e-04 - val_accuracy: 0.9609 - val_loss: 0.4223 - learning_rate: 5.8951e-07\n",
      "Epoch 66/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.0642e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 5.3056e-07\n",
      "Epoch 67/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 6.2200e-04 - val_accuracy: 0.9610 - val_loss: 0.4213 - learning_rate: 4.7750e-07\n",
      "Epoch 68/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.3233e-04 - val_accuracy: 0.9611 - val_loss: 0.4211 - learning_rate: 4.2975e-07\n",
      "Epoch 69/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.1879e-04 - val_accuracy: 0.9610 - val_loss: 0.4209 - learning_rate: 3.8678e-07\n",
      "Epoch 70/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 3.5967e-04 - val_accuracy: 0.9609 - val_loss: 0.4212 - learning_rate: 3.4810e-07\n",
      "Epoch 71/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 2.4926e-04 - val_accuracy: 0.9611 - val_loss: 0.4212 - learning_rate: 3.1329e-07\n",
      "Epoch 72/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9615 - val_loss: 0.4212 - learning_rate: 2.8196e-07\n",
      "Epoch 73/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 3.8053e-04 - val_accuracy: 0.9612 - val_loss: 0.4213 - learning_rate: 2.5376e-07\n",
      "Epoch 74/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 3.4953e-04 - val_accuracy: 0.9612 - val_loss: 0.4215 - learning_rate: 2.2839e-07\n",
      "Epoch 75/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.5668e-04 - val_accuracy: 0.9612 - val_loss: 0.4215 - learning_rate: 2.0555e-07\n",
      "Epoch 76/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 5.2047e-04 - val_accuracy: 0.9613 - val_loss: 0.4216 - learning_rate: 1.8499e-07\n",
      "Epoch 77/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9612 - val_loss: 0.4220 - learning_rate: 1.6649e-07\n",
      "Epoch 78/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 6.6309e-04 - val_accuracy: 0.9613 - val_loss: 0.4220 - learning_rate: 1.4985e-07\n",
      "Epoch 79/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 7.2118e-04 - val_accuracy: 0.9612 - val_loss: 0.4222 - learning_rate: 1.3486e-07\n",
      "Epoch 80/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 9.5206e-05 - val_accuracy: 0.9612 - val_loss: 0.4224 - learning_rate: 1.2137e-07\n",
      "Epoch 81/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.7959e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 1.0924e-07\n",
      "Epoch 82/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.2850e-04 - val_accuracy: 0.9612 - val_loss: 0.4224 - learning_rate: 9.8314e-08\n",
      "Epoch 83/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 8.4897e-04 - val_accuracy: 0.9612 - val_loss: 0.4223 - learning_rate: 8.8482e-08\n",
      "Epoch 84/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.5213e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 7.9634e-08\n",
      "Epoch 85/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 2.7316e-04 - val_accuracy: 0.9612 - val_loss: 0.4224 - learning_rate: 7.1671e-08\n",
      "Epoch 86/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.2813e-04 - val_accuracy: 0.9612 - val_loss: 0.4224 - learning_rate: 6.4504e-08\n",
      "Epoch 87/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0011 - val_accuracy: 0.9612 - val_loss: 0.4223 - learning_rate: 5.8053e-08\n",
      "Epoch 88/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 9.6767e-04 - val_accuracy: 0.9612 - val_loss: 0.4223 - learning_rate: 5.2248e-08\n",
      "Epoch 89/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1848e-04 - val_accuracy: 0.9611 - val_loss: 0.4224 - learning_rate: 4.7023e-08\n",
      "Epoch 90/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 9.1058e-04 - val_accuracy: 0.9611 - val_loss: 0.4223 - learning_rate: 4.2321e-08\n",
      "Epoch 91/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 2.1555e-04 - val_accuracy: 0.9611 - val_loss: 0.4224 - learning_rate: 3.8089e-08\n",
      "Epoch 92/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 3.8161e-04 - val_accuracy: 0.9611 - val_loss: 0.4224 - learning_rate: 3.4280e-08\n",
      "Epoch 93/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.1730e-04 - val_accuracy: 0.9611 - val_loss: 0.4224 - learning_rate: 3.0852e-08\n",
      "Epoch 94/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.1786e-04 - val_accuracy: 0.9611 - val_loss: 0.4224 - learning_rate: 2.7767e-08\n",
      "Epoch 95/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 2.8316e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 2.4990e-08\n",
      "Epoch 96/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.5053e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 2.2491e-08\n",
      "Epoch 97/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.3315e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 2.0242e-08\n",
      "Epoch 98/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 4.1320e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 1.8218e-08\n",
      "Epoch 99/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 9.9854e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 1.6396e-08\n",
      "Epoch 100/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 1.7283e-04 - val_accuracy: 0.9611 - val_loss: 0.4225 - learning_rate: 1.4756e-08\n",
      "Fold 1: Validation Loss: 0.4225, Validation Accuracy: 0.9611\n",
      "Starting Fold 2/5...\n",
      "Training samples: 65429, Validation samples: 16358\n",
      "Input shape: (None, 4, 8, 513)\n",
      "After ExpandLayer: (None, 4, 8, 513, 1)\n",
      "After Conv3D(32): (None, 4, 8, 513, 32)\n",
      "After MaxPooling3D(pool_size=(1, 1, 2)): (None, 4, 8, 257, 32)\n",
      "After Conv3D(64): (None, 4, 8, 257, 64)\n",
      "After MaxPooling3D(pool_size=(1, 1, 2)): (None, 4, 8, 129, 64)\n",
      "After Conv3D(128): (None, 4, 8, 129, 128)\n",
      "After MaxPooling3D(pool_size=(1, 1, 2)): (None, 4, 8, 65, 128)\n",
      "After Dropout(0.5): (None, 4, 8, 65, 128)\n",
      "After Flatten: (None, 266240)\n",
      "After Dense(embed_dim): (None, 64)\n",
      "After Dense(num_classes): (None, 2)\n",
      "After Concatenate: (None, 66)\n",
      "After ExpandLayer for temporal dimension: (None, 1, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "After TemporalEncodingLayer 1: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "After TemporalEncodingLayer 2: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "After TemporalEncodingLayer 3: (None, None, 66)\n",
      "After GlobalAveragePooling1D: (None, 66)\n",
      "After Dense(66): (None, 66)\n",
      "After Dropout(0.5): (None, 66)\n",
      "Output shape: (None, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 11:35:10.335176: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 4296329856 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, 1, 66)7m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4990 - loss: 0.7410\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "transformer layer에 들어오는 input shape: (None, None, 66)\n",
      "transpose 이후 attention의 shape: (None, None, 2, 33)\n",
      "mhsa layer이후 shape: (None, None, 66)\n",
      "norm1 layer이후 shape: (None, None, 66)\n",
      "ffn의 input의 모양:(None, None, 66)\n",
      "dense1 layer이후:(None, None, 66)\n",
      "dense2 layer이후:(None, None, 66)\n",
      "ffn layer이후 shape: (None, None, 66)\n",
      "norm2 layer이후 shape: (None, None, 66)\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13ms/step - accuracy: 0.4990 - loss: 0.7410 - val_accuracy: 0.5688 - val_loss: 0.6800 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.6123 - loss: 0.6707 - val_accuracy: 0.7438 - val_loss: 0.5081 - learning_rate: 4.5000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.7658 - loss: 0.4937 - val_accuracy: 0.8233 - val_loss: 0.3766 - learning_rate: 4.0500e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.8380 - loss: 0.3722 - val_accuracy: 0.8643 - val_loss: 0.2996 - learning_rate: 3.6450e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9010 - loss: 0.2512 - val_accuracy: 0.8972 - val_loss: 0.2463 - learning_rate: 3.2805e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9376 - loss: 0.1778 - val_accuracy: 0.9094 - val_loss: 0.2347 - learning_rate: 2.9525e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9603 - loss: 0.1163 - val_accuracy: 0.9275 - val_loss: 0.2234 - learning_rate: 2.6572e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9758 - loss: 0.0764 - val_accuracy: 0.9331 - val_loss: 0.2156 - learning_rate: 2.3915e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9836 - loss: 0.0554 - val_accuracy: 0.9383 - val_loss: 0.1994 - learning_rate: 2.1523e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9882 - loss: 0.0421 - val_accuracy: 0.9423 - val_loss: 0.2282 - learning_rate: 1.9371e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9918 - loss: 0.0322 - val_accuracy: 0.9479 - val_loss: 0.1896 - learning_rate: 1.7434e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9921 - loss: 0.0293 - val_accuracy: 0.9449 - val_loss: 0.2351 - learning_rate: 1.5691e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9945 - loss: 0.0203 - val_accuracy: 0.9496 - val_loss: 0.1934 - learning_rate: 1.4121e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9942 - loss: 0.0213 - val_accuracy: 0.9519 - val_loss: 0.2092 - learning_rate: 1.2709e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9965 - loss: 0.0155 - val_accuracy: 0.9528 - val_loss: 0.2537 - learning_rate: 1.1438e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0120 - val_accuracy: 0.9516 - val_loss: 0.2440 - learning_rate: 1.0295e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9973 - loss: 0.0121 - val_accuracy: 0.9544 - val_loss: 0.2179 - learning_rate: 9.2651e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9977 - loss: 0.0121 - val_accuracy: 0.9535 - val_loss: 0.2106 - learning_rate: 8.3386e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9985 - loss: 0.0061 - val_accuracy: 0.9538 - val_loss: 0.2518 - learning_rate: 7.5047e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9990 - loss: 0.0055 - val_accuracy: 0.9562 - val_loss: 0.2313 - learning_rate: 6.7543e-05\n",
      "Epoch 21/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9990 - loss: 0.0055 - val_accuracy: 0.9498 - val_loss: 0.2844 - learning_rate: 6.0788e-05\n",
      "Epoch 22/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9989 - loss: 0.0047 - val_accuracy: 0.9589 - val_loss: 0.2245 - learning_rate: 5.4709e-05\n",
      "Epoch 23/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9993 - loss: 0.0032 - val_accuracy: 0.9582 - val_loss: 0.2298 - learning_rate: 4.9239e-05\n",
      "Epoch 24/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9992 - loss: 0.0040 - val_accuracy: 0.9562 - val_loss: 0.2630 - learning_rate: 4.4315e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9993 - loss: 0.0037 - val_accuracy: 0.9597 - val_loss: 0.2359 - learning_rate: 3.9883e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0031 - val_accuracy: 0.9592 - val_loss: 0.2625 - learning_rate: 3.5895e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0035 - val_accuracy: 0.9604 - val_loss: 0.2597 - learning_rate: 3.2305e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0014 - val_accuracy: 0.9593 - val_loss: 0.2874 - learning_rate: 2.9075e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0039 - val_accuracy: 0.9608 - val_loss: 0.2579 - learning_rate: 2.6167e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0017 - val_accuracy: 0.9608 - val_loss: 0.2929 - learning_rate: 2.3551e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9995 - loss: 0.0024 - val_accuracy: 0.9607 - val_loss: 0.2641 - learning_rate: 2.1196e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0019 - val_accuracy: 0.9609 - val_loss: 0.2881 - learning_rate: 1.9076e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9996 - loss: 0.0024 - val_accuracy: 0.9619 - val_loss: 0.2751 - learning_rate: 1.7168e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0015 - val_accuracy: 0.9612 - val_loss: 0.2856 - learning_rate: 1.5452e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0017 - val_accuracy: 0.9601 - val_loss: 0.3019 - learning_rate: 1.3906e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9996 - loss: 0.0023 - val_accuracy: 0.9610 - val_loss: 0.2966 - learning_rate: 1.2516e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0015 - val_accuracy: 0.9625 - val_loss: 0.2949 - learning_rate: 1.1264e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 9.7518e-04 - val_accuracy: 0.9618 - val_loss: 0.3081 - learning_rate: 1.0138e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9997 - loss: 0.0018 - val_accuracy: 0.9620 - val_loss: 0.2896 - learning_rate: 9.1240e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0017 - val_accuracy: 0.9615 - val_loss: 0.3009 - learning_rate: 8.2116e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0025 - val_accuracy: 0.9619 - val_loss: 0.3026 - learning_rate: 7.3904e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 5.2322e-04 - val_accuracy: 0.9620 - val_loss: 0.3221 - learning_rate: 6.6514e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9999 - loss: 0.0014 - val_accuracy: 0.9632 - val_loss: 0.3205 - learning_rate: 5.9863e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0015 - val_accuracy: 0.9627 - val_loss: 0.3343 - learning_rate: 3.9276e-06\n",
      "Epoch 48/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 7.5600e-04 - val_accuracy: 0.9625 - val_loss: 0.3395 - learning_rate: 3.5348e-06\n",
      "Epoch 49/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 0.0013 - val_accuracy: 0.9628 - val_loss: 0.3390 - learning_rate: 3.1813e-06\n",
      "Epoch 50/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 6.3784e-04 - val_accuracy: 0.9628 - val_loss: 0.3347 - learning_rate: 2.8632e-06\n",
      "Epoch 51/100\n",
      "\u001b[1m3254/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 9.4694e-04 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9580 - val_loss: 0.4386 - learning_rate: 8.2116e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0011 - val_accuracy: 0.9581 - val_loss: 0.4426 - learning_rate: 7.3904e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m4090/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9574 - val_loss: 0.4495 - learning_rate: 6.6514e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m4039/4090\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9998 - loss: 8.2527e-04 "
     ]
    }
   ],
   "source": [
    "def test_model(data_dir, model_path):\n",
    "    X, y = load_data(data_dir)\n",
    "\n",
    "    # 모델 아키텍처를 재구성합니다.\n",
    "    model = build_model(\n",
    "        input_shape=X.shape[1:],  # 데이터 형태에 맞춤\n",
    "        num_classes=2,\n",
    "        embed_dim=64,\n",
    "        num_heads=2,    #2(o) 3(x) 6(o)\n",
    "        ffn_units=128\n",
    "    )\n",
    "\n",
    "    # 저장된 가중치를 불러옵니다.\n",
    "    model.load_weights(model_path)\n",
    "\n",
    "    # 입력 데이터의 차원이 모델에 맞는지 확인\n",
    "    input_shape = model.input_shape[1:]\n",
    "    if X.shape[1:] != input_shape:\n",
    "        print(f\"Adjusting test data from {X.shape[1:]} to {input_shape}...\")\n",
    "        X = tf.reshape(X, (-1,) + input_shape)\n",
    "\n",
    "    # 예측 수행\n",
    "    preds = model.predict(X)\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "\n",
    "    # 분류 리포트 생성 및 출력\n",
    "    report = classification_report(y, y_pred, target_names=[\"Negative\", \"Positive\"])\n",
    "    print(\"Test Results:\")\n",
    "    print(report)\n",
    "\n",
    "    # 분류 보고서 저장\n",
    "    report_path = \"4s_test_classification_inter_report.txt\"\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(f\"Classification report saved to {report_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "data_directory = \"/home/bcml1/2025_EMOTION/DEAP_EEG/overlap4s_seg_conv_ch_BPF+DE/train_overlap0.125\"  # Data path\n",
    "model_save_file = \"4s_0.125overlap_inter_model.h5\"\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate_kfold(data_directory, model_save_file, k=5)\n",
    "\n",
    "# Test\n",
    "test_directory = \"/home/bcml1/2025_EMOTION/DEAP_EEG/overlap4s_seg_conv_ch_BPF+DE/test_overlap0.125\"  # Test data path\n",
    "test_model(test_directory, model_save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05987664-4a44-4287-aa7c-566c2b16cfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mycondaenv)",
   "language": "python",
   "name": "mycondaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
