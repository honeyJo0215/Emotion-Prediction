# inter subject multi-modal transformer(ë‡ŒíŒŒ+eye crop video)(cross ëª¨ë‹¬ë¦¬í‹° ì‚¬ìš© í›„ concat)
# (Cross-Modality ì‚¬ìš© í›„ Concat)

# ëª¨ë‹¬ë¦¬í‹° ì ìš© ë°©ì‹: EEG â†” Eye Crop Video ê°„ Cross-Attention í•™ìŠµ í›„ Concatenation
# Inter-Modality ì²˜ë¦¬ ë°©ì‹: EEGì™€ Eye Feature ê°„ Query-Key-Value Attentionì„ ì ìš©í•˜ì—¬ ìƒí˜¸ìž‘ìš© í›„ í•©ì¹¨
# Concat ì‹œì : Inter-Modality Attention (Cross-Attention) í›„ Concatenation
# íŠ¹ì§• ê°•ì¡° ë°©ì‹: EEG â†” Eye Video ê°„ ìƒê´€ì„±ì„ í•™ìŠµí•˜ì—¬ ì¤‘ìš”í•œ ì •ë³´ êµí™˜ í›„ íŠ¹ì§• ê°•í™”

# EEGì™€ Eye Crop Video ë°ì´í„°ì—ì„œ ê°ê° íŠ¹ì§•ì„ ì¶”ì¶œí•œ í›„,
# EEGì˜ Queryì™€ Eyeì˜ Key/Value, ê·¸ë¦¬ê³  Eyeì˜ Queryì™€ EEGì˜ Key/Valueë¥¼ ì´ìš©í•œ Cross-Attentionì„ í†µí•´ ë‘ ëª¨ë‹¬ë¦¬í‹° ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.
# ê·¸ í›„, Cross-Attention ê²°ê³¼ë¥¼ Concatenationí•˜ì—¬ Fusioní•˜ê³ , ìµœì¢… ë¶„ë¥˜ì— í™œìš©í•©ë‹ˆë‹¤.

import os
import re
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Conv3D, BatchNormalization, Dropout, LayerNormalization, Lambda, GlobalAveragePooling1D, Reshape
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# =============================================================================
# 0. GPU ë©”ëª¨ë¦¬ ì œí•œ (í•„ìš” ì‹œ)
# =============================================================================
def limit_gpu_memory(memory_limit_mib=8000):
    """Limit TensorFlow GPU memory usage to the specified amount (in MiB)."""
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0],
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit_mib)]
            )
            print(f"GPU memory limited to {memory_limit_mib} MiB.")
        except RuntimeError as e:
            print(e)
    else:
        print("No GPU available, using CPU.")

limit_gpu_memory(8000)

# ---------------------------------
# 1. ë°ì´í„° ê²½ë¡œ ë° ê°ì • ë¼ë²¨ ë§¤í•‘
# ---------------------------------

EEG_DATA_PATH = "/home/bcml1/2025_EMOTION/DEAP_EEG_2D_DE"
EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
SAVE_PATH = "/home/bcml1/sigenv/_multimodal_video+eeg/result_stand_modelinter_cross"
os.makedirs(SAVE_PATH, exist_ok=True)

# ê°ì • ë¼ë²¨ ë§¤í•‘
EMOTION_MAPPING = {
    "Excited": 0,
    "Relaxed": 1,
    "Stressed": 2,
    "Bored": 3,
    "Neutral": 4
}

# =============================================================================
# 1. EEG Branch ê´€ë ¨ ëª¨ë“ˆ
# =============================================================================
# 1-1. Spatial-Spectral Convolution Module
class SpatialSpectralConvModule(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides):
        super(SpatialSpectralConvModule, self).__init__()
        self.spatial_conv = Conv3D(filters, kernel_size=(1, 3, 3), strides=strides, padding="same", activation="relu")
        self.spectral_conv = Conv3D(filters, kernel_size=(4, 1, 1), strides=strides, padding="same", activation="relu")

    def call(self, inputs):
        if len(inputs.shape) == 4:
            inputs = tf.expand_dims(inputs, axis=-1)
        spatial_features = self.spatial_conv(inputs)
        spectral_features = self.spectral_conv(inputs)
        return spatial_features + spectral_features
    
    def get_config(self):
        return {
            "filters": self.filters,
            "kernel_size": self.kernel_size,
            "strides": self.strides,
        }

# 1-2. Spatial and Spectral Attention Branch
class SpatialSpectralAttention(tf.keras.layers.Layer):
    def __init__(self):
        super(SpatialSpectralAttention, self).__init__()
        self.spatial_squeeze = Conv3D(1, kernel_size=(1, 1, 1), activation="sigmoid")
        self.spectral_squeeze = Conv3D(1, kernel_size=(1, 1, 1), activation="sigmoid")

    def call(self, inputs):
        if len(inputs.shape) == 4:
            inputs = tf.expand_dims(inputs, axis=-1)

        # Spatial attention
        spatial_mask = self.spatial_squeeze(inputs)
        spatial_output = inputs * spatial_mask

        # Spectral attention
        spectral_mask = self.spectral_squeeze(inputs)
        spectral_output = inputs * spectral_mask

        # Combine spatial and spectral outputs
        combined_output = spatial_output + spectral_output
        return combined_output

    def get_config(self):
        return {}

# 1-3. EEG CNN ëª¨ë¸ (EEGì˜ ê³µê°„-ì£¼íŒŒìˆ˜-ì‹œê°„ì  íŠ¹ì§• ì¶”ì¶œ)
class EEGCNN(tf.keras.Model):
    def __init__(self, d_model):
        super(EEGCNN, self).__init__()
        self.attention = SpatialSpectralAttention()
        self.conv1 = SpatialSpectralConvModule(8, kernel_size=(1,3,3), strides=(1,3,3))
        self.conv2 = SpatialSpectralConvModule(16, kernel_size=(4,1,1), strides=(4,1,1))
        self.conv3 = SpatialSpectralConvModule(32, kernel_size=(1,2,2), strides=(1,2,2))
        self.flatten = Flatten()
        self.dense_proj = Dense(d_model, activation="relu")
    def call(self, inputs):
        x = self.attention(inputs)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.flatten(x)
        x = self.dense_proj(x)
        return x

# =============================================================================
# 2. Eye Crop Video Branch ê´€ë ¨ ëª¨ë“ˆ
# =============================================================================
def build_eye_crop_model(d_model=64):
    """
    Eye Crop Video ë°ì´í„° (ì˜ˆ: 50í”„ë ˆìž„, 8x64, 3ì±„ë„)ë¥¼ ìž…ë ¥ìœ¼ë¡œ ë°›ì•„
    Conv3Dì™€ BatchNormalizationì„ í†µí•´ ì§€ì—­ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , Dense layerë¡œ d_model ì°¨ì›ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
    """
    eye_input = Input(shape=(50, 8, 64, 3))  # ìž…ë ¥ í¬ê¸° (í”„ë ˆìž„ ìˆ˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)
    x = Reshape((50, 8, 64, 3))(eye_input)
    x = Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(d_model, activation='relu')(x)
    x = Dropout(0.3)(x)
    return Model(eye_input, x, name="EyeCrop_CNN")
    
# =============================================================================
# 3. Transformer Encoder ë° Cross-Attention ëª¨ë“ˆ
# =============================================================================
# 3-1. Transformer Encoder Layer (Self-Attention ê¸°ë°˜)
class TransformerEncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)
        self.ffn = tf.keras.Sequential([
            Dense(d_ff, activation=tf.nn.gelu),
            Dense(d_model)
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(dropout_rate)
        self.dropout2 = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        if len(inputs.shape) == 2:  # (batch, features)
            inputs = tf.expand_dims(inputs, axis=1)  # (batch, 1, features)
        
        query, key, value = inputs, inputs, inputs  # ê°™ì€ ìž…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ self-attention ìˆ˜í–‰
        attn_output = self.mha(query=query, key=key, value=value, training=training)
        
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)  # Residual Connection

        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)  # ìµœì¢… Residual Connection

# 3-2. Transformer Encoder (ì—¬ëŸ¬ ë ˆì´ì–´ë¥¼ ìŒ“ìŒ)
def build_transformer_encoder(seq_length, n_layers=2, n_heads=4, d_ff=512, dropout_rate=0.1, d_model=64, name="Transformer_Encoder"):
    inputs = Input(shape=(seq_length, d_model))
    x = inputs
    for _ in range(n_layers):
        x = TransformerEncoderLayer(d_model=d_model, n_heads=n_heads, d_ff=d_ff, dropout_rate=dropout_rate)(x)
    return Model(inputs, x, name=name)

# ----------------------------------------------------
# 3. Inter â†’ Intra í•™ìŠµ êµ¬ì¡° êµ¬í˜„ (Cross)
# ----------------------------------------------------
def build_multimodal_model(eeg_input_shape, eye_input_shape=(50,8,64,3), seq_length=16, d_model=64):
    # ìž…ë ¥ ì •ì˜
    eeg_input = Input(shape=eeg_input_shape, name="EEG_Input")
    eye_input = Input(shape=eye_input_shape, name="Eye_Input")
    
    # ëª¨ë‹¬ë¦¬í‹°ë³„ Feature Extraction (CNN)
    f_eeg = EEGCNN(d_model=d_model)(eeg_input)           # (batch, d_model)
    f_eye = build_eye_crop_model(d_model=d_model)(eye_input)  # (batch, d_model)
    
    # ì‹œí€€ìŠ¤ë¡œ í™•ìž¥ (Tiling)
    tile_layer = Lambda(lambda x: tf.tile(tf.expand_dims(x, axis=1), [1, seq_length, 1]))
    eeg_seq = tile_layer(f_eeg)  # (batch, seq_length, d_model)
    eye_seq = tile_layer(f_eye)  # (batch, seq_length, d_model)
    
    # 1. Inter-Modality Attention (Cross-Attention ì ìš©)
    # EEGì˜ Query, Eyeì˜ Key/Value
    cross_eeg = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=d_model)(query=eeg_seq, key=eye_seq, value=eye_seq)
    # Eyeì˜ Query, EEGì˜ Key/Value
    cross_eye = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=d_model)(query=eye_seq, key=eeg_seq, value=eeg_seq)
    # Residual ì—°ê²°ê³¼ Layer Normalization ì ìš©
    eeg_cross = LayerNormalization(epsilon=1e-6)(eeg_seq + cross_eeg)
    eye_cross = LayerNormalization(epsilon=1e-6)(eye_seq + cross_eye)
    
    # 2. Intra-Modality Attention (ê° ëª¨ë‹¬ë¦¬í‹° ë‚´ë¶€ Self-Attention ì ìš©)
    transformer_eeg_intra = build_transformer_encoder(seq_length, n_layers=2, n_heads=4, d_ff=512, dropout_rate=0.1, d_model=d_model, name="Transformer_EEG_Intra")
    transformer_eye_intra = build_transformer_encoder(seq_length, n_layers=2, n_heads=4, d_ff=512, dropout_rate=0.1, d_model=d_model, name="Transformer_EYE_Intra")
    eeg_intra = transformer_eeg_intra(eeg_cross)   # (batch, seq_length, d_model)
    eye_intra = transformer_eye_intra(eye_cross)   # (batch, seq_length, d_model)
    
    # ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ìµœì¢… í‘œí˜„ì€ Global Average Poolingìœ¼ë¡œ ì§‘ê³„
    eeg_final = GlobalAveragePooling1D()(eeg_intra)  # (batch, d_model)
    eye_final = GlobalAveragePooling1D()(eye_intra)  # (batch, d_model)
    
    # 3. Concatenation & Classification
    fused = Concatenate(axis=-1)([eeg_final, eye_final])  # (batch, 2*d_model)
    x = Dense(128, activation="relu")(fused)
    x = Dropout(0.3)(x)
    x = Dense(64, activation="relu")(x)
    output = Dense(5, activation="softmax")(x)  # 5 ê°ì • í´ëž˜ìŠ¤
    
    # # ðŸš€ Mean Pooling (Transformer Sequence Length ì¶•ì†Œ)
    # output = Lambda(lambda x: tf.reduce_mean(x, axis=1))(output)  # (batch_size, num_classes)

    model = Model(inputs=[eeg_input, eye_input], outputs=output, name="Multimodal_CrossTransformer_Model")
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss="sparse_categorical_crossentropy", 
                  metrics=["accuracy"])
    
    return model

# ---------------------------------
# 4. Intra-subject Cross-Validation ì½”ë“œ (Sample ê¸°ì¤€)
# ---------------------------------

def find_subject_folder(base_path, subject):
    """ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ subject(s01, s02 ...)ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ì°¾ìŒ."""
    possible_folders = os.listdir(base_path)  # eye_crop ë‚´ í´ë” í™•ì¸
    for folder in possible_folders:
        if folder.lower() == subject.lower():  # ëŒ€ì†Œë¬¸ìž ë¬´ì‹œí•˜ê³  ë¹„êµ
            return os.path.join(base_path, folder)
    return None  # í•´ë‹¹ í´ë”ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°

# âœ… **ìž…ë ¥ ë°ì´í„° í¬ê¸° ì¤„ì´ê¸° (ë‹¤ìš´ìƒ˜í”Œë§)**
def downsample_eye_frame(frame):
    """Eye Crop ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ (64x32 â†’ 32x16)"""
    return cv2.resize(frame, (32,8), interpolation=cv2.INTER_AREA)  # í•´ìƒë„ ì ˆë°˜ ê°ì†Œ

# âœ… **Eye Crop ë°ì´í„° ë¡œë“œ ì‹œ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©**
def reshape_eye_frame(data):
    """
    (N, 32, 64, 3) í˜•íƒœì˜ eye frame ë°ì´í„°ë¥¼ (32, 64, 3)ìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©.
    - Nì´ 2 ì´ìƒì´ë©´ í‰ê· ì„ ë‚´ì„œ ë³‘í•©.
    - Nì´ 1ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """
    if len(data.shape) == 4 and data.shape[0] > 0:  
        reshaped_data = np.mean(data, axis=0)  # ëª¨ë“  ìš”ì†Œë¥¼ í‰ê·  ë‚´ì–´ ë³‘í•© (32, 64, 3)
        return downsample_eye_frame(reshaped_data)  # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
    elif len(data.shape) == 3 and data.shape == (32, 64, 3):  
        return downsample_eye_frame(data)  # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
    else:
        raise ValueError(f"Unexpected shape: {data.shape}, expected (N, 32, 64, 3) or (32, 64, 3)")

def load_multimodal_data(subject):
    eeg_data, eye_data, labels = [], [], []

    eeg_pattern = re.compile(rf"{subject}_sample_(\d+)_segment_(\d+)_label_(\w+)_2D_DE.npy")

    for sample_index in range(40):  # Sample 000 ~ Sample 040ê¹Œì§€ 40->3ë¡œ ìˆ˜ì •í•¨
        sample_number = f"{sample_index:02d}"  
        print(f"\nðŸŸ¢ Processing {subject} - Sample {sample_number}")

        eeg_files = [f for f in os.listdir(EEG_DATA_PATH) if eeg_pattern.match(f) and f"sample_{sample_number}" in f]
        if not eeg_files:
            print(f"ðŸš¨ No EEG file found for {subject} - Sample {sample_number}")
            continue

        for file_name in eeg_files:
            match = eeg_pattern.match(file_name)
            if not match:
                continue

            segment_index = int(match.group(2))
            emotion_label = match.group(3)

            if segment_index < 3:  
                continue
            segment_index -= 3  

            eeg_file_path = os.path.join(EEG_DATA_PATH, file_name)
            eeg_segment = np.load(eeg_file_path)

            eye_subject_path = os.path.join(EYE_CROP_PATH, subject)  
            if not os.path.exists(eye_subject_path):
                print(f"ðŸš¨ Subject folder not found: {eye_subject_path}")
                continue

            trial_number = sample_index + 1  

            start_frame = segment_index * 50
            end_frame = start_frame + 50

            print(f"  ðŸ”¹ Segment {segment_index}: Expected frames {start_frame} to {end_frame}, Matching Trial {trial_number:02d}")

            frame_indices = set()
            file_mapping = {}  

            for f in os.listdir(eye_subject_path):
                try:
                    if not f.startswith(subject) or not f.endswith(".npy"):
                        continue  

                    match = re.search(r"trial(\d+).avi_frame(\d+)", f)
                    if not match:
                        print(f"âš  Skipping invalid file name: {f} (No trial/frame pattern found)")
                        continue

                    file_trial_number = int(match.group(1))  
                    frame_number = int(match.group(2))  

                    if file_trial_number == trial_number:
                        frame_indices.add(frame_number)
                        file_mapping[frame_number] = os.path.join(eye_subject_path, f)

                except ValueError as e:
                    print(f"ðŸš¨ Error processing file {f}: {e}")
                    continue

            frame_indices = sorted(frame_indices)
            print(f"  ðŸ” Available frame indices: {frame_indices[:10]} ... {frame_indices[-10:]}")  

            selected_frames = sorted([frame for frame in frame_indices if start_frame <= frame < end_frame])

            if len(selected_frames) == 0:
                print(f"âš  Warning: No frames found for segment {segment_index}. Skipping Eye Crop.")
                eye_data.append(None)  # Eye Crop ë°ì´í„° ì—†ìŒ
                eeg_data.append(eeg_segment)  # EEG ë°ì´í„°ëŠ” ì¶”ê°€
                labels.append(EMOTION_MAPPING[emotion_label])  
                continue

            if len(selected_frames) < 50:
                print(f"âš  Warning: Found only {len(selected_frames)} frames for segment {segment_index}")
                while len(selected_frames) < 50:
                    selected_frames.append(selected_frames[-1])  # ë¶€ì¡±í•œ í”„ë ˆìž„ì„ ë§ˆì§€ë§‰ í”„ë ˆìž„ìœ¼ë¡œ ì±„ì›€
                    print("í”„ë ˆìž„ ë³µì œë¨")

            eye_frame_files = []
            for frame in selected_frames:
                if frame in file_mapping:
                    eye_frame_files.append(file_mapping[frame])
                if len(eye_frame_files) == 50:  
                    break

            eye_frame_stack = []
            for f in eye_frame_files:
                frame_data = np.load(f)  
                frame_data = reshape_eye_frame(frame_data)  
                # ðŸ“Œ ë§Œì•½ 32x64ë¡œ ë¡œë“œë˜ì—ˆë‹¤ë©´ 64x64ë¡œ ë§žì¶”ê¸° ìœ„í•´ padding ì ìš©
                if frame_data.shape[-2] == 32:  # ðŸš¨ ë„ˆë¹„ê°€ 32ì¸ ê²½ìš°
                    pad_width = [(0, 0)] * frame_data.ndim  # ê¸°ì¡´ shape ìœ ì§€
                    pad_width[-2] = (16, 16)  # ðŸš€ ë„ˆë¹„(32â†’64) í™•ìž¥
                    frame_data = np.pad(frame_data, pad_width, mode='constant', constant_values=0)
                eye_frame_stack.append(frame_data)

            if len(eye_frame_stack) == 50:
                eye_data.append(np.stack(eye_frame_stack, axis=0))  
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])  
            else:
                print(f"âš  Warning: Found only {len(eye_frame_stack)} matching frames for segment {segment_index}")

    print(f"âœ… Total EEG Samples Loaded: {len(eeg_data)}")
    print(f"âœ… Total Eye Crop Samples Loaded: {len([e for e in eye_data if e is not None])}")
    print(f"âœ… Labels Loaded: {len(labels)}")

    return np.array(eeg_data), np.array([e if e is not None else np.zeros((50, 8, 64, 3)) for e in eye_data]), np.array(labels)

# ðŸŸ¢ **í•™ìŠµ ë° í‰ê°€**
def train_multimodal():
    subjects = [f"s{str(i).zfill(2)}" for i in range(13, 23)]
    for subject in subjects:
        print(f"Training subject: {subject}")
        
        # ë°ì´í„° ë¡œë“œ
        eeg_data, eye_data, labels = load_multimodal_data(subject)

        # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ Train/Valid/Test ë°ì´í„° ë‚˜ëˆ„ê¸°
        unique_samples = np.arange(len(eeg_data))  
        train_samples, test_samples = train_test_split(unique_samples, test_size=0.2, random_state=42)
        train_samples, valid_samples = train_test_split(train_samples, test_size=0.2, random_state=42)

        # ìƒ˜í”Œ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
        train_eeg, train_eye, train_labels = eeg_data[train_samples], eye_data[train_samples], labels[train_samples]
        valid_eeg, valid_eye, valid_labels = eeg_data[valid_samples], eye_data[valid_samples], labels[valid_samples]
        test_eeg, test_eye, test_labels = eeg_data[test_samples], eye_data[test_samples], labels[test_samples]

        # ëª¨ë¸ì´ í•™ìŠµ ë„ì¤‘ OOMìœ¼ë¡œ ì¢…ë£Œë  ê²½ìš° ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ìž¥í•˜ê³  ìž¬ì‹œìž‘í•˜ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°ê°€ëŠ¥
        # ðŸš€ **ê° subject ë³„ ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ ê²½ë¡œ ì„¤ì •**
        checkpoint_dir = f"/home/bcml1/sigenv/_multimodal_video+eeg/checkpoint/cross_{subject}"
        checkpoint_path = os.path.join(checkpoint_dir, "cp.weights.h5")
        os.makedirs(checkpoint_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±

        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì • (ìžë™ ì €ìž¥)
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            save_weights_only=True,
            verbose=1
        )
        
        # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ êµ¬ì¶•
        model = build_multimodal_model(eeg_input_shape=train_eeg.shape[1:])
        print(model.summary())

        # ðŸš€ **ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìžˆë‹¤ë©´)**
        if os.path.exists(checkpoint_path + ".index"):
            print(f"âœ… Checkpoint found for {subject}, loading model...")
            model.load_weights(checkpoint_path)

        # ë¼ë²¨ ì°¨ì› í™•ìž¥
        train_labels = np.expand_dims(train_labels, axis=-1)
        valid_labels = np.expand_dims(valid_labels, axis=-1)

        # ðŸš€ **í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •**
        start_epoch = 0
        max_epochs = 50
        batch_size = 2
        max_retries = 3  # í•œ ì—í¬í¬ë‹¹ ìµœëŒ€ ìž¬ì‹œë„ íšŸìˆ˜

        # ì—í¬í¬ë³„ í•™ìŠµ
        for epoch in range(start_epoch, max_epochs):
            retries = 0
            while retries < max_retries:
                try:
                    print(f"\nðŸš€ Training {subject} - Epoch {epoch+1}/{max_epochs} (Retry: {retries})...")
                    model.fit(
                        [train_eeg, train_eye], train_labels,
                        validation_data=([valid_eeg, valid_eye], valid_labels),
                        epochs=1, batch_size=batch_size,
                        callbacks=[checkpoint_callback]
                    )
                    # ì—í¬í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ while ë£¨í”„ íƒˆì¶œ
                    break  
                except tf.errors.ResourceExhaustedError:
                    print(f"âš ï¸ OOM ë°œìƒ! ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ & ìž¬ì‹œìž‘ (Retry: {retries+1})...")
                    # ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ ì‹œ OOMì´ ë°œìƒí•  ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
                    try:
                        model.save_weights(checkpoint_path)
                    except tf.errors.ResourceExhaustedError:
                        print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ìž¥ ì¤‘ OOM ë°œìƒ - ì €ìž¥ ê±´ë„ˆëœ€.")
                    
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    tf.keras.backend.clear_session()
                    gc.collect()
                    
                    # ëª¨ë¸ ìž¬ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìžˆë‹¤ë©´)
                    model = build_multimodal_model(eeg_input_shape=train_eeg.shape[1:])
                    if os.path.exists(checkpoint_path + ".index"):
                        model.load_weights(checkpoint_path)
                    
                    retries += 1
                    # ìž¬ì‹œë„ ì „ì— ìž ì‹œ íœ´ì‹ (ì˜µì…˜)
                    tf.keras.backend.sleep(1)
            else:
                # ìµœëŒ€ ìž¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ì—í¬í¬ ì¢…ë£Œ ë° ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°.
                print(f"âŒ ì—í¬í¬ {epoch+1}ì—ì„œ ìµœëŒ€ ìž¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                break  # ë˜ëŠ” continueë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìžˆìŒ.

        # ðŸš€ **ìµœì¢… ëª¨ë¸ ì €ìž¥**
        subject_save_path = os.path.join(SAVE_PATH, subject)
        os.makedirs(subject_save_path, exist_ok=True)
        weight_path = os.path.join(subject_save_path, f"{subject}_multimodal_model.weights.h5")
        model.save_weights(weight_path)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ìž¥ë¨: {weight_path}")

        # ðŸš€ **í…ŒìŠ¤íŠ¸ í‰ê°€**
        predictions = model.predict([test_eeg, test_eye])
        predicted_labels = np.argmax(predictions, axis=-1)
        test_report = classification_report(
            test_labels, predicted_labels,
            target_names=["Excited", "Relaxed", "Stressed", "Bored", "Neutral"],
            labels=[0, 1, 2, 3, 4],
            zero_division=0
        )
        print(f"\nðŸ“Š Test Report for {subject}")
        print(test_report)

        report_path = os.path.join(subject_save_path, f"{subject}_test_report.txt")
        with open(report_path, "w") as f:
            f.write(test_report)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ìž¥ë¨: {report_path}")
        
if __name__ == "__main__":
    train_multimodal()