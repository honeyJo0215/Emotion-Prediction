#inter subject multi-modal transformer(ë‡ŒíŒŒ+eye crop video):ë‹¨ìˆœí•œ inter ëª¨ë¸

import os
import re
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Conv3D, BatchNormalization, Dropout, LayerNormalization, Lambda, GlobalAveragePooling1D
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# =============================================================================
# 0. GPU ë©”ëª¨ë¦¬ ì œí•œ (í•„ìš” ì‹œ)
# =============================================================================
def limit_gpu_memory(memory_limit_mib=1500):
    """Limit TensorFlow GPU memory usage to the specified amount (in MiB)."""
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0],
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit_mib)]
            )
            print(f"GPU memory limited to {memory_limit_mib} MiB.")
        except RuntimeError as e:
            print(e)
    else:
        print("No GPU available, using CPU.")

limit_gpu_memory(1500)

# ---------------------------------
# 1. ë°ì´í„° ê²½ë¡œ ë° ê°ì • ë¼ë²¨ ë§¤í•‘
# ---------------------------------

EEG_DATA_PATH = "/home/bcml1/2025_EMOTION/DEAP_EEG_2D_DE"
EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
SAVE_PATH = "/home/bcml1/sigenv/_multimodal_video+eeg/result_stand_inter_concat"
os.makedirs(SAVE_PATH, exist_ok=True)

# ê°ì • ë¼ë²¨ ë§¤í•‘
EMOTION_MAPPING = {
    "Excited": 0,
    "Relaxed": 1,
    "Stressed": 2,
    "Bored": 3,
    "Neutral": 4
}

# =============================================================================
# 1. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# =============================================================================
def preprocess_data(inputs):
    """
    ì…ë ¥ ë°ì´í„°ê°€ 4D (batch, D, H, W)ì¸ ê²½ìš° ì±„ë„ ì°¨ì› ì¶”ê°€.
    EEG ë°ì´í„°ê°€ ì´ë¯¸ 2D mappingëœ ê²½ìš° (4,6,6) í˜•íƒœë¼ë©´ ì±„ë„ ì°¨ì›ì„ ì¶”ê°€í•˜ì—¬ (4,6,6,1)ë¡œ ë§Œë“­ë‹ˆë‹¤.
    """
    if len(inputs.shape) == 4:
        inputs = tf.expand_dims(inputs, axis=-1)
    return inputs

# =============================================================================
# 2. ì œê³µëœ ëª¨ë“ˆë“¤
# =============================================================================
# 2-1. Spatial-Spectral Convolution Module
class SpatialSpectralConvModule(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides):
        super(SpatialSpectralConvModule, self).__init__()
        self.filters = filters
        self.kernel_size = kernel_size
        self.strides = strides
        self.spatial_conv = Conv3D(filters,
                                   kernel_size=(1, 3, 3),
                                   strides=strides,
                                   padding="same",
                                   activation="relu")
        self.spectral_conv = Conv3D(filters,
                                    kernel_size=(4, 1, 1),
                                    strides=strides,
                                    padding="same",
                                    activation="relu")

    def call(self, inputs):
        # ì…ë ¥ì´ 4Dë¼ë©´ (batch, D, H, W) -> (batch, D, H, W, 1)
        if len(inputs.shape) == 4:
            inputs = tf.expand_dims(inputs, axis=-1)
        spatial_features = self.spatial_conv(inputs)
        spectral_features = self.spectral_conv(inputs)
        return spatial_features + spectral_features

    def get_config(self):
        config = super().get_config().copy()
        config.update({
            "filters": self.filters,
            "kernel_size": self.kernel_size,
            "strides": self.strides,
        })
        return config

# 2-2. Spatial and Spectral Attention Branch
class SpatialSpectralAttention(tf.keras.layers.Layer):
    def __init__(self):
        super(SpatialSpectralAttention, self).__init__()
        self.spatial_squeeze = Conv3D(1, kernel_size=(1, 1, 1), activation="sigmoid")
        self.spectral_squeeze = Conv3D(1, kernel_size=(1, 1, 1), activation="sigmoid")

    def call(self, inputs):
        if len(inputs.shape) == 4:
            inputs = tf.expand_dims(inputs, axis=-1)
    
        spatial_mask = self.spatial_squeeze(inputs)
        spatial_output = inputs * spatial_mask

        spectral_mask = self.spectral_squeeze(inputs)
        spectral_output = inputs * spectral_mask

        combined_output = spatial_output + spectral_output
        return combined_output

    def get_config(self):
        return {}

# 2-3. Transformer Encoder Layer
class TransformerEncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_ff = d_ff
        self.dropout_rate = dropout_rate
        
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)
        self.ffn = tf.keras.Sequential([
            Dense(d_ff, activation=tf.nn.gelu),
            Dense(d_model)
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(dropout_rate)
        self.dropout2 = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        # ë§Œì•½ inputsê°€ (batch, d_model) shapeì´ë©´ (batch, 1, d_model)ë¡œ í™•ì¥í•©ë‹ˆë‹¤.
        if len(inputs.shape) == 2:
            inputs = tf.expand_dims(inputs, axis=1)
        attn_output = self.mha(inputs, inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
    
    def get_config(self):
        config = super().get_config().copy()
        config.update({
            "d_model": self.d_model,
            "n_heads": self.n_heads,
            "d_ff": self.d_ff,
            "dropout_rate": self.dropout_rate,
        })
        return config

# =============================================================================
# 3. ëª¨ë‹¬ë¦¬í‹°ë³„ Feature Extractor ë° Fusion ëª¨ë“ˆ
# =============================================================================
# 3-1. EEG Feature Extractor (Transformer Encoder ê¸°ë°˜)
class EEGTransformerEncoder(Model):
    def __init__(self, d_model=64, n_layers=6, n_heads=8, d_ff=2048, dropout_rate=0.5):
        """
        EEG ì…ë ¥ (DE feature mappingëœ 2D ë°ì´í„°)ì„ ë°›ì•„, Spatial-Spectral Attentionì™€ 3ê°œì˜ Conv3D ë¸”ë¡, 
        Flatten, Dense Projection í›„ ë‹¤ì¸µ Transformer Encoder Layerë¥¼ í†µê³¼ì‹œì¼œ feature vectorë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        super(EEGTransformerEncoder, self).__init__()
        self.attention = SpatialSpectralAttention()
        self.conv_block1 = SpatialSpectralConvModule(8, kernel_size=(1, 3, 3), strides=(1, 3, 3))
        self.conv_block2 = SpatialSpectralConvModule(16, kernel_size=(4, 1, 1), strides=(4, 1, 1))
        self.conv_block3 = SpatialSpectralConvModule(32, kernel_size=(1, 2, 2), strides=(1, 2, 2))
        self.flatten = Flatten()
        self.dense_projection = Dense(d_model, activation="relu")
        self.encoder_layers = [
            TransformerEncoderLayer(d_model, n_heads, d_ff, dropout_rate=dropout_rate)
            for _ in range(n_layers)
        ]
    
    def call(self, inputs, training=False):
        x = preprocess_data(inputs)  # (batch, 4,6,6,1)
        x = self.attention(x)
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.flatten(x)
        x = self.dense_projection(x)  # shape: (batch, d_model)
        # ê° Transformer layerëŠ” sequence ì…ë ¥ì„ ìš”êµ¬í•˜ë¯€ë¡œ (batch, 1, d_model)ë¡œ í™•ì¥ í›„ ì²˜ë¦¬í•˜ê³  ë‹¤ì‹œ squeeze í•©ë‹ˆë‹¤.
        for layer in self.encoder_layers:
            x = layer(tf.expand_dims(x, axis=1), training=training)
            x = tf.squeeze(x, axis=1)
        return x  # EEG feature vector (batch, d_model)

# 3-2. Eye Feature Extractor (ResNet50 ê¸°ë°˜)
class EyeFeatureExtractor(Model):
    def __init__(self):
        super(EyeFeatureExtractor, self).__init__()
        base_model = tf.keras.applications.ResNet50(include_top=False,
                                                    weights="imagenet",
                                                    input_shape=(64, 32, 3))
        self.feature_extractor = tf.keras.Sequential([
            base_model,
            GlobalAveragePooling2D()
        ])

    def call(self, x, training=False):
        return self.feature_extractor(x, training=training)

# 3-3. Fusion Module: EEGì™€ Eyeì˜ feature ë° Subject embeddingì„ ê²°í•© (Transformer Encoder Layer í™œìš©)
class FusionModule(Model):
    def __init__(self, d_model=64, n_heads=4, d_ff=256, dropout_rate=0.1, num_subjects=10):
        """
        inter-subject settingì„ ìœ„í•´ subject idë¥¼ Embeddingí•œ í† í°ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
        num_subjects: ì „ì²´ í”¼í—˜ì ìˆ˜
        """
        super(FusionModule, self).__init__()
        # Eye featureì˜ ì°¨ì›ì„ EEG featureì™€ ë§ì¶”ê¸° ìœ„í•œ projection layer
        self.eye_projection = Dense(d_model, activation="relu")
        self.transformer_layer = TransformerEncoderLayer(d_model, n_heads, d_ff, dropout_rate)
        self.classifier = Dense(5, activation="softmax")
        # subject idë¥¼ ì„ë² ë”©í•˜ì—¬ d_model ì°¨ì›ì˜ í† í°ìœ¼ë¡œ ë³€í™˜
        self.subject_embedding = tf.keras.layers.Embedding(input_dim=num_subjects, output_dim=d_model)
    
    def call(self, eeg_features, eye_features, subject_ids, training=False):
        eye_proj = self.eye_projection(eye_features)  # (batch, d_model)
        subject_token = self.subject_embedding(subject_ids)  # (batch, d_model)
        # ë‘ ëª¨ë‹¬ë¦¬í‹°ì™€ subject í† í°ì„ sequenceì˜ ì„¸ í† í°ìœ¼ë¡œ ì·¨ê¸‰í•©ë‹ˆë‹¤.
        fusion_seq = tf.stack([eeg_features, eye_proj, subject_token], axis=1)  # shape: (batch, 3, d_model)
        fusion_out = self.transformer_layer(fusion_seq, training=training)
        # ì„¸ í† í°ì— ëŒ€í•´ í‰ê· (pooling)í•˜ì—¬ ë‹¨ì¼ feature vectorë¡œ ê²°í•©
        fusion_feature = tf.reduce_mean(fusion_out, axis=1)  # (batch, d_model)
        logits = self.classifier(fusion_feature)
        return logits

# =============================================================================
# 4. ìµœì¢… ë©€í‹°ëª¨ë‹¬ ê°ì • ì¸ì‹ ëª¨ë¸ (Inter-Subject)
# =============================================================================
class MultiModalEmotionModel(Model):
    def __init__(self, d_model=64, eeg_n_layers=6, eeg_n_heads=8, eeg_d_ff=2048, eeg_dropout=0.5,
                 fusion_n_heads=4, fusion_d_ff=256, fusion_dropout=0.1, num_subjects=10):
        """
        EEG, Eye, ê·¸ë¦¬ê³  subject idë¥¼ ì…ë ¥ë°›ì•„ íŠ¹ì§•ì„ ì¶”ì¶œ í›„ ìœµí•©í•˜ì—¬ ê°ì •ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
        num_subjects: ì „ì²´ í”¼í—˜ì ìˆ˜ (inter-subject setting)
        """
        super(MultiModalEmotionModel, self).__init__()
        self.eeg_extractor = EEGTransformerEncoder(d_model, eeg_n_layers, eeg_n_heads, eeg_d_ff, eeg_dropout)
        self.eye_extractor = EyeFeatureExtractor()
        self.fusion_module = FusionModule(d_model, fusion_n_heads, fusion_d_ff, fusion_dropout, num_subjects)
    
    def call(self, eeg, eye, subject_ids, training=False):
        eeg_features = self.eeg_extractor(eeg, training=training)
        eye_features = self.eye_extractor(eye, training=training)
        logits = self.fusion_module(eeg_features, eye_features, subject_ids, training=training)
        return logits

# # =============================================================================
# # 5. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# # =============================================================================

# # -------------------------------
# # 5-0. ì „ì—­ ìƒìˆ˜ ë° ê°ì • ë¼ë²¨ ë§¤í•‘
# # -------------------------------
# EEG_DATA_PATH = "/home/bcml1/sigenv/_1ì£¼ì°¨_data_preprocess/test_DEAP_EEG_de_features_2D_mapping"
# EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
# EMOTION_MAPPING = {
#     "Excited": 0,
#     "Relaxed": 1,
#     "Stressed": 2,
#     "Bored": 3,
#     "Neutral": 4
# }

# # -------------------------------
# # 5-1. EEG ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (2D mapping DE feature)
# # -------------------------------
# def load_subject_eeg_data(subject, data_path=EEG_DATA_PATH):
#     """
#     ì£¼ì–´ì§„ subject(ì˜ˆ: "s10")ì— ëŒ€í•´, data_path ë‚´ì˜ íŒŒì¼ëª… íŒ¨í„´ì— ë”°ë¼ EEG ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
#     íŒŒì¼ëª… ì˜ˆ: "s10_sample_XX_segment_003_label_Excited_2D.npy"
#     ì„¸ê·¸ë¨¼íŠ¸ "000", "001", "002"ëŠ” ê±´ë„ˆë›°ê³ , ë‚˜ë¨¸ì§€ì— ëŒ€í•´ ì±„ë„ ì°¨ì›ì„ ì¶”ê°€í•˜ì—¬ (4,6,6,1) ëª¨ì–‘ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
#     """
#     data, labels = [], []
#     # íŒŒì¼ëª…ì„ ì •ë ¬í•˜ì—¬ ì¼ê´€ëœ ìˆœì„œë¡œ ë¡œë“œ
#     files = sorted([f for f in os.listdir(data_path) if f.startswith(subject) and f.endswith("_2D.npy")])
#     for file_name in files:
#         parts = file_name.split("_")
#         if len(parts) < 7:
#             print(f"Unexpected file format: {file_name}")
#             continue
#         segment_name = parts[4]
#         if segment_name in ["000", "001", "002"]:
#             continue  # ê±´ë„ˆë›°ê¸°
#         emotion_label = parts[-2]
#         if emotion_label in EMOTION_MAPPING:
#             label = EMOTION_MAPPING[emotion_label]
#             file_path = os.path.join(data_path, file_name)
#             try:
#                 de_features = np.load(file_path)  # (4,6,6)
#                 de_features = np.expand_dims(de_features, axis=-1)  # (4,6,6,1)
#                 data.append(de_features)
#                 labels.append(label)
#             except Exception as e:
#                 print(f"Error loading EEG file {file_path}: {e}")
#         else:
#             print(f"Unknown emotion label: {emotion_label} in file {file_name}")
#     if len(data) == 0:
#         print(f"Warning: No EEG data found for {subject}")
#     return np.array(data), np.array(labels)

# # -------------------------------
# # 5-2. Eye Crop ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# # -------------------------------
# def downsample_eye_frame(frame):
#     """
#     ì›ë³¸ eye frame ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
#     cv2.resizeì˜ dsize ì¸ìëŠ” (width, height) ìˆœì„œì´ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” (32, 8)ë¡œ ì¶•ì†Œí•©ë‹ˆë‹¤.
#     """
#     return cv2.resize(frame, (32, 8), interpolation=cv2.INTER_AREA)

# def reshape_eye_frame(data):
#     """
#     ì…ë ¥ ë°ì´í„°ê°€ 4D (N, 32, 64, 3)ì¸ ê²½ìš°, Nê°œ í”„ë ˆì„ì— ëŒ€í•´ í‰ê· ì„ ë‚´ì–´ (32,64,3)ìœ¼ë¡œ ë§Œë“  í›„,
#     downsample_eye_frame()ì„ ì ìš©í•©ë‹ˆë‹¤.
#     ë§Œì•½ ì…ë ¥ì´ (32,64,3)ì´ë©´ ê·¸ëŒ€ë¡œ downsample_eye_frame() ì ìš©í•©ë‹ˆë‹¤.
#     """
#     if len(data.shape) == 4 and data.shape[0] > 0:
#         reshaped_data = np.mean(data, axis=0)  # (32,64,3)
#         return downsample_eye_frame(reshaped_data)  # ê²°ê³¼: (8,32,3)
#     elif len(data.shape) == 3 and data.shape == (32, 64, 3):
#         return downsample_eye_frame(data)
#     else:
#         raise ValueError(f"Unexpected eye frame shape: {data.shape}, expected (N,32,64,3) or (32,64,3)")

# # -------------------------------
# # 5-3. ê° í”¼í—˜ìë³„ Eye Crop ë°ì´í„° ë¡œë“œ í•¨ìˆ˜
# # -------------------------------
# def load_subject_eye_data(subject, eye_crop_path=EYE_CROP_PATH):
#     """
#     ì£¼ì–´ì§„ subject(ì˜ˆ: "s10")ì˜ eye crop ë°ì´í„°ë¥¼ eye_crop_path ë‚´ subject í´ë”ì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
#     íŒŒì¼ëª…ì€ í´ë” ë‚´ì˜ ëª¨ë“  .npy íŒŒì¼ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬í•œ í›„,  
#     50ê°œì”© ê·¸ë£¹ìœ¼ë¡œ ë¬¶ì–´ í•œ ìƒ˜í”Œ(ì‹œí€€ìŠ¤)ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
#     ê° í”„ë ˆì„ì€ reshape_eye_frame()ìœ¼ë¡œ ì „ì²˜ë¦¬í•œ í›„,  
#     ë§Œì•½ í”„ë ˆì„ì˜ ì„¸ë¡œ í¬ê¸°ê°€ 32ë¼ë©´ (8,32,3) â†’ íŒ¨ë”©ì„ í†µí•´ (8,64,3)ë¡œ ë§ì¶¥ë‹ˆë‹¤.
#     ìµœì¢… ìƒ˜í”Œ shape: (50, 8, 64, 3)
#     """
#     subject_folder = os.path.join(eye_crop_path, subject)
#     eye_samples = []
#     if not os.path.exists(subject_folder):
#         print(f"Eye crop folder not found for subject: {subject}")
#         return np.array(eye_samples)  # ë¹ˆ ë°°ì—´ ë°˜í™˜
#     # íŒŒì¼ëª…ì„ ì •ë ¬ (ìˆ«ì ë¶€ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ëŠ” ê²ƒì„ ê³ ë ¤)
#     eye_files = sorted([f for f in os.listdir(subject_folder) if f.endswith(".npy")])
#     # ê·¸ë£¹ ë‹¨ìœ„(50 í”„ë ˆì„ì”©)ë¡œ ë¬¶ê¸°
#     num_chunks = len(eye_files) // 50
#     for i in range(num_chunks):
#         chunk_files = eye_files[i*50:(i+1)*50]
#         frames = []
#         for f in chunk_files:
#             file_path = os.path.join(subject_folder, f)
#             try:
#                 frame_data = np.load(file_path)  # ì›ë³¸ shape ì˜ˆ: (N,32,64,3) ë˜ëŠ” (32,64,3)
#                 frame_data = reshape_eye_frame(frame_data)  # â†’ (8,32,3)
#             except Exception as e:
#                 print(f"Error processing eye file {file_path}: {e}")
#                 continue
#             # ë§Œì•½ í”„ë ˆì„ì˜ ì„¸ë¡œ í¬ê¸°ê°€ 32ì´ë©´, (8,32,3) â†’ (8,64,3)ë¡œ íŒ¨ë”© ì ìš©
#             if frame_data.shape[-2] == 32:
#                 pad_width = [(0, 0)] * frame_data.ndim
#                 pad_width[-2] = (16, 16)  # ì–‘ìª½ì— 16í”½ì…€ì”© ì¶”ê°€ â†’ 32+16+16 = 64
#                 frame_data = np.pad(frame_data, pad_width, mode='constant', constant_values=0)
#             frames.append(frame_data)
#         # ê°œì„ : 50ê°œ ë¯¸ë§Œì˜ í”„ë ˆì„ì´ ìˆë‹¤ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ 50ê°œë¡œ ì±„ì›€
#         if len(frames) > 0 and len(frames) < 50:
#             while len(frames) < 50:
#                 frames.append(frames[-1])
#         if len(frames) == 50:
#             sample_eye = np.stack(frames, axis=0)  # (50, 8, 64, 3)
#             eye_samples.append(sample_eye)
#         else:
#             print(f"Warning: Unable to form a complete sample (50 frames) for subject {subject}. Skipping this sample.")
#     return np.array(eye_samples)

# # -------------------------------
# # 5-4. Inter-subject ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í†µí•© ë¡œë“œ í•¨ìˆ˜
# # -------------------------------
# def load_inter_subject_data(subjects, eeg_data_path=EEG_DATA_PATH, eye_crop_path=EYE_CROP_PATH):
#     """
#     ì—¬ëŸ¬ í”¼í—˜ì(subjects ëª©ë¡)ì— ëŒ€í•´ EEGì™€ Eye Crop ë°ì´í„°ë¥¼ ê°ê° ë¡œë“œí•˜ê³ ,
#     subject idì™€ ë¼ë²¨ì„ í•¨ê»˜ êµ¬ì„±í•©ë‹ˆë‹¤.
#     ìµœì¢… ë°˜í™˜ ë°ì´í„°:
#       - eeg_data: (total_samples, 4, 6, 6, 1)
#       - eye_data: (total_samples, 50, 8, 64, 3)
#       - subject_ids: (total_samples,) ì •ìˆ˜í˜•
#       - labels: (total_samples,)
#     """
#     all_eeg, all_eye, all_subject_ids, all_labels = [], [], [], []
#     for subject in subjects:
#         # EEG ë°ì´í„° ë¡œë“œ
#         eeg_data, labels = load_subject_eeg_data(subject, data_path=eeg_data_path)
#         if eeg_data.size == 0:
#             continue
#         # Eye Crop ë°ì´í„° ë¡œë“œ
#         eye_data = load_subject_eye_data(subject, eye_crop_path=eye_crop_path)
#         if eye_data.size == 0:
#             print(f"Warning: No eye crop data found for {subject}. Creating dummy eye data.")
#             # dummy: ê° EEG ìƒ˜í”Œë‹¹ (50,8,64,3) ì˜ìƒì„ ëª¨ë‘ 0ìœ¼ë¡œ ì±„ì›€
#             dummy_eye = np.zeros((eeg_data.shape[0], 50, 8, 64, 3), dtype=np.uint8)
#             eye_data = dummy_eye
#         # EEGì™€ Eye ë°ì´í„° ìƒ˜í”Œ ìˆ˜ ë§ì¶”ê¸°
#         num_samples = min(eeg_data.shape[0], eye_data.shape[0])
#         if eeg_data.shape[0] != eye_data.shape[0]:
#             print(f"Warning: Mismatch in number of samples for {subject}: EEG={eeg_data.shape[0]}, Eye={eye_data.shape[0]}. Using {num_samples} samples.")
#             eeg_data = eeg_data[:num_samples]
#             eye_data = eye_data[:num_samples]
#             labels = labels[:num_samples]
#         all_eeg.append(eeg_data)
#         all_eye.append(eye_data)
#         # subject id: "sXX"ì—ì„œ XXë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (ì˜ˆ: "s10" â†’ 10)
#         all_subject_ids.append(np.full((num_samples,), int(subject[1:])))
#         all_labels.append(labels)
#     if len(all_eeg) == 0:
#         raise ValueError("No data loaded for any subject!")
#     # ë¦¬ìŠ¤íŠ¸ë“¤ì„ concatenate
#     all_eeg = np.concatenate(all_eeg, axis=0)
#     all_eye = np.concatenate(all_eye, axis=0)
#     all_subject_ids = np.concatenate(all_subject_ids, axis=0)
#     all_labels = np.concatenate(all_labels, axis=0)
#     print("ìµœì¢… EEG data shape:", all_eeg.shape)
#     print("ìµœì¢… Eye data shape:", all_eye.shape)
#     print("ìµœì¢… Subject IDs shape:", all_subject_ids.shape)
#     print("ìµœì¢… Labels shape:", all_labels.shape)
#     return all_eeg, all_eye, all_subject_ids, all_labels

# # -------------------------------
# # 5-5. ì „ì²´ ë°ì´í„° ë¡œë“œ ì˜ˆì‹œ (Inter-Subject)
# # -------------------------------
# # ì‹¤ì œë¡œ subjects ëª©ë¡ì„ ì •ì˜í•˜ê³  ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# subjects = [f"s{str(i).zfill(2)}" for i in range(10, 23)]
# eeg_data, eye_data, subject_ids, labels = load_inter_subject_data(subjects, EEG_DATA_PATH, EYE_CROP_PATH)

# # =============================================================================
# # 6. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# # =============================================================================
# model = MultiModalEmotionModel(num_subjects=23)  # num_subjectsëŠ” ì‹¤ì œ í”¼í—˜ì ìˆ˜ì— ë§ê²Œ ì„¤ì •

# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
#               loss=tf.keras.losses.SparseCategoricalCrossentropy(),
#               metrics=["accuracy"])

# # ëª¨ë¸ í•™ìŠµ
# model.fit([eeg_data, eye_data, subject_ids], labels, epochs=10, batch_size=16)

# loss, accuracy = model.evaluate([eeg_data, eye_data, subject_ids], labels)
# print(f"Test Accuracy: {accuracy}")

# ---------------------------------
# 4. Intra-subject Cross-Validation ì½”ë“œ (Sample ê¸°ì¤€)
# ---------------------------------

def find_subject_folder(base_path, subject):
    """ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ subject(s01, s02 ...)ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ì°¾ìŒ."""
    possible_folders = os.listdir(base_path)  # eye_crop ë‚´ í´ë” í™•ì¸
    for folder in possible_folders:
        if folder.lower() == subject.lower():  # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë¹„êµ
            return os.path.join(base_path, folder)
    return None  # í•´ë‹¹ í´ë”ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°

# âœ… **ì…ë ¥ ë°ì´í„° í¬ê¸° ì¤„ì´ê¸° (ë‹¤ìš´ìƒ˜í”Œë§)**
def downsample_eye_frame(frame):
    """Eye Crop ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ (64x32 â†’ 32x16)"""
    return cv2.resize(frame, (32,8), interpolation=cv2.INTER_AREA)  # í•´ìƒë„ ì ˆë°˜ ê°ì†Œ

# âœ… **Eye Crop ë°ì´í„° ë¡œë“œ ì‹œ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©**
def reshape_eye_frame(data):
    """
    (N, 32, 64, 3) í˜•íƒœì˜ eye frame ë°ì´í„°ë¥¼ (32, 64, 3)ìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©.
    - Nì´ 2 ì´ìƒì´ë©´ í‰ê· ì„ ë‚´ì„œ ë³‘í•©.
    - Nì´ 1ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """
    if len(data.shape) == 4 and data.shape[0] > 0:  
        reshaped_data = np.mean(data, axis=0)  # ëª¨ë“  ìš”ì†Œë¥¼ í‰ê·  ë‚´ì–´ ë³‘í•© (32, 64, 3)
        return downsample_eye_frame(reshaped_data)  # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
    elif len(data.shape) == 3 and data.shape == (32, 64, 3):  
        return downsample_eye_frame(data)  # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
    else:
        raise ValueError(f"Unexpected shape: {data.shape}, expected (N, 32, 64, 3) or (32, 64, 3)")


#build_multimodal_data êµ¬í˜„í•´ì•¼í•¨.



def load_multimodal_data(subject):
    eeg_data, eye_data, labels = [], [], []

    eeg_pattern = re.compile(rf"{subject}_sample_(\d+)_segment_(\d+)_label_(\w+)_2D_DE.npy")

    for sample_index in range(40):  # Sample 000 ~ Sample 040ê¹Œì§€
        sample_number = f"{sample_index:02d}"  
        print(f"\nğŸŸ¢ Processing {subject} - Sample {sample_number}")

        eeg_files = [f for f in os.listdir(EEG_DATA_PATH) if eeg_pattern.match(f) and f"sample_{sample_number}" in f]
        if not eeg_files:
            print(f"ğŸš¨ No EEG file found for {subject} - Sample {sample_number}")
            continue

        for file_name in eeg_files:
            match = eeg_pattern.match(file_name)
            if not match:
                continue

            segment_index = int(match.group(2))
            emotion_label = match.group(3)

            if segment_index < 3:  
                continue
            segment_index -= 3  

            eeg_file_path = os.path.join(EEG_DATA_PATH, file_name)
            eeg_segment = np.load(eeg_file_path)

            eye_subject_path = os.path.join(EYE_CROP_PATH, subject)  
            if not os.path.exists(eye_subject_path):
                print(f"ğŸš¨ Subject folder not found: {eye_subject_path}")
                continue

            trial_number = sample_index + 1  

            start_frame = segment_index * 50
            end_frame = start_frame + 50

            print(f"  ğŸ”¹ Segment {segment_index}: Expected frames {start_frame} to {end_frame}, Matching Trial {trial_number:02d}")

            frame_indices = set()
            file_mapping = {}  

            for f in os.listdir(eye_subject_path):
                try:
                    if not f.startswith(subject) or not f.endswith(".npy"):
                        continue  

                    match = re.search(r"trial(\d+).avi_frame(\d+)", f)
                    if not match:
                        print(f"âš  Skipping invalid file name: {f} (No trial/frame pattern found)")
                        continue

                    file_trial_number = int(match.group(1))  
                    frame_number = int(match.group(2))  

                    if file_trial_number == trial_number:
                        frame_indices.add(frame_number)
                        file_mapping[frame_number] = os.path.join(eye_subject_path, f)

                except ValueError as e:
                    print(f"ğŸš¨ Error processing file {f}: {e}")
                    continue

            frame_indices = sorted(frame_indices)
            print(f"  ğŸ” Available frame indices: {frame_indices[:10]} ... {frame_indices[-10:]}")  

            selected_frames = sorted([frame for frame in frame_indices if start_frame <= frame < end_frame])

            if len(selected_frames) == 0:
                print(f"âš  Warning: No frames found for segment {segment_index}. Skipping Eye Crop.")
                eye_data.append(None)  # Eye Crop ë°ì´í„° ì—†ìŒ
                eeg_data.append(eeg_segment)  # EEG ë°ì´í„°ëŠ” ì¶”ê°€
                labels.append(EMOTION_MAPPING[emotion_label])  
                continue

            if len(selected_frames) < 50:
                print(f"âš  Warning: Found only {len(selected_frames)} frames for segment {segment_index}")
                while len(selected_frames) < 50:
                    selected_frames.append(selected_frames[-1])  # ë¶€ì¡±í•œ í”„ë ˆì„ì„ ë§ˆì§€ë§‰ í”„ë ˆì„ìœ¼ë¡œ ì±„ì›€
                    print("í”„ë ˆì„ ë³µì œë¨")

            eye_frame_files = []
            for frame in selected_frames:
                if frame in file_mapping:
                    eye_frame_files.append(file_mapping[frame])
                if len(eye_frame_files) == 50:  
                    break

            eye_frame_stack = []
            for f in eye_frame_files:
                frame_data = np.load(f)  
                frame_data = reshape_eye_frame(frame_data)  
                # ğŸ“Œ ë§Œì•½ 32x64ë¡œ ë¡œë“œë˜ì—ˆë‹¤ë©´ 64x64ë¡œ ë§ì¶”ê¸° ìœ„í•´ padding ì ìš©
                if frame_data.shape[-2] == 32:  # ğŸš¨ ë„ˆë¹„ê°€ 32ì¸ ê²½ìš°
                    pad_width = [(0, 0)] * frame_data.ndim  # ê¸°ì¡´ shape ìœ ì§€
                    pad_width[-2] = (16, 16)  # ğŸš€ ë„ˆë¹„(32â†’64) í™•ì¥
                    frame_data = np.pad(frame_data, pad_width, mode='constant', constant_values=0)
                eye_frame_stack.append(frame_data)

            if len(eye_frame_stack) == 50:
                eye_data.append(np.stack(eye_frame_stack, axis=0))  
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])  
            else:
                print(f"âš  Warning: Found only {len(eye_frame_stack)} matching frames for segment {segment_index}")

    print(f"âœ… Total EEG Samples Loaded: {len(eeg_data)}")
    print(f"âœ… Total Eye Crop Samples Loaded: {len([e for e in eye_data if e is not None])}")
    print(f"âœ… Labels Loaded: {len(labels)}")

    return np.array(eeg_data), np.array([e if e is not None else np.zeros((50, 8, 64, 3)) for e in eye_data]), np.array(labels)

# ğŸŸ¢ **í•™ìŠµ ë° í‰ê°€**
def train_multimodal():
    subjects = [f"s{str(i).zfill(2)}" for i in range(1, 23)]
    for subject in subjects:
        print(f"Training subject: {subject}")
        
        # ë°ì´í„° ë¡œë“œ
        eeg_data, eye_data, labels = load_multimodal_data(subject)

        # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ Train/Valid/Test ë°ì´í„° ë‚˜ëˆ„ê¸°
        unique_samples = np.arange(len(eeg_data))  
        train_samples, test_samples = train_test_split(unique_samples, test_size=0.2, random_state=42)
        train_samples, valid_samples = train_test_split(train_samples, test_size=0.2, random_state=42)

        # ìƒ˜í”Œ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
        train_eeg, train_eye, train_labels = eeg_data[train_samples], eye_data[train_samples], labels[train_samples]
        valid_eeg, valid_eye, valid_labels = eeg_data[valid_samples], eye_data[valid_samples], labels[valid_samples]
        test_eeg, test_eye, test_labels = eeg_data[test_samples], eye_data[test_samples], labels[test_samples]

        # ëª¨ë¸ì´ í•™ìŠµ ë„ì¤‘ OOMìœ¼ë¡œ ì¢…ë£Œë  ê²½ìš° ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ê³  ì¬ì‹œì‘í•˜ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°ê°€ëŠ¥
        # ğŸš€ **ê° subject ë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ ì„¤ì •**
        checkpoint_dir = f"/home/bcml1/sigenv/_multimodal_video+eeg/checkpoint/formal_{subject}"
        checkpoint_path = os.path.join(checkpoint_dir, "cp.weights.h5")
        os.makedirs(checkpoint_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±

        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì • (ìë™ ì €ì¥)
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            save_weights_only=True,
            verbose=1
        )
        
        # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ êµ¬ì¶•
        model = build_multimodal_model(eeg_input_shape=train_eeg.shape[1:])
        print(model.summary())

        # ğŸš€ **ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆë‹¤ë©´)**
        if os.path.exists(checkpoint_path + ".index"):
            print(f"âœ… Checkpoint found for {subject}, loading model...")
            model.load_weights(checkpoint_path)

        # ë¼ë²¨ ì°¨ì› í™•ì¥
        train_labels = np.expand_dims(train_labels, axis=-1)
        valid_labels = np.expand_dims(valid_labels, axis=-1)

        # ğŸš€ **í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •**
        start_epoch = 0
        max_epochs = 50
        batch_size = 2
        max_retries = 3  # í•œ ì—í¬í¬ë‹¹ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        # ì—í¬í¬ë³„ í•™ìŠµ
        for epoch in range(start_epoch, max_epochs):
            retries = 0
            while retries < max_retries:
                try:
                    print(f"\nğŸš€ Training {subject} - Epoch {epoch+1}/{max_epochs} (Retry: {retries})...")
                    model.fit(
                        [train_eeg, train_eye], train_labels,
                        validation_data=([valid_eeg, valid_eye], valid_labels),
                        epochs=1, batch_size=batch_size,
                        callbacks=[checkpoint_callback]
                    )
                    # ì—í¬í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ while ë£¨í”„ íƒˆì¶œ
                    break  
                except tf.errors.ResourceExhaustedError:
                    print(f"âš ï¸ OOM ë°œìƒ! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ & ì¬ì‹œì‘ (Retry: {retries+1})...")
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ OOMì´ ë°œìƒí•  ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
                    try:
                        model.save_weights(checkpoint_path)
                    except tf.errors.ResourceExhaustedError:
                        print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ OOM ë°œìƒ - ì €ì¥ ê±´ë„ˆëœ€.")
                    
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    tf.keras.backend.clear_session()
                    gc.collect()
                    
                    # ëª¨ë¸ ì¬ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆë‹¤ë©´)
                    model = build_multimodal_model(eeg_input_shape=train_eeg.shape[1:])
                    if os.path.exists(checkpoint_path + ".index"):
                        model.load_weights(checkpoint_path)
                    
                    retries += 1
                    # ì¬ì‹œë„ ì „ì— ì ì‹œ íœ´ì‹ (ì˜µì…˜)
                    tf.keras.backend.sleep(1)
            else:
                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ì—í¬í¬ ì¢…ë£Œ ë° ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°.
                print(f"âŒ ì—í¬í¬ {epoch+1}ì—ì„œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                break  # ë˜ëŠ” continueë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆìŒ.

        # ğŸš€ **ìµœì¢… ëª¨ë¸ ì €ì¥**
        subject_save_path = os.path.join(SAVE_PATH, subject)
        os.makedirs(subject_save_path, exist_ok=True)
        weight_path = os.path.join(subject_save_path, f"{subject}_multimodal_model.weights.h5")
        model.save_weights(weight_path)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ë¨: {weight_path}")

        # ğŸš€ **í…ŒìŠ¤íŠ¸ í‰ê°€**
        predictions = model.predict([test_eeg, test_eye])
        predicted_labels = np.argmax(predictions, axis=-1)
        test_report = classification_report(
            test_labels, predicted_labels,
            target_names=["Excited", "Relaxed", "Stressed", "Bored", "Neutral"],
            labels=[0, 1, 2, 3, 4],
            zero_division=0
        )
        print(f"\nğŸ“Š Test Report for {subject}")
        print(test_report)

        report_path = os.path.join(subject_save_path, f"{subject}_test_report.txt")
        with open(report_path, "w") as f:
            f.write(test_report)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_path}")
        
if __name__ == "__main__":
    train_multimodal()