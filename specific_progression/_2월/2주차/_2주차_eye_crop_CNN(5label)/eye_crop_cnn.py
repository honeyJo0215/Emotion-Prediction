import os
import re
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, Conv3D, BatchNormalization, LayerNormalization, MultiHeadAttention, Lambda
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# =============================================================================
# GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
# =============================================================================
def limit_gpu_memory(memory_limit_mib=8000):
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit_mib)]
            )
            print(f"GPU memory limited to {memory_limit_mib} MiB.")
        except RuntimeError as e:
            print(e)
    else:
        print("No GPU available, using CPU.")

limit_gpu_memory(8000)

# =============================================================================
# ë°ì´í„° ê²½ë¡œ ì„¤ì •
# =============================================================================
EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
NUMERIC_LABELS_PATH = "/home/bcml1/2025_EMOTION/DEAP_numeric_labels"
SAVE_PATH = "/home/bcml1/myenv/DEAP_EyeCrop_Transformer"
os.makedirs(SAVE_PATH, exist_ok=True)

# =============================================================================
# Transformer Encoder Layer ì •ì˜
# =============================================================================
class TransformerEncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model=64, num_heads=4, d_ff=128, dropout_rate=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)
        self.ffn = tf.keras.Sequential([
            Dense(d_ff, activation='relu'),
            Dense(d_model)
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(dropout_rate)
        self.dropout2 = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        attn_output = self.mha(inputs, inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# =============================================================================
# Eye Crop CNN + Transformer ëª¨ë¸ ì •ì˜
# =============================================================================
def build_eye_crop_transformer_model(d_model=64, num_transformer_layers=2):
    eye_input = Input(shape=(50, 8, 64, 3))  # (í”„ë ˆì„ ìˆ˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)
    
    # CNN ê¸°ë°˜ Feature Extraction
    x = Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='same')(eye_input)
    x = BatchNormalization()(x)
    x = Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(d_model, activation='relu')(x)
    x = Dropout(0.3)(x)

    # Transformer ì ìš© (Self-Attention) - Lambda ì‚¬ìš©í•˜ì—¬ expand_dims ì ìš©
    x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)  # (batch, 1, d_model)
    
    for _ in range(num_transformer_layers):
        x = TransformerEncoderLayer(d_model=d_model)(x)
    
    x = Flatten()(x)
    x = Dense(128, activation="relu")(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation="relu")(x)
    x = Dense(5, activation="softmax")(x)  # ê°ì • í´ë˜ìŠ¤ 5ê°œ
    
    model = Model(eye_input, x, name="EyeCrop_Transformer_Model")
    
    # ğŸš€ **ëª¨ë¸ ì»´íŒŒì¼ ì¶”ê°€**
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=["accuracy"]
    )
    
    return model


# =============================================================================
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (Numeric Labelê³¼ Eye Crop ë§¤ì¹­)
# =============================================================================
def find_best_eye_frames(frame_data, file_name):
    """
    ì£¼ì–´ì§„ frame_dataì—ì„œ ì •ìƒì ì¸ eye crop í”„ë ˆì„ 2ê°œë¥¼ ì„ íƒí•˜ì—¬ ë°˜í™˜.
    - frame_data.shape[0] (í”„ë ˆì„ ê°œìˆ˜)ê°€ 2ê°œë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    - frame_data.shape[0]ì´ 2ê°œ ì´ìƒì´ë©´, ê°€ì¥ ìœ ì‚¬í•œ ë‘ í”„ë ˆì„ì„ ì„ íƒ.
    """
    num_frames = frame_data.shape[0]

    if num_frames == 2:
        return frame_data  # ì´ë¯¸ ì •ìƒì ì¸ í¬ê¸°

    print(f"âš  {file_name}: Selecting best 2 eye crop frames from {num_frames} frames.")

    # (1) í”„ë ˆì„ ê°„ ì°¨ì´ ê³„ì‚°í•˜ì—¬ ê°€ì¥ ìœ ì‚¬í•œ í”„ë ˆì„ ì°¾ê¸°
    distances = []
    for i in range(num_frames - 1):
        diff = np.linalg.norm(frame_data[i] - frame_data[i + 1])  # í”„ë ˆì„ ê°„ ì°¨ì´ ê³„ì‚°
        distances.append((i, i + 1, diff))

    # (2) ì°¨ì´ê°€ ê°€ì¥ ì‘ì€ ë‘ í”„ë ˆì„ì„ ì„ íƒ (ì—°ì†ì ì¸ í”„ë ˆì„ ì¤‘ ìœ ì‚¬í•œ ê²ƒ)
    distances.sort(key=lambda x: x[2])  # ì°¨ì´ê°€ ì‘ì€ ìˆœìœ¼ë¡œ ì •ë ¬
    best_pair = distances[0][:2]  # ê°€ì¥ ì°¨ì´ê°€ ì ì€ í”„ë ˆì„ ë‘ ê°œ ì„ íƒ
    selected_frames = frame_data[list(best_pair)]

    print(f"âœ… {file_name}: Best frames selected - Index {best_pair[0]}, {best_pair[1]}")
    return selected_frames


def fix_eye_crop_shape(frame_data, file_name):
    """
    - ì •ìƒì ì¸ ë°ì´í„° (`(2, 32, 64, 3)`)ëŠ” `(8, 64, 3)`ìœ¼ë¡œ ë³€í™˜.
    - í”„ë ˆì„ ê°œìˆ˜ê°€ 2ê°œ ì´ìƒì¼ ê²½ìš°, ê°€ì¥ ì ì ˆí•œ 2ê°œ í”„ë ˆì„ì„ ì„ íƒ í›„ ë³€í™˜.
    - í”„ë ˆì„ì´ 1ê°œì¸ ê²½ìš°, ë³µì œí•˜ì—¬ 2ê°œë¡œ ë§ì¶˜ í›„ ë³€í™˜.
    """
    num_frames = frame_data.shape[0]

    # âœ… ì •ìƒì ì¸ ë°ì´í„° `(2, 32, 64, 3)`
    if num_frames == 2:
        print(f"âœ… {file_name}: ì •ìƒì ì¸ Eye Crop ë°ì´í„°. Expanding to (8, 64, 3).")
        return np.repeat(frame_data, repeats=4, axis=0).reshape(8, 64, 3)  # (8, 64, 3)ë¡œ ë³€í™˜

    # âœ… ì˜ëª» í¬ë¡­ëœ ë°ì´í„° ì²˜ë¦¬ (N â‰  2)
    print(f"âš  Warning: {file_name} has unexpected shape {frame_data.shape}. Adjusting to (8, 64, 3).")

    # (1) **ì •ìƒì ì¸ ë‘ ê°œì˜ í”„ë ˆì„ ì°¾ê¸°**
    frame_data = find_best_eye_frames(frame_data, file_name)

    # (2) âœ… `reshape(8, 64, 3)`ì„ ìœ„í•œ ë³€í™˜
    expanded_data = np.repeat(frame_data, repeats=4, axis=0)  # 2ê°œ í”„ë ˆì„ â†’ 8ê°œë¡œ í™•ì¥
    return expanded_data.reshape(8, 64, 3)

def load_and_check_npy(file_path):
    try:
        data = np.load(file_path)

        # âœ… ë°ì´í„° ì°¨ì› í™•ì¸
        if data.ndim == 1:
            print(f"ğŸš¨ Warning: {file_path} is a 1D array ({data.shape[0],}). Reshaping...")
            expected_size = 32 * 64 * 3
            if data.shape[0] % expected_size == 0:
                num_frames = data.shape[0] // expected_size
                data = data.reshape(num_frames, 32, 64, 3)
                print(f"âœ… Reshaped {file_path} to ({num_frames}, 32, 64, 3).")
            else:
                print(f"ğŸš¨ Error: {file_path} has incorrect size {data.shape[0]}!")
                return None  # ë¡œë”© ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
        return data

    except Exception as e:
        print(f"ğŸš¨ Error loading {file_path}: {e}")
        return None



def load_eye_crop_data(subject):
    eye_data, labels = [], []

    # Numeric Label íŒŒì¼ ì°¾ê¸°
    label_files = [f for f in os.listdir(NUMERIC_LABELS_PATH) if f.startswith(subject)]
    if not label_files:
        print(f"ğŸš¨ Numeric labels not found for {subject}")
        return np.array([]), np.array([])

    label_file_path = os.path.join(NUMERIC_LABELS_PATH, label_files[0])
    numeric_labels = np.load(label_file_path, allow_pickle=True)  # (40,) í˜•íƒœì˜ ë°°ì—´

    # Eye Crop ë°ì´í„° ê²½ë¡œ ì„¤ì •
    subject_path = os.path.join(EYE_CROP_PATH, subject)
    if not os.path.exists(subject_path):
        print(f"ğŸš¨ Subject í´ë” ì—†ìŒ: {subject_path}")
        return np.array([]), np.array([])

    print(f"\nğŸŸ¢ Loading eye_crop data for {subject}...")

    trial_pattern = re.compile(rf"{subject}_trial(\d+).avi_frame(\d+).npy")

    trial_frames = {}
    for file_name in sorted(os.listdir(subject_path)):
        match = trial_pattern.match(file_name)
        if not match:
            continue

        trial_number = int(match.group(1))

        # Trial ë²ˆí˜¸ê°€ 0~39ì¸ ê²½ìš°ë§Œ ì‚¬ìš©
        if trial_number >= 40:
            continue

        if trial_number not in trial_frames:
            trial_frames[trial_number] = []

        file_path = os.path.join(subject_path, file_name)

        # âœ… ë¹ˆ íŒŒì¼ ê²€ì‚¬ (ë¹„ì–´ ìˆìœ¼ë©´ ê±´ë„ˆë›°ê¸°)
        if os.path.getsize(file_path) == 0:
            print(f"âš  Warning: Skipping empty file {file_path}")
            continue

        try:
            frame_data = np.load(file_path)

            # âœ… ë°ì´í„° ì†ìƒ ê²€ì‚¬
            if frame_data is None or frame_data.size == 0:
                print(f"ğŸš¨ Error: File {file_path} is corrupted. Skipping...")
                continue

            # âœ… í¬ê¸° ì¡°ì •
            frame_data = fix_eye_crop_shape(frame_data, file_name)

            trial_frames[trial_number].append(frame_data)

            trial_frames[trial_number].append(frame_data)

        except Exception as e:
            print(f"ğŸš¨ Error loading {file_path}: {e}")
            continue

    # âœ… 50ê°œ í”„ë ˆì„ì„ ë§ì¶”ê³  `stack()` ì ìš©
    for trial_number, frames in trial_frames.items():
        frames = [np.array(f).reshape(8, 64, 3) for f in frames]  # ğŸš€ ëª¨ë“  í”„ë ˆì„ì„ (8,64,3)ìœ¼ë¡œ ë³€í™˜

        frames = frames[:50]  # **50ê°œ ì´ˆê³¼ ì‹œ ì²˜ìŒ 50ê°œë§Œ ì‚¬ìš©**

        while len(frames) < 50:  # **50ê°œ ë¯¸ë§Œì´ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ì„ ë³µì œí•˜ì—¬ 50ê°œë¡œ ë§ì¶¤**
            if len(frames) > 0:
                frames.append(frames[-1])  # ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì œ
            else:
                frames.append(np.zeros((8, 64, 3)))  # ğŸš€ `zeros` ì¶”ê°€

        try:
            trial_array = np.stack(frames).reshape(50, 8, 64, 3)  # ğŸš€ `stack()` í˜¸ì¶œ ì „ ëª¨ë“  í¬ê¸° ë™ì¼ í™•ì¸
            eye_data.append(trial_array)
            labels.append(numeric_labels[trial_number])
        except ValueError:
            print(f"ğŸš¨ Error: Cannot stack frames for trial {trial_number}. Skipping...")
            continue

    print(f"âœ… Loaded {len(eye_data)} samples from {subject}.")
    return np.array(eye_data), np.array(labels)

# =============================================================================
# í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜
# =============================================================================
def train_eye_crop_transformer():
    subjects = [f"s{str(i).zfill(2)}" for i in range(1, 23)]  # s01 ~ s22

    for subject in subjects:
        print(f"\n===== Training subject: {subject} =====")

        eye_data, labels = load_eye_crop_data(subject)
        if len(eye_data) == 0 or len(labels) == 0:
            print(f"âš  No data found for {subject}. Skipping...")
            continue

        train_idx, test_idx = train_test_split(np.arange(len(eye_data)), test_size=0.2, random_state=42)
        train_idx, valid_idx = train_test_split(train_idx, test_size=0.2, random_state=42)

        train_eye, train_labels = eye_data[train_idx], labels[train_idx]
        valid_eye, valid_labels = eye_data[valid_idx], labels[valid_idx]
        test_eye, test_labels = eye_data[test_idx], labels[test_idx]

        model = build_eye_crop_transformer_model()
        print(model.summary())

        # ğŸš€ ëª¨ë¸ í•™ìŠµ
        model.fit(
            train_eye, train_labels,
            validation_data=(valid_eye, valid_labels),
            epochs=50, batch_size=16
        )

        # ğŸš€ í…ŒìŠ¤íŠ¸ í‰ê°€
        predictions = model.predict(test_eye)
        predicted_labels = np.argmax(predictions, axis=-1)

        # âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ ê°ì§€
        unique_labels = np.unique(test_labels)  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ ì°¾ê¸°
        class_names = ["Excited", "Relaxed", "Stressed", "Bored", "Neutral"]
        valid_class_names = [class_names[i] for i in unique_labels]  # âœ… ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë§Œ ì‚¬ìš©

        test_report = classification_report(
            test_labels, predicted_labels,
            target_names=valid_class_names,  # âœ… ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
            labels=unique_labels  # âœ… ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë§Œ ì‚¬ìš©
        )

        report_path = os.path.join(SAVE_PATH, subject, f"{subject}_test_report.txt")
        os.makedirs(os.path.dirname(report_path), exist_ok=True)

        with open(report_path, "w") as f:
            f.write(test_report)

        print(f"âœ… Test report saved for {subject}: {report_path}")

# =============================================================================
# ì‹¤í–‰
# =============================================================================
if __name__ == "__main__":
    train_eye_crop_transformer()



#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Eye ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•˜ëŠ” ëª¨ë¸

- Eye ë°ì´í„°ë¥¼ CNNìœ¼ë¡œ í”¼ì²˜ ì¶”ì¶œí•œ í›„ Transformer Encoderë¡œ ìœµí•©
- Positional Encodingì„ í†µí•´ ì‹œí€€ìŠ¤ ìˆœì„œ ì •ë³´ë¥¼ ë°˜ì˜
- CNN ëª¨ë¸ì— BatchNormalizationê³¼ Dropoutì„ ì¶”ê°€í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- Test ë°ì´í„° ë¶„í• ì€ sample ê¸°ì¤€ ê·¸ëŒ€ë¡œ ìœ ì§€

ê° ì„¹ì…˜ì€ í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

import os
import re
import cv2
import gc
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Lambda, Reshape, Input, Dense, Flatten, Conv3D, BatchNormalization, Dropout, LayerNormalization
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# =============================================================================
# GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (í•„ìš” ì‹œ ìˆ˜ì •)
# =============================================================================
def limit_gpu_memory(memory_limit_mib=8000):
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            tf.config.experimental.set_virtual_device_configuration(
                gpus[0],
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit_mib)]
            )
            print(f"GPU memory limited to {memory_limit_mib} MiB.")
        except RuntimeError as e:
            print(e)
    else:
        print("No GPU available, using CPU.")

limit_gpu_memory(8000)

# =============================================================================
# ë°ì´í„° ê²½ë¡œ ë° ê°ì • ë¼ë²¨ ë§¤í•‘ (í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
# =============================================================================
EEG_DATA_PATH = "/home/bcml1/2025_EMOTION/DEAP_EEG_2D_DE"  # EEG ê´€ë ¨ ê²½ë¡œ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
SAVE_PATH = "/home/bcml1/sigenv/_2ì£¼ì°¨_eye_crop_CNN/eye_result_final"
os.makedirs(SAVE_PATH, exist_ok=True)

EMOTION_MAPPING = {
    "Excited": 0,
    "Relaxed": 1,
    "Stressed": 2,
    "Bored": 3,
    "Neutral": 4
}

# =============================================================================
# Positional Encoding
# =============================================================================
class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, sequence_length, d_model, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.sequence_length = sequence_length
        self.d_model = d_model

    def build(self, input_shape):
        self.pos_embedding = self.add_weight(
            name="pos_embedding",
            shape=(self.sequence_length, self.d_model),
            initializer="uniform",
            trainable=True
        )
        super(PositionalEncoding, self).build(input_shape)

    def call(self, inputs):
        # inputs: (batch, sequence_length, d_model)
        return inputs + self.pos_embedding

# =============================================================================
# Eye Crop CNN ëª¨ë¸
# =============================================================================
def build_eye_crop_model(d_model=64):
    eye_input = Input(shape=(50, 8, 64, 3))  # ì…ë ¥ í¬ê¸° (í”„ë ˆì„ ìˆ˜, ë†’ì´, ë„ˆë¹„, ì±„ë„)
    x = Reshape((50, 8, 64, 3))(eye_input)
    x = Conv3D(32, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Conv3D(64, kernel_size=(3, 3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(d_model, activation='relu')(x)
    x = Dropout(0.3)(x)
    return Model(eye_input, x, name="EyeCrop_CNN")

# =============================================================================
# Transformer Encoder Layer
# =============================================================================
class TransformerEncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, n_heads, d_ff, dropout_rate=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model)
        self.ffn = tf.keras.Sequential([
            Dense(d_ff, activation=tf.nn.gelu),
            Dense(d_model)
        ])
        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)
        self.dropout1 = Dropout(dropout_rate)
        self.dropout2 = Dropout(dropout_rate)

    def call(self, inputs, training=False):
        # Multi-Head Self-Attention
        attn_output = self.mha(query=inputs, key=inputs, value=inputs, training=training)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        # Feed Forward Network
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# =============================================================================
# Transformer Encoder ëª¨ë¸ (ì—¬ëŸ¬ ì¸µ ì ìš©)
# =============================================================================
def build_transformer_encoder(input_dim, n_layers=2, n_heads=4, d_ff=512,
                              dropout_rate=0.1, d_model=64, name="Transformer_Encoder"):
    inputs = Input(shape=(input_dim, d_model))
    x = inputs
    for _ in range(n_layers):
        x = TransformerEncoderLayer(d_model=d_model, n_heads=n_heads,
                                    d_ff=d_ff, dropout_rate=dropout_rate)(x)
    return Model(inputs, x, name=name)

# =============================================================================
# Eye Only ëª¨ë¸ êµ¬ì„±
# =============================================================================
def build_eye_only_model(eye_input_shape=(50, 8, 64, 3)):
    # ì…ë ¥ ì •ì˜ (Eye ë°ì´í„°ë§Œ ì‚¬ìš©)
    eye_input = Input(shape=eye_input_shape)
    
    # Eye Crop CNN ì ìš©
    eye_cnn_model = build_eye_crop_model(d_model=64)
    eye_features = eye_cnn_model(eye_input)  # (batch, 64)
    
    # ì‹œí€€ìŠ¤ í˜•íƒœë¡œ í™•ì¥ (ê¸¸ì´ 64, feature dimension 64)
    expand_dims_layer = Lambda(lambda x: tf.tile(x[:, None, :], [1, 64, 1]))
    eye_seq = expand_dims_layer(eye_features)  # (batch, 64, 64)
    
    # Positional Encoding ì ìš©
    eye_seq = PositionalEncoding(sequence_length=64, d_model=64)(eye_seq)
    
    # Transformer Encoder ì ìš© (Intra-modality)
    eye_transformer = build_transformer_encoder(input_dim=64, name="Transformer_Encoder_EYE")
    eye_transformed = eye_transformer(eye_seq)  # (batch, 64, 64)
    
    # Classification Head
    x = Dense(128, activation="relu")(eye_transformed)
    x = Dropout(0.3)(x)
    x = Dense(64, activation="relu")(x)
    x = Dense(5, activation="softmax")(x)
    # Mean Poolingìœ¼ë¡œ ì‹œí€€ìŠ¤ ì¶•ì†Œ â†’ ìµœì¢… ì¶œë ¥ (batch, 5)
    output = Lambda(lambda x: tf.reduce_mean(x, axis=1))(x)
    
    model = Model(inputs=eye_input, outputs=output, name="Eye_Only_Model")
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss="sparse_categorical_crossentropy", 
                  metrics=["accuracy"])
    return model

# =============================================================================
# ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¡œë“œ í•¨ìˆ˜ë“¤ (í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
# =============================================================================
def downsample_eye_frame(frame):
    """Eye Crop ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ (ì˜ˆ: 64x32 â†’ 32x8)"""
    return cv2.resize(frame, (32, 8), interpolation=cv2.INTER_AREA)

def reshape_eye_frame(data):
    """
    (N, 32, 64, 3) í˜•íƒœì˜ eye frame ë°ì´í„°ë¥¼ ì²˜ë¦¬.
    Nì´ 2 ì´ìƒì´ë©´ í‰ê· ë‚´ê³ , 1ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© í›„ ë‹¤ìš´ìƒ˜í”Œë§.
    """
    if len(data.shape) == 4 and data.shape[0] > 0:
        reshaped_data = np.mean(data, axis=0)
        return downsample_eye_frame(reshaped_data)
    elif len(data.shape) == 3 and data.shape == (32, 64, 3):
        return downsample_eye_frame(data)
    else:
        raise ValueError(f"Unexpected shape: {data.shape}, expected (N, 32, 64, 3) or (32, 64, 3)")

def load_multimodal_data(subject):
    """
    ì§€ì •í•œ subjectì˜ EEG ë° Eye Crop ë°ì´í„°ë¥¼ ë¡œë“œ.
    ë³¸ í•¨ìˆ˜ì—ì„œëŠ” EEG ë°ì´í„°ëŠ” ë¡œë“œí•˜ë‚˜, Eye Only ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ eye ë°ì´í„°ì™€ ë¼ë²¨ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    íŒŒì¼ëª… ë° ê²½ë¡œ ê·œì¹™ì€ ë°ì´í„°ì…‹ì— ë§ê²Œ ìˆ˜ì •.
    """
    eeg_data, eye_data, labels = [], [], []
    eeg_pattern = re.compile(rf"{subject}_sample_(\d+)_segment_(\d+)_label_(\w+)_2D_DE.npy")

    for sample_index in range(40):  # Sample 000 ~ Sample 039
        sample_number = f"{sample_index:02d}"
        print(f"\nğŸŸ¢ Processing {subject} - Sample {sample_number}")

        eeg_files = [f for f in os.listdir(EEG_DATA_PATH) if eeg_pattern.match(f) and f"sample_{sample_number}" in f]
        if not eeg_files:
            print(f"ğŸš¨ No EEG file found for {subject} - Sample {sample_number}")
            continue

        for file_name in eeg_files:
            match = eeg_pattern.match(file_name)
            if not match:
                continue

            segment_index = int(match.group(2))
            emotion_label = match.group(3)

            # ì´ˆê¸° ì„¸ê·¸ë¨¼íŠ¸ (3ê°œ ë¯¸ë§Œ) ë¬´ì‹œ
            if segment_index < 3:
                continue
            segment_index -= 3

            eeg_file_path = os.path.join(EEG_DATA_PATH, file_name)
            eeg_segment = np.load(eeg_file_path)

            eye_subject_path = os.path.join(EYE_CROP_PATH, subject)
            if not os.path.exists(eye_subject_path):
                print(f"ğŸš¨ Subject folder not found: {eye_subject_path}")
                continue

            trial_number = sample_index + 1
            start_frame = segment_index * 50
            end_frame = start_frame + 50

            print(f"  ğŸ”¹ Segment {segment_index}: Expected frames {start_frame} to {end_frame}, Matching Trial {trial_number:02d}")

            frame_indices = set()
            file_mapping = {}
            for f in os.listdir(eye_subject_path):
                try:
                    if not f.startswith(subject) or not f.endswith(".npy"):
                        continue
                    match = re.search(r"trial(\d+).avi_frame(\d+)", f)
                    if not match:
                        print(f"âš  Skipping invalid file name: {f}")
                        continue
                    file_trial_number = int(match.group(1))
                    frame_number = int(match.group(2))
                    if file_trial_number == trial_number:
                        frame_indices.add(frame_number)
                        file_mapping[frame_number] = os.path.join(eye_subject_path, f)
                except ValueError as e:
                    print(f"ğŸš¨ Error processing file {f}: {e}")
                    continue

            frame_indices = sorted(frame_indices)
            if frame_indices:
                print(f"  ğŸ” Available frame indices: {frame_indices[:10]} ... {frame_indices[-10:]}")
            else:
                print("  âš  No frame indices found.")

            selected_frames = sorted([frame for frame in frame_indices if start_frame <= frame < end_frame])
            if len(selected_frames) == 0:
                print(f"âš  Warning: No frames found for segment {segment_index}. Skipping Eye Crop.")
                eye_data.append(None)
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])
                continue

            if len(selected_frames) < 50:
                print(f"âš  Warning: Found only {len(selected_frames)} frames for segment {segment_index}")
                while len(selected_frames) < 50:
                    selected_frames.append(selected_frames[-1])
                    print("í”„ë ˆì„ ë³µì œë¨")

            eye_frame_files = []
            for frame in selected_frames:
                if frame in file_mapping:
                    eye_frame_files.append(file_mapping[frame])
                if len(eye_frame_files) == 50:
                    break

            eye_frame_stack = []
            for f in eye_frame_files:
                frame_data = np.load(f)
                frame_data = reshape_eye_frame(frame_data)
                # ë§Œì•½ í”„ë ˆì„ ë„ˆë¹„ê°€ 32ì´ë©´ íŒ¨ë”©ìœ¼ë¡œ 64ë¡œ í™•ì¥
                if frame_data.shape[-2] == 32:
                    pad_width = [(0, 0)] * frame_data.ndim
                    pad_width[-2] = (16, 16)
                    frame_data = np.pad(frame_data, pad_width, mode='constant', constant_values=0)
                eye_frame_stack.append(frame_data)

            if len(eye_frame_stack) == 50:
                eye_data.append(np.stack(eye_frame_stack, axis=0))
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])
            else:
                print(f"âš  Warning: Found only {len(eye_frame_stack)} matching frames for segment {segment_index}")

    print(f"âœ… Total EEG Samples Loaded: {len(eeg_data)}")
    print(f"âœ… Total Eye Crop Samples Loaded: {len([e for e in eye_data if e is not None])}")
    print(f"âœ… Labels Loaded: {len(labels)}")
    # None ê°’ì€ ëª¨ë‘ zerosë¡œ ëŒ€ì²´ (ëˆˆ ë°ì´í„°ë§Œ ì‚¬ìš©)
    eye_data = np.array([e if e is not None else np.zeros((50, 8, 64, 3)) for e in eye_data])
    return np.array(eeg_data), eye_data, np.array(labels)

# =============================================================================
# í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜ (Eye ë°ì´í„°ë§Œ ì‚¬ìš©)
# =============================================================================
def train_eye_only():
    subjects = [f"s{str(i).zfill(2)}" for i in range(1, 23)]
    
    for subject in subjects:
        print(f"\n===== Training subject: {subject} =====")

        # ë°ì´í„° ë¡œë“œ (EEG ë°ì´í„°ëŠ” ë¬´ì‹œ)
        _, eye_data, labels = load_multimodal_data(subject)

        # ìƒ˜í”Œ ë‹¨ìœ„ Train/Validation/Test ë¶„í• 
        unique_samples = np.arange(len(eye_data))
        train_samples, test_samples = train_test_split(unique_samples, test_size=0.2, random_state=42)
        train_samples, valid_samples = train_test_split(train_samples, test_size=0.2, random_state=42)

        train_eye, valid_eye, test_eye = eye_data[train_samples], eye_data[valid_samples], eye_data[test_samples]
        train_labels, valid_labels, test_labels = labels[train_samples], labels[valid_samples], labels[test_samples]

        # ê° subject ë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ ì„¤ì •
        checkpoint_dir = f"/home/bcml1/myenv/DEAP_EyeOnly_checkpoint/{subject}"
        checkpoint_path = os.path.join(checkpoint_dir, "cp.weights.h5")
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì • (ìë™ ì €ì¥)
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            save_weights_only=True,
            verbose=1
        )

        # ëª¨ë¸ ìƒì„± (Eye Only)
        model = build_eye_only_model(eye_input_shape=train_eye.shape[1:])
        print(model.summary())

        # ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆë‹¤ë©´)
        if os.path.exists(checkpoint_path + ".index"):
            print(f"âœ… Checkpoint found for {subject}, loading model...")
            model.load_weights(checkpoint_path)

        # ë¼ë²¨ ì°¨ì› í™•ì¥ (í•„ìš”í•œ ê²½ìš°)
        train_labels = np.expand_dims(train_labels, axis=-1)
        valid_labels = np.expand_dims(valid_labels, axis=-1)

        # í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •
        start_epoch = 0
        max_epochs = 4
        batch_size = 2
        max_retries = 5  # í•œ ì—í¬í¬ë‹¹ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        # ì—í¬í¬ë³„ í•™ìŠµ
        for epoch in range(start_epoch, max_epochs):
            retries = 0
            while retries < max_retries:
                try:
                    print(f"\nğŸš€ Training {subject} - Epoch {epoch+1}/{max_epochs} (Retry: {retries})...")
                    model.fit(
                        train_eye, train_labels,
                        validation_data=(valid_eye, valid_labels),
                        epochs=1, batch_size=batch_size,
                        callbacks=[checkpoint_callback]
                    )
                    # ì—í¬í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ while ë£¨í”„ íƒˆì¶œ
                    break  
                except tf.errors.ResourceExhaustedError:
                    print(f"âš ï¸ OOM ë°œìƒ! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ & ì¬ì‹œì‘ (Retry: {retries+1})...")
                    try:
                        model.save_weights(checkpoint_path)
                    except tf.errors.ResourceExhaustedError:
                        print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ OOM ë°œìƒ - ì €ì¥ ê±´ë„ˆëœ€.")
                    
                    tf.keras.backend.clear_session()
                    gc.collect()
                    
                    model = build_eye_only_model(eye_input_shape=train_eye.shape[1:])
                    if os.path.exists(checkpoint_path + ".index"):
                        model.load_weights(checkpoint_path)
                    
                    retries += 1
                    tf.keras.backend.sleep(1)
            else:
                print(f"âŒ ì—í¬í¬ {epoch+1}ì—ì„œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                break

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        subject_save_path = os.path.join(SAVE_PATH, subject)
        os.makedirs(subject_save_path, exist_ok=True)
        weight_path = os.path.join(subject_save_path, f"{subject}_eye_only_model.weights.h5")
        model.save_weights(weight_path)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ë¨: {weight_path}")

        # í…ŒìŠ¤íŠ¸ í‰ê°€
        predictions = model.predict(test_eye)
        predicted_labels = np.argmax(predictions, axis=-1)
        test_report = classification_report(
            test_labels, predicted_labels,
            target_names=["Excited", "Relaxed", "Stressed", "Bored", "Neutral"],
            labels=[0, 1, 2, 3, 4],
            zero_division=0
        )
        print(f"\nğŸ“Š Test Report for {subject}")
        print(test_report)

        report_path = os.path.join(subject_save_path, f"{subject}_test_report.txt")
        with open(report_path, "w") as f:
            f.write(test_report)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_path}")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================
if __name__ == "__main__":
    train_eye_only()
