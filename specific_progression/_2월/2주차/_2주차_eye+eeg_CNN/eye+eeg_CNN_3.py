# CNN ê¸°ë°˜ì˜ Dual-Stream Feature Extractor + Cross-Modal Transformer

import os
import re
import cv2  # downsample ì‹œ ì‚¬ìš©
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.layers import (
    Conv2D, Dense, Flatten, Dropout, AveragePooling2D, DepthwiseConv2D, LayerNormalization, 
    MultiHeadAttention, Reshape, Concatenate, GlobalAveragePooling2D, Input, TimeDistributed, 
    LSTM, Add, Dropout, Softmax, Lambda
)
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical

# ğŸ“Œ ê°ì • ë¼ë²¨ ë§¤í•‘
EMOTION_MAPPING = {
    "Negative": 0,
    "Positive": 1,
    "Neutral": 2
}

# ğŸ“Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
BATCH_SIZE = 8
EPOCHS = 50
LEARNING_RATE = 1e-4

# ğŸ“Œ ë°ì´í„° ê²½ë¡œ
EEG_DATA_PATH = "/home/bcml1/2025_EMOTION/DEAP_EEG_10s_1soverlap_labeled"
EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
SAVE_PATH = "/home/bcml1/sigenv/_2ì£¼ì°¨_eye+eeg_CNN/result3"
os.makedirs(SAVE_PATH, exist_ok=True)
SUBJECTS = [f"s{str(i).zfill(2)}" for i in range(1, 23)]  # í”¼ì‹¤í—˜ì 1~22ëª…

# -------------------------
# ğŸ“Œ Dual-Stream Feature Extractor
def create_dual_stream_feature_extractor():
    """
    EEGì™€ Eye Crop ë°ì´í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” Dual-Stream Feature Extractor
    """
    # ğŸ“Œ EEG Stream (32ì±„ë„, 1280 ìƒ˜í”Œ â†’ (32, 1280, 1))
    eeg_input = Input(shape=(32, 1280, 1), name="EEG_Input")
    x = Conv2D(16, kernel_size=(3,3), activation="relu", padding="same")(eeg_input)
    x = DepthwiseConv2D(kernel_size=(3,3), activation="relu", padding="same")(x)
    x = AveragePooling2D(pool_size=(2,2))(x)
    x = Conv2D(32, kernel_size=(3,3), activation="relu", padding="same")(x)
    x = Flatten()(x)
    eeg_output = Dense(128, activation="relu")(x)

    # ğŸ“Œ Eye Crop Stream (500, 8, 64, 3)
    eye_input = Input(shape=(100, 8, 64, 3), name="Eye_Input")
    eye_cnn = TimeDistributed(Conv2D(16, kernel_size=(1,16), activation="relu", padding="same"))(eye_input)
    eye_cnn = TimeDistributed(DepthwiseConv2D(kernel_size=(4,6), activation="relu", padding="same"))(eye_cnn)
    eye_cnn = TimeDistributed(AveragePooling2D(pool_size=(3,2)))(eye_cnn)
    eye_cnn = TimeDistributed(Conv2D(32, kernel_size=(1,1), activation="relu", padding="same"))(eye_cnn)
    eye_cnn = TimeDistributed(Flatten())(eye_cnn)
    eye_cnn = TimeDistributed(Dense(64, activation="relu"))(eye_cnn)
    eye_lstm = LSTM(128, return_sequences=False, dropout=0.3)(eye_cnn)
    eye_output = Dense(128, activation="relu")(eye_lstm)

    model = Model(inputs=[eeg_input, eye_input], outputs=[eeg_output, eye_output], name="DualStreamFeatureExtractor")
    return model

# -------------------------
# ğŸ“Œ Inter-Modality Fusion Module (Cross-Modal Transformer)
def create_inter_modality_fusion(eeg_features, eye_features, num_heads=4, d_model=128, dropout_rate=0.1):
    """
    EEGì™€ Eye Crop ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” Inter-Modality Fusion Module (Cross-Modal Transformer)
    """
    # Cross-Modal Transformer: EEG â†’ Eye Crop
    eeg_query = Dense(d_model)(eeg_features)    # shape: (batch_size, 128)
    eye_key_value = Dense(d_model)(eye_features)
    # Shape í™•ì¸
    print("eeg_query shape after Dense:", eeg_query.shape)
    print("eye_key_value shape after Dense:", eye_key_value.shape)
    
    # âœ… ì°¨ì› í™•ì¥ (Lambda ì‚¬ìš©) -> ì°¨ì›í™•ì¥ ì œê±°í–ˆì—ˆìœ¼ë‚˜, ì´ë ‡ê²Œ ì°¨ì›í™•ì¥ í•˜ê³  ì•„ë˜ì˜ ê²ƒë“¤ë„ ì°¨ì›í™•ì¥í•˜ëŠ” ê²ƒì´ ì˜¤ë¥˜ë¥¼ ì œê±°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ì—ˆìŒ.
    eeg_query = Lambda(lambda x: tf.expand_dims(x, axis=1))(eeg_query)  # (batch_size, 1, 128)
    eye_key_value = Lambda(lambda x: tf.expand_dims(x, axis=1))(eye_key_value)  # (batch_size, 1, 128)
    # ìµœì¢… Shape í™•ì¸
    print("eeg_query shape after expand_dims:", eeg_query.shape)
    print("eye_key_value shape after expand_dims:", eye_key_value.shape)

    cross_modal_attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="CrossModal_EEG_to_Eye")(
        query=eeg_query, key=eye_key_value, value=eye_key_value
    )
   
    cross_modal_attention_1 = Dropout(dropout_rate)(cross_modal_attention_1)
    cross_modal_attention_1 = Add()([eeg_query, cross_modal_attention_1])
    cross_modal_attention_1 = LayerNormalization(epsilon=1e-6)(cross_modal_attention_1)
    
    cross_modal_attention_1 = Lambda(lambda x: tf.squeeze(x, axis=1))(cross_modal_attention_1)  # (batch, d_model)
    
    # Cross-Modal Transformer: Eye Crop â†’ EEG
    eye_query = Dense(d_model)(eye_features)
    eye_query = Lambda(lambda x: tf.expand_dims(x, axis=1))(eye_query)  # Eye â†’ EEG Branch: ì‹œí€€ìŠ¤ ê¸¸ì´ 1ë¡œ í™•ì¥
    
    eeg_key_value_2 = Dense(d_model)(eeg_features)
    eeg_key_value_2 = Lambda(lambda x: tf.expand_dims(x, axis=1))(eeg_key_value_2)
    
    cross_modal_attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="CrossModal_Eye_to_EEG")(
        query=eye_query, key=eeg_key_value_2, value=eeg_key_value_2
    )
    cross_modal_attention_2 = Dropout(dropout_rate)(cross_modal_attention_2)
    cross_modal_attention_2 = Add()([eye_query, cross_modal_attention_2])
    cross_modal_attention_2 = LayerNormalization(epsilon=1e-6)(cross_modal_attention_2)

    # ì°¨ì› í™•ì¥ (cross_modal_attention_2ë¥¼ (None, 128)ë¡œ ì¶•ì†Œ)
    cross_modal_attention_2 = Lambda(lambda x: tf.squeeze(x, axis=1))(cross_modal_attention_2)  # (batch, d_model)
    
    fused_features = Concatenate(axis=-1)([cross_modal_attention_1, cross_modal_attention_2])
    fused_features = Dense(d_model, activation="relu", name="Fused_Linear")(fused_features)
    
    # Self-Attention Transformer: fused_featuresë¥¼ ì‹œí€€ìŠ¤ ì°¨ì›ìœ¼ë¡œ í™•ì¥í•˜ì—¬ Self-Attention ì ìš© í›„ squeeze
    fused_features_expanded = Lambda(lambda x: tf.expand_dims(x, axis=1))(fused_features)  # (batch, 1, d_model)

    # Self-Attention Transformer
    self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="SelfAttentionFusion")(
        query=fused_features_expanded, key=fused_features_expanded, value=fused_features_expanded
    )
    self_attention = Dropout(dropout_rate)(self_attention)
    self_attention = Add()([fused_features_expanded, self_attention])
    self_attention = LayerNormalization(epsilon=1e-6)(self_attention)

    self_attention = Lambda(lambda x: tf.squeeze(x, axis=1))(self_attention)  # (batch, d_model) -> ì°¨ì› ì¶•ì†Œ ì¶”ê°€
    return self_attention

# -------------------------
# Intra-Modality Encoding Module
def create_intra_modality_encoding(eeg_features, eye_features, num_heads=4, d_model=128, dropout_rate=0.1):
    """
    EEGì™€ Eye Cropì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ìœ ì§€í•˜ë©° ê°•í™”í•˜ëŠ” Intra-Modality Encoding Module
    """
    eeg_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(eeg_features)  # (batch, 1, 128) -> ì°¨ì›í™•ì¥í•´ì•¼í•¨ ì¶”ê°€.
    
    eeg_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="SelfAttention_EEG")(
        query=eeg_features, key=eeg_features, value=eeg_features
    )
    eeg_self_attention = Dropout(dropout_rate)(eeg_self_attention)
    eeg_self_attention = Add()([eeg_features, eeg_self_attention])
    eeg_self_attention = LayerNormalization(epsilon=1e-6)(eeg_self_attention)

    eeg_self_attention = Lambda(lambda x: tf.squeeze(x, axis=1))(eeg_self_attention)  # (batch, d_model) -> ì°¨ì› ì¶•ì†Œ ì¶”ê°€.
    
    
    # Eye ëª¨ë‹¬ë¦¬í‹°: ì‹œí€€ìŠ¤ ì°¨ì› ì¶”ê°€ í›„ Self-Attention ì ìš© -> ì¶”ê°€
    eye_features = Lambda(lambda x: tf.expand_dims(x, axis=1))(eye_features)  # (batch, 1, 128)

    eye_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="SelfAttention_Eye")(
        query=eye_features, key=eye_features, value=eye_features
    )
    eye_self_attention = Dropout(dropout_rate)(eye_self_attention)
    eye_self_attention = Add()([eye_features, eye_self_attention])
    eye_self_attention = LayerNormalization(epsilon=1e-6)(eye_self_attention)

    eye_self_attention = Lambda(lambda x: tf.squeeze(x, axis=1))(eye_self_attention)  # (batch, d_model) ì°¨ì› ì¶•ì†Œ ì¶”ê°€
    
    return eeg_self_attention, eye_self_attention

# -------------------------
# ì›ë³¸ Inputë¶€í„° ìµœì¢… ì¶œë ¥ê³¼ ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ê¹Œì§€ í¬í•¨í•˜ëŠ” ë‹¨ì¼ ëª¨ë¸ ìƒì„± í•¨ìˆ˜
def build_combined_model(num_classes=3):
    """
    EEGì™€ Eye Crop ë°ì´í„°ë¥¼ ë°›ì•„ Dualâ€‘Stream Feature Extraction, 
    Crossâ€‘Modal Fusion, Intraâ€‘Modal Encodingì„ ê±°ì³ 3ê°œì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±.
    """
    # 1ï¸âƒ£ ì›ë³¸ Input ë ˆì´ì–´ ì •ì˜
    eeg_input = Input(shape=(32, 1280, 1), name="EEG_Input")
    eye_input = Input(shape=(100, 8, 64, 3), name="Eye_Input")
    
    # 2ï¸âƒ£ Dual-Stream Feature Extractor (EEGì™€ Eye Crop ê°ê°ì˜ íŠ¹ì§• ì¶”ì¶œ)
    dual_extractor = create_dual_stream_feature_extractor()
    eeg_features, eye_features = dual_extractor([eeg_input, eye_input])
    
    # 3ï¸âƒ£ Inter-Modality Fusion Module (Cross-Modal Transformer)
    fused_features = create_inter_modality_fusion(eeg_features, eye_features)

    # 4ï¸âƒ£ Intra-Modality Encoding Module (ê° ëª¨ë‹¬ë¦¬í‹°ì˜ ê³ ìœ  íŠ¹ì„±ì„ ê°•í™”)
    eeg_encoded, eye_encoded = create_intra_modality_encoding(eeg_features, eye_features)
    
    # 5ï¸âƒ£ ê° ë¶„ë¥˜ ë¸Œëœì¹˜ êµ¬ì„±
    inter_classification = Dense(num_classes, activation="softmax", name="Inter_Classification")(fused_features)
    eeg_classification   = Dense(num_classes, activation="softmax", name="EEG_Classification")(eeg_encoded)
    eye_classification   = Dense(num_classes, activation="softmax", name="EyeCrop_Classification")(eye_encoded)
    
    # 6ï¸âƒ£ ë¶„ë¥˜ ê²°ê³¼ ê²°í•©ì„ ìœ„í•œ ê°€ì¤‘ì¹˜(weight) ì˜ˆì¸¡
    concat_features = Concatenate()([fused_features, eeg_encoded, eye_encoded]) # (batch, 384)
    
    # ğŸ”¥ 1. `Dense(3)`ì„ í™•ì‹¤íˆ ì ìš©
    weights_logits = Dense(units=3, activation=None, name="Weight_Logits")(concat_features)
    
    # ğŸ”¥ 2. `Softmax`ì˜ `axis`ë¥¼ ëª…í™•íˆ ì„¤ì •(ì•„ë§ˆ Lambdaë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒê³¼ ì„±ëŠ¥ì€ ë˜‘ê°™ì´ ë‚˜ì˜¬ ê²ƒ)
    weights = Softmax(axis=-1, name="Weight_Softmax")(weights_logits)
    
    # ğŸ”¥ ë””ë²„ê¹…ì„ ìœ„í•œ ì¶œë ¥
    print("concat_features.shape:", concat_features.shape) #í™•ì¸
    print("Weight_Logits.shape:", weights_logits.shape) #í™•ì¸
    print("weights.shape:", weights.shape)  # (batch_size, 3) ì˜ˆìƒ
    
    # âœ… ìµœì¢… ëª¨ë¸ ìƒì„± (ì…ë ¥: ì›ë³¸ Input / ì¶œë ¥: ì„¸ ê°œì˜ ë¶„ë¥˜ ê²°ê³¼ì™€ ê°€ì¤‘ì¹˜)
    model = Model(inputs=[eeg_input, eye_input],
                  outputs=[inter_classification, eeg_classification, eye_classification, weights],
                  name="Multimodal_Emotion_Classifier")
    
    return model

# -------------------------
# ì»¤ìŠ¤í…€ í•™ìŠµ ë‹¨ê³„ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class MultimodalEmotionClassifier(tf.keras.Model):
    def __init__(self, base_model, **kwargs):
        super(MultimodalEmotionClassifier, self).__init__(**kwargs)
        self.base_model = base_model

    def call(self, inputs, training=False):
        return self.base_model(inputs, training=training)

    def train_step(self, data):
        x, y = data
        # ë§Œì•½ yê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if isinstance(y, dict):
            y_true = y["Inter_Classification"]
        else:
            y_true = y
        with tf.GradientTape() as tape:
            inter_pred, eeg_pred, eye_pred, weights = self(x, training=True)
            loss_inter = tf.keras.losses.sparse_categorical_crossentropy(y_true, inter_pred)
            loss_eeg   = tf.keras.losses.sparse_categorical_crossentropy(y_true, eeg_pred)
            loss_eye   = tf.keras.losses.sparse_categorical_crossentropy(y_true, eye_pred)
            w1, w2, w3 = tf.split(weights, num_or_size_splits=3, axis=-1)
            loss = tf.reduce_mean(w1 * loss_inter + w2 * loss_eeg + w3 * loss_eye)
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        self.compiled_metrics.update_state(y_true, inter_pred)
        results = {m.name: m.result() for m in self.metrics}
        results.update({"loss": loss})
        return results
        # with tf.GradientTape() as tape:
        #     # ëª¨ë¸ ì˜ˆì¸¡: outputsëŠ” 4ê°œì˜ í…ì„œë¡œ êµ¬ì„±ë¨
        #     inter_pred, eeg_pred, eye_pred, weights = self(x, training=True)
        #     # ê°œë³„ ë¶„ë¥˜ ì†ì‹¤ ê³„ì‚° (sparse categorical crossentropy)
        #     loss_inter = tf.keras.losses.sparse_categorical_crossentropy(y, inter_pred)
        #     loss_eeg   = tf.keras.losses.sparse_categorical_crossentropy(y, eeg_pred)
        #     loss_eye   = tf.keras.losses.sparse_categorical_crossentropy(y, eye_pred)
        #     # ì˜ˆì¸¡ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶„í•  (ê°ê° shape: (batch,1))
        #     w1, w2, w3 = tf.split(weights, num_or_size_splits=3, axis=-1)
        #     loss = tf.reduce_mean(w1 * loss_inter + w2 * loss_eeg + w3 * loss_eye)
        # # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë° ì—…ë°ì´íŠ¸
        # trainable_vars = self.trainable_variables
        # gradients = tape.gradient(loss, trainable_vars)
        # self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸ (ì—¬ê¸°ì„œëŠ” inter_predë¥¼ ëŒ€í‘œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©)
        # self.compiled_metrics.update_state(y, inter_pred)
        # results = {m.name: m.result() for m in self.metrics}
        # results.update({"loss": loss})
        # return results
    def test_step(self, data):
        x, y = data
        y_true = y["Inter_Classification"] if isinstance(y, dict) else y
        inter_pred, _, _, _ = self(x, training=False)

        loss_inter = tf.keras.losses.sparse_categorical_crossentropy(y_true, inter_pred)
        loss = tf.reduce_mean(loss_inter)

        self.compiled_metrics.update_state(y_true, inter_pred)
        metric_results = {m.name: m.result() for m in self.metrics}
        # ë§Œì•½ "accuracy"ê°€ Noneì´ë©´ 0.0ì„ ì‚¬ìš©í•˜ë„ë¡ í•¨.
        acc = metric_results.get("accuracy")
        if acc is None:
            acc = 0.0
        results = {
            "accuracy": acc,
            "loss": loss
        }
        return results

# -------------------------
# ì…ë ¥ ë°ì´í„° í¬ê¸° ì¤„ì´ê¸° (ë‹¤ìš´ìƒ˜í”Œë§)
def downsample_eye_frame(frame):
    """Eye Crop ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ (64x32 â†’ 32x16)"""
    return cv2.resize(frame, (32, 8), interpolation=cv2.INTER_AREA)

# Eye Crop ë°ì´í„° ë¡œë“œ ì‹œ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
def reshape_eye_frame(data):
    """
    (N, 32, 64, 3) í˜•íƒœì˜ eye frame ë°ì´í„°ë¥¼ (32, 64, 3)ìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©.
    - Nì´ 2 ì´ìƒì´ë©´ í‰ê· ì„ ë‚´ì„œ ë³‘í•©.
    - Nì´ 1ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """
    if len(data.shape) == 4 and data.shape[0] > 0:
        reshaped_data = np.mean(data, axis=0)
        return downsample_eye_frame(reshaped_data)
    elif len(data.shape) == 3 and data.shape == (32, 64, 3):
        return downsample_eye_frame(data)
    else:
        raise ValueError(f"Unexpected shape: {data.shape}, expected (N, 32, 64, 3) or (32, 64, 3)")

# -------------------------
# ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (Sample Index ê¸°ì¤€ ê·¸ë£¹ ë‹¨ìœ„ 8:2 ë¶„í• )
def load_multimodal_data(subject):
    eeg_data, eye_data, labels, sample_indices = [], [], [], []
    eeg_pattern = re.compile(rf"{subject}_sample_(\d+)_segment_(\d+)_label_(\w+).npy")

    # âœ… Sample 1~40ë¥¼ ëŒ€ìƒìœ¼ë¡œ ë°ì´í„° ë¡œë“œ (ì—¬ê¸°ì„œ ë°˜ë³µ íšŸìˆ˜ê°€ sampleì˜ ê°œìˆ˜)
    for sample_index in range(1, 41):  
        sample_number = f"{sample_index:02d}"
        print(f"\nğŸŸ¢ Processing {subject} - Sample {sample_number}")

        eeg_files = [f for f in os.listdir(EEG_DATA_PATH) if eeg_pattern.match(f) and f"sample_{sample_number}" in f]
        if not eeg_files:
            print(f"ğŸš¨ No EEG file found for {subject} - Sample {sample_number}")
            continue

        for file_name in eeg_files:
            match = eeg_pattern.match(file_name)
            if not match:
                continue

            segment_index = int(match.group(2))
            emotion_label = match.group(3)
            eeg_file_path = os.path.join(EEG_DATA_PATH, file_name)
            eeg_segment = np.load(eeg_file_path)

            eye_subject_path = os.path.join(EYE_CROP_PATH, subject)
            if not os.path.exists(eye_subject_path):
                print(f"ğŸš¨ Subject folder not found: {eye_subject_path}")
                continue

            trial_number = sample_index  # trial_numberì™€ sample_indexë¥¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©
            expected_start = (segment_index - 1) * 500
            frame_indices = set()
            file_mapping = {}

            for f in os.listdir(eye_subject_path):
                try:
                    if not f.startswith(subject) or not f.endswith(".npy"):
                        continue
                    match_frame = re.search(r"trial(\d+).avi_frame(\d+)", f)
                    if not match_frame:
                        print(f"âš  Skipping invalid file name: {f} (No trial/frame pattern found)")
                        continue
                    file_trial_number = int(match_frame.group(1))
                    frame_number = int(match_frame.group(2))
                    if file_trial_number == trial_number:
                        frame_indices.add(frame_number)
                        file_mapping[frame_number] = os.path.join(eye_subject_path, f)
                except ValueError as e:
                    print(f"ğŸš¨ Error processing file {f}: {e}")
                    continue

            frame_indices = sorted(frame_indices)
            print(f"  ğŸ” Available frame indices: {frame_indices[:10]} ... {frame_indices[-10:]}")

            # ì›ë˜ 500í”„ë ˆì„ ìœˆë„ìš° ì‚¬ìš©
            window_length = 500
            if len(frame_indices) < window_length:
                print(f"âš  Warning: Not enough frames ({len(frame_indices)}) for segment {segment_index:03d}. Skipping Eye Crop.")
                eye_data.append(None)
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])
                sample_indices.append(sample_index)
                continue

            # 500í”„ë ˆì„ ì¤‘ ì²˜ìŒ 500í”„ë ˆì„ ì„ íƒ
            selected_frames = frame_indices[:window_length]

            # ë§Œì•½ 500í”„ë ˆì„ì´ ë¶€ì¡±í•œ ê²½ìš° ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì œ (ì›ë˜ ì¡°ê±´ì´ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ë³´í†µ ì¶©ë¶„í•  ê²ƒìœ¼ë¡œ ê°€ì •)
            if len(selected_frames) < window_length:
                print(f"âš  Warning: Found only {len(selected_frames)} frames in selected window for segment {segment_index:03d}")
                while len(selected_frames) < window_length:
                    selected_frames.append(selected_frames[-1])
                    print("í”„ë ˆì„ ë³µì œë¨")

            # â˜… ê· ì¼ ìƒ˜í”Œë§: 500í”„ë ˆì„ ì¤‘ np.linspaceë¥¼ ì‚¬ìš©í•´ 100ê°œì˜ ì¸ë±ìŠ¤ ì„ íƒ
            indices = np.linspace(0, window_length - 1, num=100, endpoint=True, dtype=int)
            selected_frames = [selected_frames[i] for i in indices]

            eye_frame_files = []
            for frame in selected_frames:
                if frame in file_mapping:
                    eye_frame_files.append(file_mapping[frame])
                if len(eye_frame_files) == 100:  # 100ê°œë¡œ ë§ì¶¤
                    break

            eye_frame_stack = []
            for f in eye_frame_files:
                frame_data = np.load(f)
                frame_data = reshape_eye_frame(frame_data)
                if frame_data.shape[-2] == 32:
                    pad_width = [(0, 0)] * frame_data.ndim
                    pad_width[-2] = (16, 16)
                    frame_data = np.pad(frame_data, pad_width, mode='constant', constant_values=0)
                eye_frame_stack.append(frame_data)

            if len(eye_frame_stack) == 100:
                eye_data.append(np.stack(eye_frame_stack, axis=0))
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])
                # ê° ë°ì´í„°ì— í•´ë‹¹í•˜ëŠ” sample index ê¸°ë¡
                sample_indices.append(sample_index)
            else:
                print(f"âš  Warning: Found only {len(eye_frame_stack)} matching frames for segment {segment_index:03d}")

    print(f"âœ… Total EEG Samples Loaded: {len(eeg_data)}")
    print(f"âœ… Total Eye Crop Samples Loaded: {len([e for e in eye_data if e is not None])}")
    print(f"âœ… Labels Loaded: {len(labels)}")
    
    # ===== ê·¸ë£¹ ë‹¨ìœ„ (sample_index)ë¡œ train/test ë¶„í•  =====
    unique_samples = sorted(set(sample_indices))
    print(f"ğŸ” Unique Sample Indices: {unique_samples}")
    # ê³ ìœ  sample ì¤‘ 20%ë¥¼ í…ŒìŠ¤íŠ¸ ê·¸ë£¹ìœ¼ë¡œ ë¶„í• 
    train_samples, test_samples = train_test_split(unique_samples, test_size=0.2, random_state=42)
    print(f"ğŸ” Train Samples: {train_samples}, Test Samples: {test_samples}")

    # ê° ë°ì´í„°ê°€ ì†í•œ sample indexë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    train_eeg_data, train_eye_data, train_labels = [], [], []
    test_eeg_data, test_eye_data, test_labels = [], [], []

    for idx, s in enumerate(sample_indices):
        if s in train_samples:
            train_eeg_data.append(eeg_data[idx])
            train_eye_data.append(eye_data[idx])
            train_labels.append(labels[idx])
        else:
            test_eeg_data.append(eeg_data[idx])
            test_eye_data.append(eye_data[idx])
            test_labels.append(labels[idx])

    # Numpy ë°°ì—´ë¡œ ë³€í™˜ (eye ë°ì´í„°ëŠ” Noneì¸ ê²½ìš° 0ìœ¼ë¡œ ì±„ìš´ ë°°ì—´ë¡œ ëŒ€ì²´)
    train_eeg_data = np.array(train_eeg_data)
    train_eye_data = np.array([e if e is not None else np.zeros((100, 8, 64, 3)) for e in train_eye_data])
    train_labels = np.array(train_labels)
    
    test_eeg_data = np.array(test_eeg_data)
    test_eye_data = np.array([e if e is not None else np.zeros((100, 8, 64, 3)) for e in test_eye_data])
    test_labels = np.array(test_labels)
    
    print(f"âœ… Train EEG Samples: {train_eeg_data.shape[0]}, Test EEG Samples: {test_eeg_data.shape[0]}")
    print(f"âœ… Train Eye Crop Samples: {train_eye_data.shape[0]}, Test Eye Crop Samples: {test_eye_data.shape[0]}")
    print(f"âœ… Train Labels: {train_labels.shape[0]}, Test Labels: {test_labels.shape[0]}")
    
    return train_eeg_data, train_eye_data, train_labels, test_eeg_data, test_eye_data, test_labels

# -------------------------
# í•™ìŠµ ë° í‰ê°€
def train_multimodal():
    subjects = [f"s{str(i).zfill(2)}" for i in range(1, 23)]
    for subject in subjects:
        print(f"Training subject: {subject}")

        # âœ… ìˆ˜ì •: load_multimodal_data()ì—ì„œ ì´ë¯¸ 8:2ë¡œ ë‚˜ëˆ ì§„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´.
        train_eeg, train_eye, train_labels, test_eeg, test_eye, test_labels = load_multimodal_data(subject)

        # âœ… ì¶”ê°€: Train ë°ì´í„°ë¥¼ 80:20ìœ¼ë¡œ Validation Setìœ¼ë¡œ ë¶„í• 
        train_eeg, valid_eeg, train_eye, valid_eye, train_labels, valid_labels = train_test_split(
            train_eeg, train_eye, train_labels, test_size=0.2, random_state=42
        )

        # âœ… ê¸°ì¡´ ëª¨ë¸ ìƒì„± í›„, MultimodalEmotionClassifier ë˜í•‘
        # ëª¨ë¸ ìƒì„± ë° ë˜í•‘
        base_model = build_combined_model(num_classes=3)
        model = MultimodalEmotionClassifier(base_model)
        # ì»¤ìŠ¤í…€ train_stepê³¼ test_stepì„ ì˜¤ë²„ë¼ì´ë“œí–ˆìœ¼ë¯€ë¡œ loss ì¸ìëŠ” ìƒëµí•´ë„ ë©ë‹ˆë‹¤.
        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                      metrics=["accuracy"])
    
        print(model.summary())

        start_epoch = 0
        max_epochs = 50
        batch_size = 2
        max_retries = 3

        for epoch in range(start_epoch, max_epochs):
            retries = 0
            while retries < max_retries:
                try:
                    print(f"\nğŸš€ Training {subject} - Epoch {epoch+1}/{max_epochs} (Retry: {retries})...")
                    
                    dummy_y_train = {
                        "Inter_Classification": train_labels,
                        "EEG_Classification": train_labels,
                        "EyeCrop_Classification": train_labels,
                        "Weight_Softmax": train_labels
                    }

                    dummy_y_valid = {
                        "Inter_Classification": valid_labels,
                        "EEG_Classification": valid_labels,
                        "EyeCrop_Classification": valid_labels,
                        "Weight_Softmax": valid_labels
                    }

                    model.fit(
                        [train_eeg, train_eye],
                        dummy_y_train,
                        validation_data=([valid_eeg, valid_eye], dummy_y_valid),
                        epochs=1,
                        batch_size=batch_size
                    )

                    # model.fit(
                    #     [train_eeg, train_eye], train_labels,
                    #     validation_data=([valid_eeg, valid_eye], valid_labels),
                    #     epochs=1, batch_size=batch_size
                    # )
                    
                    break  
                except tf.errors.ResourceExhaustedError:
                    print(f"âš ï¸ OOM ë°œìƒ! ì¬ì‹œì‘ (Retry: {retries+1})...")
                    tf.keras.backend.clear_session()
                    import gc
                    gc.collect()
                    model = build_combined_model(num_classes=3)
                    retries += 1
                    tf.keras.backend.sleep(1)
            else:
                print(f"âŒ ì—í¬í¬ {epoch+1}ì—ì„œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                break

        subject_save_path = os.path.join(SAVE_PATH, subject)
        os.makedirs(subject_save_path, exist_ok=True)
        weight_path = os.path.join(subject_save_path, f"{subject}_multimodal_model.weights.h5")
        model.save_weights(weight_path)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ë¨: {weight_path}")

        # âœ… ì˜ˆì¸¡ ê²°ê³¼ ì²˜ë¦¬
        predictions = model.predict([test_eeg, test_eye])
        inter_pred = predictions[0]  # í‰ê°€ì— ì‚¬ìš©í•  ë¶„ë¥˜ ë¸Œëœì¹˜ ì„ íƒ
        predicted_labels = np.argmax(inter_pred, axis=-1)

        test_report = classification_report(
            test_labels, predicted_labels,
            target_names=["Negative", "Positive", "Neutral"],
            labels=[0, 1, 2],
            zero_division=0
        )
        print(f"\nğŸ“Š Test Report for {subject}")
        print(test_report)

        report_path = os.path.join(subject_save_path, f"{subject}_test_report.txt")
        with open(report_path, "w") as f:
            f.write(test_report)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_path}")
        
if __name__ == "__main__":
    train_multimodal()
