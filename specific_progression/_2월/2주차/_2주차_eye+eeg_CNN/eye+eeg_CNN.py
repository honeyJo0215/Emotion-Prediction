# CNN ê¸°ë°˜ì˜ Dual-Stream Feature Extractor + Cross-Modal Transformer

import os
import re
import cv2  # downsample ì‹œ ì‚¬ìš©
import numpy as np
import tensorflow as tf
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.layers import (
    Conv2D, Dense, Flatten, Dropout, AveragePooling2D, DepthwiseConv2D, LayerNormalization, 
    MultiHeadAttention, Reshape, Concatenate, GlobalAveragePooling2D, Input, TimeDistributed, 
    LSTM, Add, Dropout, Softmax
)
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical

# ğŸ“Œ ê°ì • ë¼ë²¨ ë§¤í•‘
EMOTION_MAPPING = {
    "Negative": 0,
    "Positive": 1,
    "Neutral": 2
}

# ğŸ“Œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
BATCH_SIZE = 8
EPOCHS = 50
LEARNING_RATE = 1e-4

# ğŸ“Œ ë°ì´í„° ê²½ë¡œ
EEG_DATA_PATH = "/home/bcml1/2025_EMOTION/DEAP_EEG_10s_1soverlap_labeled"
EYE_CROP_PATH = "/home/bcml1/2025_EMOTION/eye_crop"
SAVE_PATH = "/home/bcml1/sigenv/_2ì£¼ì°¨_eye+eeg_CNN/result_e+ec"
os.makedirs(SAVE_PATH, exist_ok=True)
SUBJECTS = [f"s{str(i).zfill(2)}" for i in range(1, 23)]  # í”¼ì‹¤í—˜ì 1~22ëª…

# -------------------------
# ğŸ“Œ Dual-Stream Feature Extractor
def create_dual_stream_feature_extractor():
    """
    EEGì™€ Eye Crop ë°ì´í„°ë¥¼ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” Dual-Stream Feature Extractor
    """

    # ğŸ“Œ EEG Stream (32ì±„ë„, 1280 ìƒ˜í”Œ â†’ (32, 1280, 1))
    eeg_input = Input(shape=(32, 1280, 1), name="EEG_Input")

    x = Conv2D(16, kernel_size=(3,3), activation="relu", padding="same")(eeg_input)  # Primary Convolution Layer
    x = DepthwiseConv2D(kernel_size=(3,3), activation="relu", padding="same")(x)  # Depthwise Convolution Layer
    x = AveragePooling2D(pool_size=(2,2))(x)  # Average Pooling Layer
    x = Conv2D(32, kernel_size=(3,3), activation="relu", padding="same")(x)  # Pointwise Convolution Layer
    x = Flatten()(x)  # Flatten Layer
    eeg_output = Dense(128, activation="relu")(x)  # Feature ë²¡í„° ë³€í™˜

    # ğŸ“Œ Eye Crop Stream (500, 8, 64, 3)
    eye_input = Input(shape=(500, 8, 64, 3), name="Eye_Input")

    # ğŸ“Œ ì‹œê°„ì¶•(500 í”„ë ˆì„) ë³„ë¡œ CNN ì ìš©ì„ ìœ„í•´ TimeDistributed ì‚¬ìš©
    eye_cnn = TimeDistributed(Conv2D(16, kernel_size=(1,16), activation="relu", padding="same"))(eye_input)
    eye_cnn = TimeDistributed(DepthwiseConv2D(kernel_size=(4,6), activation="relu", padding="same"))(eye_cnn)
    eye_cnn = TimeDistributed(AveragePooling2D(pool_size=(3,2)))(eye_cnn)
    eye_cnn = TimeDistributed(Conv2D(32, kernel_size=(1,1), activation="relu", padding="same"))(eye_cnn)
    eye_cnn = TimeDistributed(Flatten())(eye_cnn)  # ê° í”„ë ˆì„(8,64,3)ì—ì„œ íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
    eye_cnn = TimeDistributed(Dense(64, activation="relu"))(eye_cnn)

    # ğŸ“Œ LSTMì„ ì ìš©í•˜ì—¬ ì‹œê°„ì¶•(500) í”„ë ˆì„ì— ëŒ€í•œ ìš”ì•½
    eye_lstm = LSTM(128, return_sequences=False, dropout=0.3)(eye_cnn)  # ìµœì¢… 128ì°¨ì› íŠ¹ì§• ë²¡í„°
    eye_output = Dense(128, activation="relu")(eye_lstm)

    # ğŸ“Œ ìµœì¢… ëª¨ë¸ ìƒì„±
    model = Model(inputs=[eeg_input, eye_input], outputs=[eeg_output, eye_output], name="DualStreamFeatureExtractor")
    
    return model

# -------------------------
# ğŸ“Œ Inter-Modality Fusion Module (Cross-Modal Transformer)
def create_inter_modality_fusion(eeg_features, eye_features, num_heads=4, d_model=128, dropout_rate=0.1):
    """
    EEGì™€ Eye Crop ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” Inter-Modality Fusion Module (Cross-Modal Transformer)
    
    Args:
        eeg_features: EEGì—ì„œ ì¶”ì¶œëœ íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        eye_features: Eye Cropì—ì„œ ì¶”ì¶œëœ íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        num_heads: Multi-Head Attentionì—ì„œ ì‚¬ìš©í•  í—¤ë“œ ìˆ˜
        d_model: Feature dimension (128)
        dropout_rate: Dropout ë¹„ìœ¨

    Returns:
        ìµœì¢… ìœµí•©ëœ íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
    """

    # 1ï¸âƒ£ **Cross-Modal Transformer: EEG â†’ Eye Crop**
    eeg_query = Dense(d_model)(eeg_features)  # Query from EEG
    eye_key_value = Dense(d_model)(eye_features)  # Key, Value from Eye Crop

    cross_modal_attention_1 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="CrossModal_EEG_to_Eye")(
        query=eeg_query, key=eye_key_value, value=eye_key_value
    )
    cross_modal_attention_1 = Dropout(dropout_rate)(cross_modal_attention_1)
    cross_modal_attention_1 = Add()([eeg_query, cross_modal_attention_1])
    cross_modal_attention_1 = LayerNormalization(epsilon=1e-6)(cross_modal_attention_1)

    # 2ï¸âƒ£ **Cross-Modal Transformer: Eye Crop â†’ EEG**
    eye_query = Dense(d_model)(eye_features)  # Query from Eye Crop
    eeg_key_value = Dense(d_model)(eeg_features)  # Key, Value from EEG

    cross_modal_attention_2 = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="CrossModal_Eye_to_EEG")(
        query=eye_query, key=eeg_key_value, value=eeg_key_value
    )
    cross_modal_attention_2 = Dropout(dropout_rate)(cross_modal_attention_2)
    cross_modal_attention_2 = Add()([eye_query, cross_modal_attention_2])
    cross_modal_attention_2 = LayerNormalization(epsilon=1e-6)(cross_modal_attention_2)

    # 3ï¸âƒ£ **ìœµí•©ëœ íŠ¹ì§• ë²¡í„° ìƒì„±**
    fused_features = tf.keras.layers.Concatenate(axis=-1)([cross_modal_attention_1, cross_modal_attention_2])  # ë³‘í•©
    fused_features = Dense(d_model, activation="relu", name="Fused_Linear")(fused_features)

    # 4ï¸âƒ£ **Self-Attention Transformer**
    self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="SelfAttentionFusion")(
        query=fused_features, key=fused_features, value=fused_features
    )
    self_attention = Dropout(dropout_rate)(self_attention)
    self_attention = Add()([fused_features, self_attention])
    self_attention = LayerNormalization(epsilon=1e-6)(self_attention)

    return self_attention  # ìµœì¢… 128ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶œë ¥

# Intra-Modality Encoding Module
def create_intra_modality_encoding(eeg_features, eye_features, num_heads=4, d_model=128, dropout_rate=0.1):
    """
    EEGì™€ Eye Cropì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ ìœ ì§€í•˜ë©° ê°•í™”í•˜ëŠ” Intra-Modality Encoding Module (Self-Attention Transformer)
    
    Args:
        eeg_features: EEGì—ì„œ ì¶”ì¶œëœ íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        eye_features: Eye Cropì—ì„œ ì¶”ì¶œëœ íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        num_heads: Multi-Head Attentionì—ì„œ ì‚¬ìš©í•  í—¤ë“œ ìˆ˜
        d_model: Feature dimension (128)
        dropout_rate: Dropout ë¹„ìœ¨

    Returns:
        EEG & Eye Crop ê°ê°ì˜ Self-Attentionì„ ì ìš©í•œ ê°œë³„ íŠ¹ì„± ë²¡í„° (128ì°¨ì›)
    """

    # ğŸ“Œ **Self-Attention Transformer: EEG Encoding**
    eeg_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="SelfAttention_EEG")(
        query=eeg_features, key=eeg_features, value=eeg_features
    )
    eeg_self_attention = Dropout(dropout_rate)(eeg_self_attention)
    eeg_self_attention = Add()([eeg_features, eeg_self_attention])
    eeg_self_attention = LayerNormalization(epsilon=1e-6)(eeg_self_attention)

    # ğŸ“Œ **Self-Attention Transformer: Eye Crop Encoding**
    eye_self_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model, name="SelfAttention_Eye")(
        query=eye_features, key=eye_features, value=eye_features
    )
    eye_self_attention = Dropout(dropout_rate)(eye_self_attention)
    eye_self_attention = Add()([eye_features, eye_self_attention])
    eye_self_attention = LayerNormalization(epsilon=1e-6)(eye_self_attention)

    return eeg_self_attention, eye_self_attention


def create_classifier(eeg_features, eye_features, num_classes=3):
    """
    ìµœì¢… ê°ì • ë¶„ë¥˜ê¸° (EEG + Eye Crop Feature ìœµí•© í›„ Softmax ê¸°ë°˜ ë¶„ë¥˜)
    
    Args:
        inter_features: Inter-Modality Fusionì„ ê±°ì¹œ ìµœì¢… íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        eeg_features: Intra-Modality Encodingì„ ê±°ì¹œ EEG íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        eye_features: Intra-Modality Encodingì„ ê±°ì¹œ Eye Crop íŠ¹ì§• ë²¡í„° (128ì°¨ì›)
        num_classes: ê°ì • í´ë˜ìŠ¤ ìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
        ìµœì¢… ëª¨ë¸ (Softmax ë¶„ë¥˜ê¸° ì ìš©)
    """
    # ğŸ“Œ **Inter-Modality Fusionì„ ë‚´ë¶€ì—ì„œ ìƒì„± (ì…ë ¥ í•„ìš” ì—†ìŒ)**
    fused_features = create_inter_modality_fusion(eeg_features, eye_features)

    # ğŸ“Œ **ìœµí•©ëœ Inter-Modality Featureë¥¼ í†µí•œ ê°ì • ë¶„ë¥˜**
    inter_classification = Dense(num_classes, activation="softmax", name="Inter_Classification")(fused_features)

    # ğŸ“Œ **EEG ë‹¨ì¼ ëª¨ë‹¬ Feature ë¶„ë¥˜**
    eeg_classification = Dense(num_classes, activation="softmax", name="EEG_Classification")(eeg_features)

    # ğŸ“Œ **Eye Crop ë‹¨ì¼ ëª¨ë‹¬ Feature ë¶„ë¥˜**
    eye_classification = Dense(num_classes, activation="softmax", name="EyeCrop_Classification")(eye_features)

    # ğŸ“Œ **ê°€ì¤‘ì¹˜ í•™ìŠµì„ ìœ„í•œ softmax ì ìš©**
    # 3ê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“  í›„ softmaxë¥¼ ì ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜
    

    # ğŸ“Œ **ê°€ì¤‘ì¹˜ í•™ìŠµì„ ìœ„í•œ softmax ì ìš©**
    concat_features = Concatenate()([fused_features, eeg_features, eye_features])  # âœ… Keras Concatenate ì‚¬ìš©
    weights_logits = Dense(3)(concat_features)  
    weights = Softmax(axis=-1, name="Weight_Softmax")(weights_logits)  # (batch_size, 3)

    # ğŸ“Œ **ìµœì¢… ëª¨ë¸ ìƒì„±**
    model = Model(inputs=[eeg_features, eye_features], 
                  outputs=[inter_classification, eeg_classification, eye_classification, weights],
                  name="Multimodal_Emotion_Classifier")

    # ğŸ“Œ **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (ê°€ì¤‘ì¹˜ softmax ì ìš©)**
    def multimodal_loss(y_true, y_preds):
        """
        L_total = W1 * L_inter + W2 * L_EEG + W3 * L_EyeCrop
        """
        inter_pred, eeg_pred, eye_pred, weight_logits = y_preds  # ëª¨ë¸ì˜ 4ê°œ ì¶œë ¥ (ê°€ì¤‘ì¹˜ logits í¬í•¨)
        
        # ê°€ì¤‘ì¹˜ softmax ì ìš©
        weight_softmax = tf.nn.softmax(weight_logits)  # (batch_size, 3)
        w1, w2, w3 = tf.split(weight_softmax, num_or_size_splits=3, axis=-1)  # ê°ê° ë¶„í• 
        
        # ê°œë³„ ì†ì‹¤ ê³„ì‚°
        loss_inter = tf.keras.losses.sparse_categorical_crossentropy(y_true, inter_pred)
        loss_eeg = tf.keras.losses.sparse_categorical_crossentropy(y_true, eeg_pred)
        loss_eye = tf.keras.losses.sparse_categorical_crossentropy(y_true, eye_pred)

        # ê° ì†ì‹¤ì— ë™ì ìœ¼ë¡œ í• ë‹¹ëœ ê°€ì¤‘ì¹˜ ì ìš©
        total_loss = (w1 * loss_inter) + (w2 * loss_eeg) + (w3 * loss_eye)

        return total_loss

    # ğŸ“Œ **ëª¨ë¸ ì»´íŒŒì¼**
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
                  loss=multimodal_loss,
                  metrics=["accuracy"])

    return model

# -------------------------
# ğŸ“Œ ìµœì¢… Classifier
# def create_multimodal_emotion_classifier():
#     """
#     EEG + Eye Crop ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ ê°ì • ë¶„ë¥˜ ëª¨ë¸ ìƒì„±.
#     """
#     # Feature Extractor
#     dual_stream_extractor = create_dual_stream_feature_extractor()
#     eeg_features, eye_features = dual_stream_extractor.output

#     # Inter-Modality Fusion
#     fused_features = create_inter_modality_fusion(eeg_features, eye_features)

#     # ìµœì¢… ë¶„ë¥˜ê¸°
#     x = Dense(64, activation="relu")(fused_features)
#     x = Dropout(0.5)(x)
#     output = Dense(3, activation="softmax", name="Emotion_Output")(x)

#     return Model(inputs=dual_stream_extractor.input, outputs=output, name="MultimodalEmotionClassifier")


#ë°ì´í„° ë¡œë“œ í•¨ìˆ˜

# def find_subject_folder(base_path, subject):
#     """ì‹¤ì œ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ subject(s01, s02 ...)ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ì°¾ìŒ."""
#     possible_folders = os.listdir(base_path)  # eye_crop ë‚´ í´ë” í™•ì¸
#     for folder in possible_folders:
#         if folder.lower() == subject.lower():  # ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ë¹„êµ
#             return os.path.join(base_path, folder)
#     return None  # í•´ë‹¹ í´ë”ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°

# # -------------------------
# # ğŸ“Œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# def train_multimodal_model():
#     model = create_multimodal_emotion_classifier()
#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
#                   loss="sparse_categorical_crossentropy",
#                   metrics=["accuracy"])

#     print(model.summary())

#     # ë°ì´í„° ë¡œë“œ
#     eeg_data, eye_crop_data, labels = load_multimodal_data("s01")
#     X_train, X_test, y_train, y_test = train_test_split([eeg_data, eye_crop_data], labels, test_size=0.2, random_state=42)

#     # Dataset ìƒì„±
#     train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(8).shuffle(1000)
#     test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(8)

#     # í•™ìŠµ
#     model.fit(train_dataset, epochs=50, validation_data=test_dataset)

#     # ëª¨ë¸ ì €ì¥
#     model.save(os.path.join(SAVE_PATH, "MultimodalEmotionModel.h5"))

# # -------------------------
# # ğŸ“Œ ì‹¤í–‰
# if __name__ == "__main__":
#     train_multimodal_model()


# def build_full_model(num_classes=3):
#     """
#     EEGì™€ Eye Crop ë°ì´í„°ë¥¼ ë°›ì•„ Dualâ€‘Stream Feature Extraction, 
#     Crossâ€‘Modal Fusion, Intraâ€‘Modal Encodingì„ ê±°ì³ 3ê°œì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ìƒì„±.
#     """
#     # 1. ì…ë ¥ ë ˆì´ì–´ ì •ì˜
#     eeg_input = tf.keras.layers.Input(shape=(32, 1280, 1), name="EEG_Input")
#     eye_input = tf.keras.layers.Input(shape=(500, 8, 64, 3), name="Eye_Input")
    
#     # 2. Dualâ€‘Stream Feature Extraction (ê°ê° 128ì°¨ì›ì˜ feature ë²¡í„° ì‚°ì¶œ)
#     dual_extractor = create_dual_stream_feature_extractor()
#     eeg_features, eye_features = dual_extractor([eeg_input, eye_input])
    
#     # 3. Interâ€‘Modality Fusion (Crossâ€‘Modal Transformer)
#     fused_features = create_inter_modality_fusion(eeg_features, eye_features,
#                                                    num_heads=4, d_model=128, dropout_rate=0.1)
    
#     # 4. Intraâ€‘Modality Encoding (Selfâ€‘Attention Transformerë¡œ ê° ëª¨ë‹¬ íŠ¹ì„± ê°•í™”)
#     eeg_encoded, eye_encoded = create_intra_modality_encoding(eeg_features, eye_features,
#                                                               num_heads=4, d_model=128, dropout_rate=0.1)
    
#     # 5. ê° ë¶„ë¥˜ê¸° êµ¬ì„±  
#     # # (A) ìœµí•©ëœ inter-modal featureë¥¼ í†µí•œ ê°ì • ë¶„ë¥˜  
#     # inter_output = tf.keras.layers.Dense(num_classes, activation="softmax", name="Inter_Classification")(fused_features)
#     # # (B) EEG ë‹¨ì¼ ëª¨ë‹¬ ë¶„ë¥˜  
#     # eeg_output   = tf.keras.layers.Dense(num_classes, activation="softmax", name="EEG_Classification")(eeg_encoded)
#     # # (C) Eye Crop ë‹¨ì¼ ëª¨ë‹¬ ë¶„ë¥˜  
#     # eye_output   = tf.keras.layers.Dense(num_classes, activation="softmax", name="EyeCrop_Classification")(eye_encoded)
    
#     # # 6. ìµœì¢… ëª¨ë¸ ìƒì„±
#     # model = tf.keras.models.Model(inputs=[eeg_input, eye_input],
#     #                               outputs=[inter_output, eeg_output, eye_output],
#     #                               name="Multimodal_Emotion_Classifier")
    
#     # # 7. ëª¨ë¸ ì»´íŒŒì¼: ê° ë¶„ë¥˜ê¸°ì˜ ì†ì‹¤ì— ëŒ€í•´ ê°€ì¤‘í•© (ì˜ˆ, 0.5:0.25:0.25)
#     # loss_weights = {
#     #     "Inter_Classification": 0.5,
#     #     "EEG_Classification": 0.25,
#     #     "EyeCrop_Classification": 0.25
#     # }
    
#     # # ğŸ“Œ **ì†ì‹¤ í•¨ìˆ˜ ì •ì˜ (Cross-Entropy)**
#     # def multimodal_loss(y_true, y_pred):
#     #     """
#     #     L_total = Î»1 * L_inter + Î»2 * L_EEG + Î»3 * L_EyeCrop
#     #     """
#     #     loss_inter = tf.keras.losses.sparse_categorical_crossentropy(y_true, inter_classification)
#     #     loss_eeg = tf.keras.losses.sparse_categorical_crossentropy(y_true, eeg_classification)
#     #     loss_eye = tf.keras.losses.sparse_categorical_crossentropy(y_true, eye_classification)

#     #     return lambda1 * loss_inter + lambda2 * loss_eeg + lambda3 * loss_eye

    
#     # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
#     #               loss={
#     #                   "Inter_Classification": "sparse_categorical_crossentropy",
#     #                   "EEG_Classification": "sparse_categorical_crossentropy",
#     #                   "EyeCrop_Classification": "sparse_categorical_crossentropy"
#     #               },
#     #               loss_weights=loss_weights,
#     #               metrics=["accuracy"])
    
#     model = create_classifier(eeg_encoded, eye_encoded, num_classes=3)
#     # inter_features, eeg_features, eye_features, num_classes=3
#     return model

def build_full_model(num_classes=3):
    # 1. ì›ë³¸ ì…ë ¥ ë ˆì´ì–´ ì •ì˜
    eeg_input = tf.keras.layers.Input(shape=(32, 1280, 1), name="EEG_Input")
    eye_input = tf.keras.layers.Input(shape=(500, 8, 64, 3), name="Eye_Input")
    
    # 2. Dualâ€‘Stream Feature Extraction
    dual_extractor = create_dual_stream_feature_extractor()
    eeg_features, eye_features = dual_extractor([eeg_input, eye_input])
    
    # 3. Interâ€‘Modality Fusion
    fused_features = create_inter_modality_fusion(eeg_features, eye_features,
                                                   num_heads=4, d_model=128, dropout_rate=0.1)
    
    # 4. Intraâ€‘Modality Encoding (ê°œë³„ ëª¨ë‹¬ íŠ¹ì„± ê°•í™”)
    eeg_encoded, eye_encoded = create_intra_modality_encoding(eeg_features, eye_features,
                                                              num_heads=4, d_model=128, dropout_rate=0.1)
    
    # 5. ë¶„ë¥˜ê¸° ì¸µ êµ¬ì„±
    inter_classification = Dense(num_classes, activation="softmax", name="Inter_Classification")(fused_features)
    eeg_classification   = Dense(num_classes, activation="softmax", name="EEG_Classification")(eeg_encoded)
    eye_classification   = Dense(num_classes, activation="softmax", name="EyeCrop_Classification")(eye_encoded)
    
    # 6. ê°€ì¤‘ì¹˜ í•™ìŠµ branch (ì„¸ ë¶„ë¥˜ ê²°ê³¼ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ë™ì ìœ¼ë¡œ í•™ìŠµ)
    concat_features = Concatenate()([fused_features, eeg_encoded, eye_encoded])
    weights_logits = Dense(3)(concat_features)
    # Softmaxì— axis=-1ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • (ì´ë ‡ê²Œ í•˜ë©´ ì¶œë ¥ í…ì„œì˜ ë§ˆì§€ë§‰ ì°¨ì›ì— ëŒ€í•´ softmaxê°€ ì ìš©ë©ë‹ˆë‹¤)
    weights = Softmax(axis=-1, name="Weight_Softmax")(weights_logits)
    
    # 7. ìµœì¢… ëª¨ë¸ ìƒì„± (ì…ë ¥ì€ ì›ë³¸ ì…ë ¥ í…ì„œ ì‚¬ìš©)
    model = Model(inputs=[eeg_input, eye_input],
                  outputs=[inter_classification, eeg_classification, eye_classification, weights],
                  name="Multimodal_Emotion_Classifier")
    
    # 8. ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆì‹œ)
    def multimodal_loss(y_true, y_preds):
        # y_predsëŠ” 4ê°œ ì¶œë ¥: inter, eeg, eye, weight_logits
        inter_pred, eeg_pred, eye_pred, weight_logits = y_preds  
        # ê°€ì¤‘ì¹˜ softmax ì ìš©
        weight_softmax = tf.nn.softmax(weight_logits)  # (batch_size, 3)
        w1, w2, w3 = tf.split(weight_softmax, num_or_size_splits=3, axis=-1)
        loss_inter = tf.keras.losses.sparse_categorical_crossentropy(y_true, inter_pred)
        loss_eeg   = tf.keras.losses.sparse_categorical_crossentropy(y_true, eeg_pred)
        loss_eye   = tf.keras.losses.sparse_categorical_crossentropy(y_true, eye_pred)
        total_loss = (w1 * loss_inter) + (w2 * loss_eeg) + (w3 * loss_eye)
        return total_loss

    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),
                  loss=multimodal_loss,
                  metrics=["accuracy"])
    
    return model

# # âœ… **ì…ë ¥ ë°ì´í„° í¬ê¸° ì¤„ì´ê¸° (ë‹¤ìš´ìƒ˜í”Œë§)**
def downsample_eye_frame(frame):
    """Eye Crop ì´ë¯¸ì§€ ë‹¤ìš´ìƒ˜í”Œë§ (64x32 â†’ 32x16)"""
    return cv2.resize(frame, (32,8), interpolation=cv2.INTER_AREA)  # í•´ìƒë„ ì ˆë°˜ ê°ì†Œ

# # âœ… **Eye Crop ë°ì´í„° ë¡œë“œ ì‹œ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©**
def reshape_eye_frame(data):
    """
    (N, 32, 64, 3) í˜•íƒœì˜ eye frame ë°ì´í„°ë¥¼ (32, 64, 3)ìœ¼ë¡œ ë³€í™˜ í›„ ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©.
    - Nì´ 2 ì´ìƒì´ë©´ í‰ê· ì„ ë‚´ì„œ ë³‘í•©.
    - Nì´ 1ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
    """
    if len(data.shape) == 4 and data.shape[0] > 0:  
        reshaped_data = np.mean(data, axis=0)  # ëª¨ë“  ìš”ì†Œë¥¼ í‰ê·  ë‚´ì–´ ë³‘í•© (32, 64, 3)
        return downsample_eye_frame(reshaped_data)  # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
    elif len(data.shape) == 3 and data.shape == (32, 64, 3):  
        return downsample_eye_frame(data)  # ë‹¤ìš´ìƒ˜í”Œë§ ì ìš©
    else:
        raise ValueError(f"Unexpected shape: {data.shape}, expected (N, 32, 64, 3) or (32, 64, 3)")


# # ì…ë ¥ë°›ëŠ” EEG dataì˜ shapeëŠ” (32, 1280)ì´ê³ , Eye Crop ì˜ shapeì€ (2, 32, 64, 3) -> (500, 32, 64, 6)ì´ì•¼.
def load_multimodal_data(subject):
    eeg_data, eye_data, labels = [], [], []

    # íŒŒì¼ ì´ë¦„ì— í¬í•¨ëœ Sample ë° Segment ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì •ê·œí‘œí˜„ì‹
    eeg_pattern = re.compile(rf"{subject}_sample_(\d+)_segment_(\d+)_label_(\w+).npy")

    # Sample ë²ˆí˜¸ë¥¼ 1ë¶€í„° 40ê¹Œì§€ ì‚¬ìš© (ì¦‰, Sample 01 ~ Sample 40)
    for sample_index in range(1, 2):
        sample_number = f"{sample_index:02d}"  
        print(f"\nğŸŸ¢ Processing {subject} - Sample {sample_number}")

        eeg_files = [f for f in os.listdir(EEG_DATA_PATH)
                     if eeg_pattern.match(f) and f"sample_{sample_number}" in f]
        if not eeg_files:
            print(f"ğŸš¨ No EEG file found for {subject} - Sample {sample_number}")
            continue

        for file_name in eeg_files:
            match = eeg_pattern.match(file_name)
            if not match:
                continue

            # Segment ë²ˆí˜¸ëŠ” ì´ì œ 001, 002, ... ë¡œ ë˜ì–´ ìˆìœ¼ë¯€ë¡œ, ì²« Segment("001")ê°€ 1ì´ ë¨.
            segment_index = int(match.group(2))  # ì˜ˆ: "001" -> 1
            emotion_label = match.group(3)

            eeg_file_path = os.path.join(EEG_DATA_PATH, file_name)
            eeg_segment = np.load(eeg_file_path)

            eye_subject_path = os.path.join(EYE_CROP_PATH, subject)
            if not os.path.exists(eye_subject_path):
                print(f"ğŸš¨ Subject folder not found: {eye_subject_path}")
                continue

            # Trial ë²ˆí˜¸ëŠ” ì´ì œ sample_index ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¦‰, Sample 01ì´ë©´ Trial 01)
            trial_number = sample_index

            # ê¸°ì¡´ì—ëŠ” non-overlap ë°©ì‹ìœ¼ë¡œ ìœˆë„ìš°ë¥¼ ì„ íƒí–ˆë‹¤ë©´,
            # ì´ì œ sliding window (window_length=500, stride=50)ë¥¼ ì ìš©í•¨.
            # EEG íŒŒì¼ì˜ segment ë²ˆí˜¸ì— ë”°ë¼ ì˜ˆìƒ ì‹œì‘ í”„ë ˆì„ì€:
            expected_start = (segment_index - 1) * 500  # ì˜ˆ: segment "001" -> 0, "002" -> 500, ...
            # frame_indices: í•´ë‹¹ trialì—ì„œ ì¡´ì¬í•˜ëŠ” í”„ë ˆì„ ë²ˆí˜¸ ëª©ë¡
            frame_indices = set()
            file_mapping = {}

            for f in os.listdir(eye_subject_path):
                try:
                    if not f.startswith(subject) or not f.endswith(".npy"):
                        continue

                    match_frame = re.search(r"trial(\d+).avi_frame(\d+)", f)
                    if not match_frame:
                        print(f"âš  Skipping invalid file name: {f} (No trial/frame pattern found)")
                        continue

                    file_trial_number = int(match_frame.group(1))
                    frame_number = int(match_frame.group(2))

                    if file_trial_number == trial_number:
                        frame_indices.add(frame_number)
                        file_mapping[frame_number] = os.path.join(eye_subject_path, f)

                except ValueError as e:
                    print(f"ğŸš¨ Error processing file {f}: {e}")
                    continue

            frame_indices = sorted(frame_indices)
            print(f"  ğŸ” Available frame indices: {frame_indices[:10]} ... {frame_indices[-10:]}")

            # --- ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš© ---
            window_length = 500
            stride = 50
            candidate_windows = []
            # candidate windows: stride ê°„ê²©ìœ¼ë¡œ 500ê°œì˜ í”„ë ˆì„ ëª©ë¡ ìƒì„±
            for i in range(0, len(frame_indices) - window_length + 1, stride):
                window = frame_indices[i:i+window_length]
                candidate_windows.append(window)
            if len(candidate_windows) == 0:
                print(f"âš  Warning: Not enough frames for sliding window for segment {segment_index:03d}. Skipping Eye Crop.")
                eye_data.append(None)  # Eye Crop ë°ì´í„° ì—†ìŒ
                eeg_data.append(eeg_segment)  # EEG ë°ì´í„°ëŠ” ì¶”ê°€
                labels.append(EMOTION_MAPPING[emotion_label])
                continue

            # ì„ íƒ: candidate window ì¤‘ ì˜ˆìƒ ì‹œì‘(expected_start)ê³¼ ì²« í”„ë ˆì„ ì°¨ì´ê°€ ê°€ì¥ ì‘ì€ ìœˆë„ìš° ì„ íƒ
            selected_frames = min(candidate_windows, key=lambda w: abs(w[0] - expected_start))
            # --- ë ---

            # ë§Œì•½ ì„ íƒëœ ìœˆë„ìš°ì˜ ê¸¸ì´ê°€ 500ì´ ì•„ë‹ˆë¼ë©´ (ë“œë¬¼ê²Œ ë°œìƒí•  ê²½ìš°) ë³µì œí•˜ì—¬ ì±„ì›€
            if len(selected_frames) < 500:
                print(f"âš  Warning: Found only {len(selected_frames)} frames in selected window for segment {segment_index:03d}")
                while len(selected_frames) < 500:
                    selected_frames.append(selected_frames[-1])
                    print("í”„ë ˆì„ ë³µì œë¨")

            # eye_frame_files: ì„ íƒëœ í”„ë ˆì„ ë²ˆí˜¸ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ ê²½ë¡œ ëª©ë¡ ìƒì„±
            eye_frame_files = []
            for frame in selected_frames:
                if frame in file_mapping:
                    eye_frame_files.append(file_mapping[frame])
                if len(eye_frame_files) == 500:
                    break

            eye_frame_stack = []
            for f in eye_frame_files:
                frame_data = np.load(f)
                frame_data = reshape_eye_frame(frame_data)  # ë°˜ë“œì‹œ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨.
                # ë§Œì•½ frame_dataì˜ shapeê°€ (32,64,3)ë¼ë©´, padding ì ìš©í•˜ì—¬ (64,64,3)ë¡œ ë³€ê²½
                if frame_data.shape[-2] == 32:
                    pad_width = [(0, 0)] * frame_data.ndim
                    pad_width[-2] = (16, 16)
                    frame_data = np.pad(frame_data, pad_width, mode='constant', constant_values=0)
                eye_frame_stack.append(frame_data)

            if len(eye_frame_stack) == 500:
                eye_data.append(np.stack(eye_frame_stack, axis=0))
                eeg_data.append(eeg_segment)
                labels.append(EMOTION_MAPPING[emotion_label])
            else:
                print(f"âš  Warning: Found only {len(eye_frame_stack)} matching frames for segment {segment_index:03d}")

    print(f"âœ… Total EEG Samples Loaded: {len(eeg_data)}")
    print(f"âœ… Total Eye Crop Samples Loaded: {len([e for e in eye_data if e is not None])}")
    print(f"âœ… Labels Loaded: {len(labels)}")

    return np.array(eeg_data), np.array([e if e is not None else np.zeros((500, 8, 64, 3)) for e in eye_data]), np.array(labels)

# ğŸŸ¢ **í•™ìŠµ ë° í‰ê°€**
def train_multimodal():
    subjects = [f"s{str(i).zfill(2)}" for i in range(1, 23)]
    for subject in subjects:
        print(f"Training subject: {subject}")
        
        # ë°ì´í„° ë¡œë“œ
        eeg_data, eye_data, labels = load_multimodal_data(subject)

        # ìƒ˜í”Œ ë‹¨ìœ„ë¡œ Train/Valid/Test ë°ì´í„° ë‚˜ëˆ„ê¸°
        unique_samples = np.arange(len(eeg_data))  
        train_samples, test_samples = train_test_split(unique_samples, test_size=0.2, random_state=42)
        train_samples, valid_samples = train_test_split(train_samples, test_size=0.2, random_state=42)

        # ìƒ˜í”Œ ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ë¶„í• 
        train_eeg, train_eye, train_labels = eeg_data[train_samples], eye_data[train_samples], labels[train_samples]
        valid_eeg, valid_eye, valid_labels = eeg_data[valid_samples], eye_data[valid_samples], labels[valid_samples]
        test_eeg, test_eye, test_labels = eeg_data[test_samples], eye_data[test_samples], labels[test_samples]

        # ëª¨ë¸ì´ í•™ìŠµ ë„ì¤‘ OOMìœ¼ë¡œ ì¢…ë£Œë  ê²½ìš° ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ê³  ì¬ì‹œì‘í•˜ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œë¥¼ í•´ê²°ê°€ëŠ¥
        # ğŸš€ **ê° subject ë³„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ ì„¤ì •**
        checkpoint_dir = f"/home/bcml1/sigenv/_2ì£¼ì°¨_eye+eeg_CNN/checkpoint/cross_{subject}"
        checkpoint_path = os.path.join(checkpoint_dir, "cp.weights.h5")
        os.makedirs(checkpoint_dir, exist_ok=True)  # ë””ë ‰í† ë¦¬ ì—†ìœ¼ë©´ ìƒì„±

        # ì²´í¬í¬ì¸íŠ¸ ì½œë°± ì„¤ì • (ìë™ ì €ì¥)
        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
            filepath=checkpoint_path,
            save_weights_only=True,
            verbose=1
        )
        
        # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ êµ¬ì¶•
        model = build_full_model(num_classes=3)
        print(model.summary())

        # ğŸš€ **ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆë‹¤ë©´)**
        # if os.path.exists(checkpoint_path + ".index"):
        #     print(f"âœ… Checkpoint found for {subject}, loading model...")
        #     model.load_weights(checkpoint_path)

        # ë¼ë²¨ ì°¨ì› í™•ì¥
        train_labels = np.expand_dims(train_labels, axis=-1)
        valid_labels = np.expand_dims(valid_labels, axis=-1)

        # ğŸš€ **í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •**
        start_epoch = 0
        max_epochs = 50
        batch_size = 2
        max_retries = 3  # í•œ ì—í¬í¬ë‹¹ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        # ì—í¬í¬ë³„ í•™ìŠµ
        for epoch in range(start_epoch, max_epochs):
            retries = 0
            while retries < max_retries:
                try:
                    print(f"\nğŸš€ Training {subject} - Epoch {epoch+1}/{max_epochs} (Retry: {retries})...")
                    model.fit(
                        [train_eeg, train_eye], train_labels,
                        validation_data=([valid_eeg, valid_eye], valid_labels),
                        epochs=1, batch_size=batch_size
                        #callbacks=[checkpoint_callback]
                    )
                    # ì—í¬í¬ê°€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ë©´ while ë£¨í”„ íƒˆì¶œ
                    break  
                except tf.errors.ResourceExhaustedError:
                    print(f"âš ï¸ OOM ë°œìƒ! ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ & ì¬ì‹œì‘ (Retry: {retries+1})...")
                    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹œ OOMì´ ë°œìƒí•  ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
                    try:
                        model.save_weights(checkpoint_path)
                    except tf.errors.ResourceExhaustedError:
                        print("âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì¤‘ OOM ë°œìƒ - ì €ì¥ ê±´ë„ˆëœ€.")
                    
                    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                    tf.keras.backend.clear_session()
                    gc.collect()
                    
                    # ëª¨ë¸ ì¬ìƒì„± ë° ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ìˆë‹¤ë©´)
                    model = build_full_model(eeg_input_shape=train_eeg.shape[1:])
                  
                    retries += 1
                    # ì¬ì‹œë„ ì „ì— ì ì‹œ íœ´ì‹ (ì˜µì…˜)
                    tf.keras.backend.sleep(1)
            else:
                # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ë©´ ì—í¬í¬ ì¢…ë£Œ ë° ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°.
                print(f"âŒ ì—í¬í¬ {epoch+1}ì—ì„œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì˜€ìŠµë‹ˆë‹¤. ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                break  # ë˜ëŠ” continueë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆìŒ.

        # ğŸš€ **ìµœì¢… ëª¨ë¸ ì €ì¥**
        subject_save_path = os.path.join(SAVE_PATH, subject)
        os.makedirs(subject_save_path, exist_ok=True)
        weight_path = os.path.join(subject_save_path, f"{subject}_multimodal_model.weights.h5")
        model.save_weights(weight_path)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ë¨: {weight_path}")

        # ğŸš€ **í…ŒìŠ¤íŠ¸ í‰ê°€**
        predictions = model.predict([test_eeg, test_eye])
        predicted_labels = np.argmax(predictions, axis=-1)
        test_report = classification_report(
            test_labels, predicted_labels,
            target_names=["Negative", "Positive", "Neutral"],
            labels=[0, 1, 2],
            zero_division=0
        )
        print(f"\nğŸ“Š Test Report for {subject}")
        print(test_report)

        report_path = os.path.join(subject_save_path, f"{subject}_test_report.txt")
        with open(report_path, "w") as f:
            f.write(test_report)
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {report_path}")
        
if __name__ == "__main__":
    train_multimodal()


# # ======================================================
# # â˜… Intraâ€‘Subject Crossâ€‘Validation: subjectë³„ ë°ì´í„° ë¡œë“œ â†’ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í•  â†’ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
# # ======================================================
# if __name__ == "__main__":
#     for subject in SUBJECTS:
#         print("="*50)
#         print(f"ğŸ”¹ Processing Subject: {subject}")
        
#         # 1. í•´ë‹¹ subjectì˜ ë°ì´í„° ë¡œë“œ
#         eeg_data, eye_data, labels = load_multimodal_data(subject)
        
#         if len(eeg_data) == 0:
#             print(f"ğŸš¨ Subject {subject}: ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ subjectë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
#             continue
        
#         print(f"âœ… {subject} - Total Samples: {len(eeg_data)}")
        
#         # 2. 80:20 ë¹„ìœ¨ë¡œ í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í•  (ë ˆì´ë¸” ë¶ˆê· í˜•ì„ ê³ ë ¤í•˜ì—¬ stratify ì‚¬ìš©)
#         X_train_eeg, X_test_eeg, X_train_eye, X_test_eye, y_train, y_test = train_test_split(
#             eeg_data, eye_data, labels, test_size=0.2, random_state=42, stratify=labels)
        
#         print(f"í•™ìŠµ ìƒ˜í”Œ: {len(X_train_eeg)} | í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(X_test_eeg)}")
        
#         # 3. subjectë³„ ëª¨ë¸ ìƒì„±
#         model = build_full_model(num_classes=3)
#         model.summary()
        
#         # 4. ëª¨ë¸ í•™ìŠµ
#         history = model.fit([X_train_eeg, X_train_eye],
#                             {"Inter_Classification": y_train,
#                              "EEG_Classification": y_train,
#                              "EyeCrop_Classification": y_train},
#                             validation_data=([X_test_eeg, X_test_eye],
#                                              {"Inter_Classification": y_test,
#                                               "EEG_Classification": y_test,
#                                               "EyeCrop_Classification": y_test}),
#                             epochs=EPOCHS,
#                             batch_size=BATCH_SIZE,
#                             verbose=1)
        
#         # 5. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€
#         print(f"\nğŸ”¸ Evaluating Subject {subject}...")
#         eval_results = model.evaluate([X_test_eeg, X_test_eye],
#                                       {"Inter_Classification": y_test,
#                                        "EEG_Classification": y_test,
#                                        "EyeCrop_Classification": y_test},
#                                       batch_size=BATCH_SIZE, verbose=1)
#         print("Evaluation results:")
#         for name, value in zip(model.metrics_names, eval_results):
#             print(f"  {name}: {value:.4f}")
        
#         # 6. ì˜ˆì¸¡ ë° Classification Report (ìœµí•© ë¶„ë¥˜ ê²°ê³¼ ê¸°ì¤€)
#         predictions = model.predict([X_test_eeg, X_test_eye], batch_size=BATCH_SIZE)
#         # predictionsì˜ ì²« ë²ˆì§¸ ì¶œë ¥(Inter_Classification)ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡ ì‚°ì¶œ
#         inter_preds = np.argmax(predictions[0], axis=1)
#         print(f"\nSubject {subject} Classification Report (Inter-Modal Output):")
#         print(classification_report(y_test, inter_preds, target_names=list(EMOTION_MAPPING.keys())))
        
#         # 7. í•™ìŠµ ì™„ë£Œëœ subjectë³„ ëª¨ë¸ ì €ì¥
#         subject_save_path = os.path.join(SAVE_PATH, subject)
#         os.makedirs(subject_save_path, exist_ok=True)
#         model_save_file = os.path.join(subject_save_path, "multimodal_emotion_classifier.h5")
#         model.save(model_save_file)
#         print(f"âœ… Model for subject {subject} saved at {model_save_file}\n")