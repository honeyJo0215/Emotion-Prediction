import os
import glob
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Flatten, Bidirectional, concatenate
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.layers import BatchNormalization, Dropout
from numpy.lib.stride_tricks import sliding_window_view

# ==============================
# 1. ë°ì´í„° ë¡œë“œ ë° ë³€í™˜ (10ì´ˆ ë‹¨ìœ„ + 1ì´ˆ ì˜¤ë²„ë©)
# ==============================
DATA_DIR = "/home/bcml1/2025_EMOTION/DEAP_PPG_1s"
INTRA_RESULTS_DIR = "./new_PPG_intra_tf"
INTER_RESULTS_DIR = "./new_PPG_inter_tf"

LABEL_MAP = {"0": 0, "1": 1, "2": 2, "3": 3}  # excited(0), relaxed(1), stressed(2), bored(3)

# í´ë” ìƒì„±
os.makedirs(INTRA_RESULTS_DIR, exist_ok=True)
os.makedirs(INTER_RESULTS_DIR, exist_ok=True)

def load_data(data_dir, window_size=10, overlap_size=1):
    """
    PPG ë°ì´í„°ë¥¼ window_size ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  overlap_size ì´ˆ ë‹¨ìœ„ë¡œ ì˜¤ë²„ë©í•˜ì—¬ ë³€í™˜
    :param data_dir: ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
    :param window_size: nì´ˆ ë‹¨ìœ„ (ê¸°ë³¸: 10ì´ˆ)
    :param overlap_size: mì´ˆ ì˜¤ë²„ë© (ê¸°ë³¸: 1ì´ˆ, 0.5ì´ˆë„ ê°€ëŠ¥)
    :return: ë³€í™˜ëœ ë°ì´í„° (X, y, subjects, trials)
    """
    file_list = glob.glob(os.path.join(data_dir, "*.npy"))
    data_list, labels, subjects, trials = [], [], [], []
    
    samples_per_sec = 128
    window_size_samples = int(window_size * samples_per_sec)  # ì˜ˆ: 10ì´ˆ -> 1280 ìƒ˜í”Œ
    step_size = window_size_samples - int(overlap_size * samples_per_sec)
    
    for file in file_list:
        file_name = os.path.basename(file)
        parts = file_name.split("_")
        subject_id = parts[0]  # ì˜ˆ: "sXX"
        trial_id = parts[2]    # ì˜ˆ: "trial_XX"
        label_str = parts[-1].replace(".npy", "")
        # ê°ì • ë¼ë²¨ ë§¤í•‘
        LABEL_MAP = {"0": 0, "1": 1, "2": 2, "3": 3}
        label = LABEL_MAP[label_str]
        
        npy_data = np.load(file)  # ì›ë³¸ shape: (60, 5, 128)
        if npy_data.shape != (60, 5, 128):
            print(f"âŒ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {file} - Shape {npy_data.shape}")
            continue
        
        # ê° ì´ˆë³„ ë°ì´í„°ë¥¼ í•˜ë‚˜ì˜ ì—°ì† ì‹ í˜¸ë¡œ ë³‘í•© (ë³µì‚¬ ì—†ì´ ë·° ìƒì„±)
        continuous_data = np.transpose(npy_data, (0, 2, 1)).reshape(-1, 5)  # shape: (7680, 5)
        
        # sliding_window_viewë¥¼ ì‚¬ìš©í•˜ì—¬ (window_size_samples, 5) í¬ê¸°ì˜ ë·°ë¥¼ ìƒì„± (ë³µì‚¬í•˜ì§€ ì•ŠìŒ)
        windows_view = sliding_window_view(continuous_data, window_shape=(window_size_samples, 5))
        # windows_viewì˜ ì²« ë²ˆì§¸ ì°¨ì›ì€ ëª¨ë“  ê°€ëŠ¥í•œ ì°½ì´ë¯€ë¡œ, step_size ê°„ê²©ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
        windowed_data = windows_view[::step_size, 0, :, :]  # shape: (N, window_size_samples, 5)
        
        data_list.append(windowed_data)
        labels.extend([label] * windowed_data.shape[0])
        subjects.extend([subject_id] * windowed_data.shape[0])
        trials.extend([trial_id] * windowed_data.shape[0])
    
    if data_list:
        X = np.concatenate(data_list, axis=0)
        y = np.array(labels)
        subjects = np.array(subjects)
        trials = np.array(trials)
        return X, y, subjects, trials
    else:
        print("âš  No valid data found!")
        return np.array([]), np.array([]), np.array([]), np.array([])

# ==============================
# 2. TensorFlow Dataset ì •ì˜
# ==============================
def create_tf_dataset(X, y, batch_size=16, shuffle=True):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    if shuffle:
        dataset = dataset.shuffle(len(X))
    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)

# ==============================
# 3. CNN-LSTM ëª¨ë¸ ì •ì˜
# ==============================
# def build_cnn_lstm_model():
#     input_layer = Input(shape=(5, 1280))  # (channels, time_steps)

#     # CNN Layers (Multi-scale Feature Extraction)
#     conv1 = Conv1D(32, kernel_size=3, padding="same", activation="relu")(input_layer)
#     conv1 = BatchNormalization()(conv1)
#     conv2 = Conv1D(32, kernel_size=5, padding="same", activation="relu")(input_layer)
#     conv2 = BatchNormalization()(conv2)
#     conv3 = Conv1D(32, kernel_size=7, padding="same", activation="relu")(input_layer)
#     conv3 = BatchNormalization()(conv3)

#     # Max Pooling (Size Reduction)
#     pool1 = MaxPooling1D(pool_size=4)(conv1)  # (batch, 320, 32)
#     pool2 = MaxPooling1D(pool_size=4)(conv2)  # (batch, 320, 32)
#     pool3 = MaxPooling1D(pool_size=4)(conv3)  # (batch, 320, 32)

#     # Multi-scale Feature Fusion
#     merged = concatenate([pool1, pool2, pool3])  # (batch_size, 320, 96)

#     # LSTM Layer
#     lstm = Bidirectional(LSTM(64, return_sequences=False, dropout=0.3))(merged)

#     # Fully Connected Layer
#     dense = Dense(64, activation="relu")(lstm)
#     dense = Dropout(0.3)(dense)
#     output_layer = Dense(4, activation="softmax")(dense)  # 4-class classification (excited, relaxed, stressed, bored)

#     # Model Compilation
#     model = Model(inputs=input_layer, outputs=output_layer)
#     model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    
#     return model
def build_flexible_cnn_lstm_model(time_steps):
    input_layer = Input(shape=(5, time_steps))  # (channels, time_steps)

    # CNN Layers (Multi-scale Feature Extraction)
    conv1 = Conv1D(32, kernel_size=3, padding="same", activation="relu")(input_layer)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling1D(pool_size=2)(conv1)

    conv2 = Conv1D(32, kernel_size=5, padding="same", activation="relu")(pool1)
    conv2 = BatchNormalization()(conv2)
    pool2 = MaxPooling1D(pool_size=2)(conv2)

    conv3 = Conv1D(32, kernel_size=7, padding="same", activation="relu")(pool2)
    conv3 = BatchNormalization()(conv3)
    pool3 = MaxPooling1D(pool_size=2)(conv3)

    # Multi-scale Feature Fusion
    merged = concatenate([pool1, pool2, pool3])  # (batch_size, reduced_T, 96)

    # LSTM Layer (Sequence Learning)
    lstm = Bidirectional(LSTM(64, return_sequences=True, dropout=0.3))(merged)
    lstm = LSTM(64, return_sequences=False)(lstm)  # ì¶”ê°€ ë ˆì´ì–´

    # Fully Connected Layer
    dense = Dense(64, activation="relu")(lstm)
    dense = Dropout(0.3)(dense)
    output_layer = Dense(4, activation="softmax")(dense)

    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
    
    return model

# ==============================
# 4. Training Curve ë° Confusion Matrix ì €ì¥ í•¨ìˆ˜
# ==============================
def save_training_curves(history, results_dir, fold):
    plt.figure(figsize=(10, 5))
    plt.plot(history.history["loss"], label="Loss")
    plt.plot(history.history["accuracy"], label="Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Value")
    plt.legend()
    plt.title(f"Training Curve - {fold}")
    plt.savefig(os.path.join(results_dir, f"training_curve_{fold}.png"))
    plt.close()

def save_confusion_matrix(y_true, y_pred, results_dir, fold):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=LABEL_MAP.keys(), yticklabels=LABEL_MAP.keys())
    plt.xlabel("Predicted")
    plt.ylabel("True")
    plt.title(f"Confusion Matrix - {fold}")
    plt.savefig(os.path.join(results_dir, f"confusion_matrix_{fold}.png"))
    plt.close()
    
# ==============================
# 5. Inter-subject Cross-validation (Leave-One-Subject-Out)
# ==============================
def inter_subject_train(window_size=10, overlap_size=1):
    unique_subjects = np.unique(subjects)
    
    for test_subject in unique_subjects:
        print(f"\n===== Leave-One-Subject-Out: Testing on {test_subject} ({window_size}s window) =====")
        subject_dir = os.path.join(INTER_RESULTS_DIR, f"subject_{test_subject}")
        os.makedirs(subject_dir, exist_ok=True)

        # âœ… í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„í• 
        train_indices = [i for i, s in enumerate(subjects) if s != test_subject]
        test_indices = [i for i, s in enumerate(subjects) if s == test_subject]

        X_train, y_train = X[train_indices], y[train_indices]
        X_test, y_test = X[test_indices], y[test_indices]

        # âœ… í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ë¶„í•  (í›ˆë ¨:ê²€ì¦ = 9:1)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

        train_dataset = create_tf_dataset(X_train, y_train, batch_size=32)
        val_dataset = create_tf_dataset(X_val, y_val, shuffle=False, batch_size=32)
        test_dataset = create_tf_dataset(X_test, y_test, shuffle=False, batch_size=32)

        # âœ… ìœ ì—°í•œ CNN-LSTM ëª¨ë¸ ìƒì„± (time_steps ìë™ ì„¤ì •)
        time_steps = window_size * 128  # ìƒ˜í”Œ ê°œìˆ˜ (128Hz)
        print(f"Time steps for model ({window_size}s window):", time_steps)
        model = build_flexible_cnn_lstm_model(time_steps)
        
        # âœ… ëª¨ë¸ í•™ìŠµ
        history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=2)

        # âœ… í‰ê°€
        y_pred = np.argmax(model.predict(X_test), axis=1)
        report = classification_report(y_test, y_pred, digits=4)
        print(f"\n===== Classification Report for {test_subject} ({window_size}s window) =====\n{report}")

        # âœ… ê²°ê³¼ ì €ì¥
        save_training_curves(history, subject_dir, str(test_subject))
        save_confusion_matrix(y_test, y_pred, subject_dir, str(test_subject))

        with open(os.path.join(subject_dir, "classification_report.txt"), "w") as f:
            f.write(report + "\n")

# ==============================
# 6. Intra-subject Cross-validation (5-fold)
# ==============================
def intra_subject_train(window_size=10, overlap_size=1):
    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    for fold, (train_idx, test_idx) in enumerate(kf.split(X), start=1):
        print(f"\n===== Fold {fold}/5 ({window_size}s window) =====")
        fold_dir = os.path.join(INTRA_RESULTS_DIR, f"kfold{fold}")
        os.makedirs(fold_dir, exist_ok=True)

        X_train, y_train = X[train_idx], y[train_idx]
        X_test, y_test = X[test_idx], y[test_idx]

        # âœ… í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ë¶„í•  (í›ˆë ¨:ê²€ì¦ = 9:1)
        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

        train_dataset = create_tf_dataset(X_train, y_train, batch_size=32)
        val_dataset = create_tf_dataset(X_val, y_val, shuffle=False, batch_size=32)
        test_dataset = create_tf_dataset(X_test, y_test, shuffle=False, batch_size=32)

        # âœ… ìœ ì—°í•œ CNN-LSTM ëª¨ë¸ ìƒì„± (time_steps ìë™ ì„¤ì •)
        time_steps = window_size * 128  # ìƒ˜í”Œ ê°œìˆ˜ (128Hz)
        print(f"Time steps for model ({window_size}s window):", time_steps)
        model = build_flexible_cnn_lstm_model(time_steps)

        # âœ… ëª¨ë¸ í•™ìŠµ
        history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, verbose=2)

        # âœ… í‰ê°€
        y_pred = np.argmax(model.predict(X_test), axis=1)
        report = classification_report(y_test, y_pred, digits=4)
        print(f"\n===== Fold {fold} Classification Report ({window_size}s window) =====\n{report}")

        # âœ… ê²°ê³¼ ì €ì¥
        save_training_curves(history, fold_dir, str(fold))
        save_confusion_matrix(y_test, y_pred, fold_dir, str(fold))

        with open(os.path.join(fold_dir, "classification_report.txt"), "w") as f:
            f.write(report + "\n")

# ì‹¤í–‰
X, y, subjects, trials = load_data(DATA_DIR, window_size=10, overlap_size=0.5)

# âœ… ë°ì´í„° ë¡œë”©ì´ ì‹¤íŒ¨í–ˆì„ ê²½ìš° í”„ë¡œê·¸ë¨ ì¢…ë£Œ
if X.shape[0] == 0:
    print("ğŸš¨ Error: No valid data loaded! Check data directory and preprocessing.")
    exit(1)

# âœ… ë°ì´í„° í¬ê¸° í™•ì¸
print("X shape:", X.shape)  # ì˜ˆìƒ: (ìƒ˜í”Œ ìˆ˜, 5, 1280)
print("y shape:", y.shape)  # ì˜ˆìƒ: (ìƒ˜í”Œ ìˆ˜,)
print("subjects shape:", subjects.shape)  # ì˜ˆìƒ: (ìƒ˜í”Œ ìˆ˜,)
print("trials shape:", trials.shape)  # ì˜ˆìƒ: (ìƒ˜í”Œ ìˆ˜,)

# í•™ìŠµ ì‹œì‘
inter_subject_train()
intra_subject_train()
